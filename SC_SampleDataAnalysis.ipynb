{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# An example of the scientific usefulness of Python"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(np.arange)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on built-in function arange in module numpy.core.multiarray:\n",
        "\n",
        "arange(...)\n",
        "    arange([start,] stop[, step,], dtype=None)\n",
        "    \n",
        "    Return evenly spaced values within a given interval.\n",
        "    \n",
        "    Values are generated within the half-open interval ``[start, stop)``\n",
        "    (in other words, the interval including `start` but excluding `stop`).\n",
        "    For integer arguments the function is equivalent to the Python built-in\n",
        "    `range <http://docs.python.org/lib/built-in-funcs.html>`_ function,\n",
        "    but returns a ndarray rather than a list.\n",
        "    \n",
        "    When using a non-integer step, such as 0.1, the results will often not\n",
        "    be consistent.  It is better to use ``linspace`` for these cases.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    start : number, optional\n",
        "        Start of interval.  The interval includes this value.  The default\n",
        "        start value is 0.\n",
        "    stop : number\n",
        "        End of interval.  The interval does not include this value, except\n",
        "        in some cases where `step` is not an integer and floating point\n",
        "        round-off affects the length of `out`.\n",
        "    step : number, optional\n",
        "        Spacing between values.  For any output `out`, this is the distance\n",
        "        between two adjacent values, ``out[i+1] - out[i]``.  The default\n",
        "        step size is 1.  If `step` is specified, `start` must also be given.\n",
        "    dtype : dtype\n",
        "        The type of the output array.  If `dtype` is not given, infer the data\n",
        "        type from the other input arguments.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    out : ndarray\n",
        "        Array of evenly spaced values.\n",
        "    \n",
        "        For floating point arguments, the length of the result is\n",
        "        ``ceil((stop - start)/step)``.  Because of floating point overflow,\n",
        "        this rule may result in the last element of `out` being greater\n",
        "        than `stop`.\n",
        "    \n",
        "    See Also\n",
        "    --------\n",
        "    linspace : Evenly spaced numbers with careful handling of endpoints.\n",
        "    ogrid: Arrays of evenly spaced numbers in N-dimensions\n",
        "    mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> np.arange(3)\n",
        "    array([0, 1, 2])\n",
        "    >>> np.arange(3.0)\n",
        "    array([ 0.,  1.,  2.])\n",
        "    >>> np.arange(3,7)\n",
        "    array([3, 4, 5, 6])\n",
        "    >>> np.arange(3,7,2)\n",
        "    array([3, 5])\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=np.arange(0.,10.,0.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.sin(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%don't forget: %pylab inline\n",
      "import pylab as plb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[<matplotlib.lines.Line2D at 0x107c33310>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlcVuW2B/Dfq2Ca5pSBCjgCAY4YamkqDmgOoCgmNkiZ\nZXo8WXm7x0/3nk72KcNOp45mg+Y9qeVBzZEMMYdQ04yc0hwSFApQySEyRUVw3z/WwZHpHZ89/L6f\nD59C97v3it538ey1n2c9Nk3TNBARkSVUUx0AERF5DpM+EZGFMOkTEVkIkz4RkYUw6RMRWQiTPhGR\nhTid9MeOHQtfX1+0a9eu3GOee+45BAUFoUOHDtizZ4+zlyQiIgc5nfSffPJJpKamlvv3KSkpyMzM\nREZGBubOnYsJEyY4e0kiInKQ00m/R48eaNCgQbl/n5ycjISEBABA165dUVBQgPz8fGcvS0REDnB7\nTT8vLw8BAQHXvvf390dubq67L0tERGXwyIPcWzs92Gw2T1yWiIhu4eXuC/j5+SEnJ+fa97m5ufDz\n87vtuMDAQBw9etTd4RARmUrr1q2RmZlZ5ePdPtKPiYnBwoULAQA7duxA/fr14evre9txR48ehaZp\nhvk6eVLDiBEagoM1LFmiobi44uPz8zVMnaqhUSMNs2ZpuHq1/GP/9re/Kf/v08sXfxbm/lmUlGh4\n5x0Nd9+t4X/+R8OpUxUfX1ysYdEiDQ0a/A3x8ZUfb4UvewfLTo/0R48ejc2bN+P06dMICAjAtGnT\ncOXKFQDA+PHjMWjQIKSkpCAwMBC1a9fGJ5984uwllfv2W2DkSODxx4HPPgNq1qz8NT4+wJtvAk88\nIa9LSwM++QSoW9fd0RLpU0GBfBbOnpXPVFBQ5a+pXh145BHgwAHg0iUgPBxYuRKIiHB/vGbhdNJP\nSkqq9JjZs2c7exndWL4cmDAB+Ne/gCFD7H/9vfcCW7cCzz0H9OwJrFsHlHHjQ2RqJ04A/foBffrI\nZ6pGDfte7+0NvPEG8OCDwMCBwLx5wNCh7onVbLgi1w5LlgB/+hPw1VeOJfxSd9wBfPQREBsL9OgB\nHD9+899HRkY6FaeZ8GdxnVl+Fjk5QK9ewKOPAu+9Z3/CB67/LGJjgdRUYPx4+eVBlbNpmqaLTVRs\nNht0EkqZVq8Gnn1WRubt27vuvNOnA4sXA1u2APXru+68RHp0+jTQrZsk6SlTXHfevXuBAQOAhQvl\nn1Zib+5k0q+CXbuAhx4CUlKAzp1de25NAyZPBn74Qe4g7rjDtecn0otLl6Sk8+CDQGKi68//zTfA\n8OHAxo1ABV1hTIdJ38WOHwe6dAFmzZI3lDtcvQrExUlt/8MP3XMNIpU0DRgzBigqApKSgGpuKiwn\nJQFTpwLffy+TJ6zA3tzJmn4FSkpkpsAzz7gv4QPyAZg/H9i0SW5Picxm3jy5m50/330JHwBGj5bP\n7JgxMpii23GkX4HXXgM2b5ayS/Xq7r/e/v0ym2HzZiAszP3XI/KEffuAvn1l1lpIiPuvV1wMREbK\nZIupU91/PdVY3nGRbduk5LJrF9C0qeeu+9FHMh10+3bAy+3rpYnc6/Jl4L77gJdeAv7Td9EjcnLk\nuikp5p/Dz/KOC1y6BIwbB8ye7dmED8ishvr1gRkzPHtdIneYPh1o1UrKLZ4UEAC88w4wdqw8R6Dr\nONIvw1//Kiv+VqxQc/3SUcqmTUDbtmpiIHJWaVln716gjHZbbqdpQHS0zLj72988f31PYXnHSaV1\n9R9+8Pwo/0YffAAsXQp8/TXApqRkNCUlwAMPAE8/LV+q5OZKq4YtW4DQUHVxuBPLO07QNGmx8Prr\nahM+IGWec+dkChqR0fzf/8mak3Hj1Mbh7w+8/DLwwgvy+SaO9G+ydKk0Rdu50zOzdSrz7bfyMPnQ\nITZmI+P4/XfpMZWSAnTqpDoa4MoVWUX/1ltS7jEblnccdOmSTCebP1+me+nF2LFAo0byhiUygv/6\nL+mgOW+e6kiu++orYOJEeVZntlXvTPoOKh3h661p0/HjsqR8716ZkUCkZ0eOSG+dAwf01z126FBp\nAfHSS6ojcS0mfQecOiUPeb77DmjdWkkIFXr5ZSA/X+qkRHoWHy+llJdfVh3J7Q4elLv4jAygXj3V\n0bgOk74DXnoJKCwE3n9fyeUrVVAABAfLxitcqUt6tW8f0L8/kJkJ1KmjOpqyPfEE0Lw5MG2a6khc\nh0nfTidPAm3ayBtWxVziqnr7bVklvHKl6kiIyhYbKxsDvfCC6kjKl5UlK3QPHwbuuUd1NK7BpG+n\nyZOlAdS773r80na5eFFKTykpQMeOqqMhutnOnVIzz8wEatVSHU3FJk2Sh7n/+IfqSFyDSd8OublA\nhw5S69PbQ6eyvPOO9ORZtkx1JEQ3GzRIGpxNnKg6ksqdOCF39z/9ZI7RPpO+HZ5/XubjG+U3/oUL\n0sdk0yZ50xLpwfffAyNGyANSo0yHfPZZmQr9+uuqI3Eek34VnTkDBAVJ2wU91/JvlZgoMS9apDoS\nIjFyJNC9uwyijOLYMdkc6dgx4y98ZNKvotdeA37+2XjTIM+dk9r+t98CgYGqoyGry8iQeflZWfqd\nsVOeRx+V6aV/+YvqSJzDpF8FpWWSzZs9s6mDq/3v/8o0ztmzVUdCVjd+vDwPe+011ZHYb/9+mWJ6\n7Jj+Hz5XhEm/Ct57T+riRp3+WPogKjMTaNhQdTRkVSdPyqLGI0eM+0A0JgYYOFAaLRoVu2xWorhY\nHtwa+ZauSRN5s86dqzoSsrJZs2Q/WqMmfACYMgWYOdNa++laLuknJ8uD2/vvVx2Jc154Qe5YuCsQ\nqVBYCHz8MfDii6ojcU7PnkDNmsD69aoj8RzLJf3Zs4E//1l1FM7r0EFurZcuVR0JWVFSEtC1qz57\nVdnDZpMFmjNnqo7EcyxV0//xR3lwk50N1Kjh1kt5REqKPNTdtYu7a5HnaJr0yU9MBAYMUB2N8y5d\nkn48W7bIPgBGw5p+BWbPltkGZkj4APDQQzKF87vvVEdCVrJ9u8yAi4pSHYlr1KwpWzq+957qSDzD\nMiP9ggKgZUvZhapxY7ddxuPeflumni1YoDoSsorRo+WZ2OTJqiNxnePHgbZtZb2B0douc8pmOd59\nV5aL//vfbruEEqdPyyKtY8c4fZPcz8jJsTKjRgG9ehmjf9CNWN4pw9Wr0ivfDA9wb9Wokez7OX++\n6kjICubOlY1SzJbwAeCZZ4A5c8y/gbolkv7XX8sScaNP0yzPhAnARx9Za64xeV5xsUzTNNpIuKp6\n95ZnFenpqiNxL0sk/XnzgHHjzDvD5YEH5GHU11+rjoTMLDUVaNZMyjtmVK2ajPbNvujR9DX9M2dk\nLnFWFtCggctPrxsffghs3Mhe++Q+sbHSM/+pp1RH4j6//irTNrOzjVPCYk3/FosWAYMHmzvhA8Bj\njwEbNsiblsjVTp6UPZofflh1JO7l4yNTUc3cutzUSV/Trpd2zO6uu2S7us8+Ux0JmdGCBbJRyl13\nqY7E/cz+QNfUSX/nTukR0quX6kg848kngU8+Me+bldTQNNl3wgqDJwDo0wc4f17yhxmZOunPmweM\nHSsPaKygZ0+ZfbBrl+pIyEy2bpVV7F27qo7EM6pVAxISgIULVUfiHqZ9kHvhAhAQIP12mjZ12Wl1\n77XXgPx8WZdA5ApjxgDh4dLZ1SqysmQ7xbw8/bdt4YPc/1i2TPbttFLCB2SEsnixNJEictb589KO\n/LHHVEfiWS1bAmFhwJdfqo7E9Uyb9D/7TEYoVtO8uYzKVq9WHQmZwYoVQI8ext4oxVEJCebsaWXK\npJ+XJ3Xt6GjVkahR+kCXyFmffgo8/rjqKNSIi5MFj6dPq47EtUyZ9JOSgOHDZZWqFQ0fLu2WT5xQ\nHQkZmdUHT3XrymK0pCTVkbiWKZO+lUcnAFCrlszZX7xYdSRkZKWDp1q1VEeijhlLPKZL+vv2Se/8\nHj1UR6LWo4+ar400eZbVB08A0Lev3DEfOKA6EtcxXdL/7DNJeFaZm1+e3r2B3FzgyBHVkZARcfAk\nqleXmUuffqo6EtdxOjWmpqYiJCQEQUFBmDFjxm1/n5aWhnr16iE8PBzh4eF4/fXXnb1kuUpKZHRr\ntellZfHykk0hzNxDhNzn00/lc2T1wRMAPPKIlEr1saLJeV7OvLikpASTJk3Chg0b4Ofnh86dOyMm\nJgahoaE3HderVy8kJyc7FWhVpKUBvr4yv5bkjmf0aODVV83bVppcr3TwtGGD6kj0oX174M47gR07\npI250Tn1ezw9PR2BgYFo0aIFvL29ER8fj9VlTBD31KLfzz7jKP9GEREyUvv+e9WRkJFs3iz7SN8y\ndrMsm012CzPLLB6nkn5eXh4CAgKufe/v74+8vLybjrHZbNi+fTs6dOiAQYMG4eDBg85cslyXL8uC\npFGj3HJ6Q7LZZLTPEg/ZY8kSfo5uFR8PfP653AUZnVPlHVsVagadOnVCTk4O7rzzTqxduxbDhg3D\nkXKeLr766qvX/j0yMhKRkZFVjmXdOqBdO+u1XajMI4/Iw7h//EPq/EQVuXJFVuHy7vBmwcGSW9LS\nZEaPSmlpaUhLS3P49U6lAT8/P+Tk5Fz7PicnB/7+/jcdc9cNDbgHDhyIiRMn4uzZs2jYsOFt57sx\n6dtr6VKOTsoSFCRb3G3aBPTvrzoa0rtNm2SnuRYtVEeiP6NHS4lHddK/dUA8bdo0u17vVHknIiIC\nGRkZyM7ORlFREZYsWYKYmJibjsnPz79W009PT4emaWUmfGdcvCiNkUaMcOlpTWPUKPmlSFQZlnbK\nN2oUsHIlUFSkOhLnOJX0vby8MHv2bAwYMABhYWEYNWoUQkNDMWfOHMyZMwcAsGzZMrRr1w4dO3bE\n888/j8VuWCa6di3QqZPM3KHbxcUBq1bJrTtReYqK5LnYyJGqI9GngACZGbhunepInGOKfvrx8bIY\nafx4FwdlIvffL732WeKh8qxZA8yYIZumUNk++ADYtk1fkyMs10//wgUgNZWlncqMHMkSD1WMpZ3K\nxcVJKbmwUHUkjjN80v/yS9nGrVEj1ZHoG0s8VJFLl2SkHxenOhJ98/GRHbVSUlRH4jjDJ/2lS4GH\nH1Ydhf41bw4EBkp/cKJbrV0rm+80bqw6Ev2LiwOWL1cdheMMnfT/+ANYvx6IjVUdiTGwxEPlYWmn\n6oYNk1+SFy+qjsQxhk76X3wh++C6eAaoabHEQ2UpLORzMXv4+Mhd0VdfqY7EMYZO+izt2Ke0xLNp\nk+pISE9SU4HOnflczB4jRhi3xGPYpH/+vCSvoUNVR2IsI0dKDxGiUitWcJRvr+HD5cG3ERdqGTbp\np6bK3PMGDVRHYiws8dCNiopkJgoHT/Zp2lQWam3cqDoS+xk26a9cKb9tyT4s8dCNNm2S5NWkiepI\njGfECGDZMtVR2M+QSf/yZY5OnDF8uPzSJFqxgoMnR40YIW0rjHbXbMikz9GJc2Jj5c169arqSEil\nkhJ5H3DKs2OaNQNatZJNZ4zEkEmfoxPnBAUBd98t27+RdW3bBvj5AS1bqo7EuOLijFfiMVzS5+jE\nNWJjWeKxOg6enDdihEyMMNKOWoZL+tu2yZPzVq1UR2JspUlfHz1WydM0jUnfFVq3ljLzN9+ojqTq\nDJf0+UZ1jfBwoLgY+PFH1ZGQCrt2AXfeyc3PXaH0GZlRGCrpaxqnarqKzSY9RFjisabSwVMVtrmm\nSgwdKiUeo9w1Gyrp794N1KgBtGmjOhJzYF3fmjRNWghw8OQa7dvLz3T/ftWRVI2hkj5HJ6714INA\nXh6QlaU6EvKkQ4ekQ+R996mOxBxK75qNUuIxVNJnace1qlcHoqPl1pSsg4Mn1xs2zDifI8Mk/UOH\ngN9/l26A5Dos8VjPihWc8uxq3bsDP/8M/PKL6kgqZ5ikv3KlvFGrGSZiY+jXD9i3D/j1V9WRkCfk\n5Ehi6t5ddSTm4uUFDBlijBKPYVLoqlUcnbhDzZrAgAFAcrLqSMgTvvgCGDxYkhS5llHq+oZI+seP\nA5mZQM+eqiMxJ5Z4rGP1aiAmRnUU5tS/P5CeDvz2m+pIKmaIpL9mDfDQQ4C3t+pIzGnQIGDrVtlz\nmMzr3Dng228lOZHr3Xkn0Ls38OWXqiOpmCGSfnKyzDIh96hbF3jgAdlknsxr3TqZpnvXXaojMS8j\nzOLRfdK/cAHYskVG+uQ+MTGs65tdcjJLO+42ZIgMni5dUh1J+XSf9DdskGma3BbRvaKj5bbUSN0C\nqeqKi2XjoSFDVEdibvfcA3TsqO9tFHWf9Dk68YxmzQB/f6n5kvl88430zff3Vx2J+ZX24tErXSf9\nq1flIS7r+Z7BEo95cfDkOUOHys9br3fNuk766elyu8Te+Z4RHc2kb0aaxqTvSa1bA76++t2ZTtdJ\nn29Uz+rUSaZt/vST6kjIlQ4dks27O3RQHYl1REfLQjg9YtKna6pV0/eblRxT+jligzXP0fPnSLdJ\n/9gx4NQpoEsX1ZFYC+v65sPBk+d16QKcOSN5TG90m/S/+EKml7HBmmf16QP88IO8Ycn48vOlvNOr\nl+pIrKVaNelxpMfRvm5TKkcnatSsCfTtK3O6yfjWrJG2CzVqqI7EevRa4tFl0i8oAL7/Xtr+kuex\nxGMeHDypExUlMxB//111JDfTZdJPTZXb0dq1VUdiTYMGyVLyy5dVR0LOKCwEvv4aGDhQdSTWVLu2\n9DpKTVUdyc10mfTZYE0tHx/ZfD4tTXUk5IyNG4GICKBhQ9WRWJceSzy6S/pXrshvRvYIUYslHuNj\naUe96Ghg7VrpfaQXukv6W7cCgYFA06aqI7G20qSvaaojIUdcvSojTN4xq+XvDzRvDmzfrjqS63SX\n9L/4gqMTPQgJkZk8e/eqjoQckZ4ONGokLQFILb2VeHSV9DVNtnPj6EQ9m40lHiNjaUc/mPQrcPCg\ndKZr3151JARI0tDTm5WqjklfPzp1kq0qjxxRHYnQVdJnjxB96d4dyMoCcnNVR0L2OHoUOH2aLUz0\nolo1mZiilwGUrpI+6/n64uUlc7z18malqmELE/3R012zrt4W7BGiP3p6s1LVcNaO/vTtC+zeDfz2\nm+pIdJb0o6LYI0RvBgyQrfbOn1cdCVUFW5joU61aQGSkzNlXzemkn5qaipCQEAQFBWHGjBllHvPc\nc88hKCgIHTp0wJ49e8o9F0s7+lOvHtC1q7RlIP1LTQV69GALEz3Syywep5J+SUkJJk2ahNTUVBw8\neBBJSUk4dOjQTcekpKQgMzMTGRkZmDt3LiZMmFDu+QYNciYachdO3TQOPhfTryFD5JfylStq43Aq\n6aenpyMwMBAtWrSAt7c34uPjsXr16puOSU5ORkJCAgCga9euKCgoQH5+fpnnY48QfYqOBr78Ur8b\nPZMoLmYLEz1r0gQICpKuAyo5lfTz8vIQEBBw7Xt/f3/k5eVVekwu5wAaSosWQOPGwHffqY6EKrJt\nm/y/8vNTHQmVRw8lHi9nXmyr4oR67ZYGLuW97tVXX73275GRkYiMjHQ0NHKx0hJPt26qI6HycNaO\n/kVHA3FxwDvvOL4eKS0tDWlOtMB1Kun7+fkhJyfn2vc5OTnw9/ev8Jjc3Fz4lTMUuTHpk77ExABP\nPAEkJqqOhMqTnAwkJamOgirSoQNQVAQcPgyEhjp2jlsHxNOmTbPr9U6VdyIiIpCRkYHs7GwUFRVh\nyZIliLnlKVJMTAwWLlwIANixYwfq168PX19fZy5LCkREyBzjzEzVkVBZfvoJuHBBlvyTftls6lfn\nOpX0vby8MHv2bAwYMABhYWEYNWoUQkNDMWfOHMyZMwcAMGjQILRq1QqBgYEYP348PvjgA5cETp6l\nt6XkdLPS0g5bmOhfdLTa2XA27daCuyI2m+222j/pyxdfSC3y669VR0K36tUL+O//BgYPVh0JVebS\nJcDXV3okNWrk/PnszZ26WpFL+ta3L7Brlz6WktN1Z8/Kvgd9+qiOhKqiZk35LKWkqLk+kz5V2Z13\n6mcpOV23dq38f6lVS3UkVFUqp24y6ZNduDpXf5KTOVXTaAYPltYmly97/tpM+mSXIUOAdetk2hmp\nV1QEfPUVV+EajY8PEBYGbN7s+Wsz6ZNdGjcGgoPVLyUnsXWr/P9o3Fh1JGSvoUPV3DUz6ZPdWOLR\nD67CNa7Sz5GnJy0y6ZPdVL1Z6Waaxnq+kYWEAHfcITOvPIlJn+zWtq0knB9/VB2JtR08KJ0127dX\nHQk5wmZTc9fMpE92K32zcnWuWqW987kK17iY9MkwWNdXj/V84+veHcjOBjzZbZ5JnxzSs6d0Cjx5\nUnUk1nTqFHDggCzKIuPy8pIdAz1518ykTw6pUUM2Tf/yS9WRWFNKiizlv+MO1ZGQs4YOBW7ZcNCt\nmPTJYSzxqMNZO+YxYIDsenbunGeuxy6b5LCzZ2V7vvx89n3xpMuXZUVnRob8k4zvoYeAceNkVy17\nscsmeUzDhrJpx8aNqiOxlrQ0mTbLhG8enrxrZtInp7DE43mctWM+0dHynKa42P3XYtInp5S2iL16\nVXUk1qBpTPpmFBAANGsGbN/u/msx6ZNTgoKA+vWBnTtVR2INe/cC3t7SoZHMxVOzeJj0yWlcnes5\nq1YBw4ZxFa4ZxcRI0nf3fBYmfXIa6/qes3q1JH0yn44dZWbW4cPuvQ6TPjnt/vuB48eBn39WHYm5\nZWXJz/mBB1RHQu7gqQZsTPrktOrVZfs3lnjca/VqeYBbvbrqSMhdmPTJMFjicT+WdswvMlJ6KuXn\nu+8aXJFLLnH+PNCkCZCXB9Stqzoa8zlzBmjVShrccfWzuY0cCQwcCIwdW7XjuSKXlKhTB3jwQdk0\nnVxvzRppsMaEb37u3juXSZ9chiUe9ymdqknmN2gQsGkTcPGie87PpE8u48ml5FZSWChJYPBg1ZGQ\nJ5T2tNqwwT3nZ9Inl/H3B1q2BLZsUR2JuWzYANx3H3D33aojIU+JjQVWrnTPuZn0yaWGDwdWrFAd\nhbmsWiV1XrKO2FgplbrjrplJn1xq+HAZobABm2uUlMhDXCZ9a2nWzH13zUz65FIhIUC9ekB6uupI\nzGH7dsDPTzarIWtx110zkz653IgRLPG4CmftWJe77pqZ9MnlSkcoXGvnHE1jPd/K7r1X2pa7+q6Z\nSZ9crmNHqUXv3686EmP74QdJ/B06qI6EVHFHiYdJn1zOZuMsHldYvlw2ymbvfOtyx10zkz65BZO+\nczQN+PxzeT5C1uWOu2YmfXKLBx4ATp0CMjJUR2JMBw/KStwuXVRHQiq5466ZSZ/colo1mXXirlWF\nZrd8uXzYWdohJn0yDJZ4HLdsmdTziVx918ykT24TGSlv1Nxc1ZEYy5Ej8iHv1k11JKQHrr5rZtIn\nt/H2BoYMkbnmVHWlpZ1q/HTSf7jyrplvK3Kr4cOlVEFVt3w5Z+3QzSIjgcxM4JdfnD8Xkz651YAB\nssjoxAnVkRhDVpZ8sHv2VB0J6Ym3t5R4Pv/c+XMx6ZNb1awpm6twtF81y5dL2wUvL9WRkN6MGgUs\nWeL8eZj0ye1c9Wa1gtJVuES36t0byM4Gjh1z7jxM+uR2UVHAoUNATo7qSPTt559ltlPv3qojIT3y\n8pJnPUuXOnceJn1yuxo1XFePNLMlS+TBd40aqiMhvXLFXbPDSf/s2bOIiopCcHAw+vfvj4KCgjKP\na9GiBdq3b4/w8HB04Zpyy2KJp3KLFwOjR6uOgvSsRw/g5ElZy+Eoh5N+YmIioqKicOTIEfTt2xeJ\niYllHmez2ZCWloY9e/YgndspWVafPjIzxdl6pFn99JN8mDlrhypSvbo883GmxONw0k9OTkZCQgIA\nICEhAasqWIGjcTcNy/PyktKFs/VIs1q8GHj4YflQE1XE2btmh5N+fn4+fH19AQC+vr7Iz88v8zib\nzYZ+/fohIiICH3/8saOXIxNgiadsmgYkJQHx8aojISPo1g0oKJBOrI6ocDZwVFQUTp48edufv/HG\nGzd9b7PZYCunHeC2bdvQpEkTnDp1ClFRUQgJCUGPHj3KPPbVV1+99u+RkZGIjIysJHwykp49r9cj\ng4NVR6MfP/wAXL4MdO2qOhIygi1b0uDvn4YJExyb6WXTHKy9hISEIC0tDY0bN8aJEyfQu3dvHD58\nuMLXTJs2DXXq1MGUKVNuD8RmYxnIAv78Z8DHB/jrX1VHoh9/+Yv02XnzTdWRkFF89x2QkCBToatV\nsy93OlzeiYmJwYIFCwAACxYswLBhw247prCwEH/88QcA4MKFC/jqq6/Qrl07Ry9JJhAfL6UM/n4X\nmsZZO2S/Ll2AoiJg7177X+tw0p86dSrWr1+P4OBgbNq0CVOnTgUAHD9+HIMHDwYAnDx5Ej169EDH\njh3RtWtXDBkyBP3793f0kmQC3boBly4Be/aojkQfduwAatcGOBYie9hsMlD4978deK2j5R1XY3nH\nOl55BfjjD+Ddd1VHot5zzwGNGsnPhMgemZkyeHr4YftyJ5M+eVxGhiwyyc21dmOxoiLA319G+61a\nqY6GjMre3Mk2DORxQUFAixbA+vWqI1Fr7Vrg3nuZ8MmzmPRJiccfBz79VHUUai1cKDMwiDyJ5R1S\n4vRpIDBQOm/edZfqaDzvzBmgdWvprFmvnupoyMhY3iFDaNQI6NVL+sdb0eLFwMCBTPjkeUz6pIyV\nSzws7ZAqLO+QMpcuAX5+Mu2sWTPV0XjO4cPSdfSXX6w9e4lcg+UdMoyaNWWF7vz5qiPxrAULgEcf\nZcInNTjSJ6X27JFdtY4ds0Zb4ZISma6aksJVuOQaHOmToYSHy0PdjRtVR+IZ69YBTZow4ZM6TPqk\n3LhxwLx5qqPwjLlzgWeeUR0FWRnLO6RcQYGUPDIygHvuUR2N++TlAW3bytqEOnVUR0NmwfIOGU79\n+sDQoeafvvnJJ7J7GBM+qcSRPunC1q3A+PHAgQPSNtZsSkpkBe6KFUCnTqqjITPhSJ8M6cEHgatX\nJfmb0fr18sCaCZ9UY9InXbDZgD/9CXj/fdWRuAcf4JJesLxDunHunDzQ3b9fVuqaRU4O0KGDNFez\nYnM5ci99sYGiAAAJNUlEQVSWd8iw6tYFHnkEmDNHdSSu9eGHwGOPMeGTPnCkT7py6BDQu7eMiu+4\nQ3U0zrt4EWjeHNi2TTaPIXI1jvTJ0EJDZbXqsmWqI3GNRYuALl2Y8Ek/mPRJdyZNAt57T3UUztM0\nYNYs2fycSC+Y9El3hgwB8vNlw3AjS0sDiouBqCjVkRBdx6RPulO9OvDii8Bbb6mOxDnvviujfDMu\nNiPj4oNc0qULF4CWLWWx1r33qo7Gfj/+CPTrB2RlAbVqqY6GzIwPcskUatcGJk4E3n5bdSSOmTED\nmDyZCZ/0hyN90q3Tp4HgYOnH06SJ6miqLisLiIiQjWG48Tm5G0f6ZBqNGsmipn/+U3Uk9vn736Xl\nAhM+6RFH+qRrP/8sTcp++kl+CejdyZOy1uDwYcDXV3U0ZAUc6ZOpNG8uPeiNMpPnrbfk7oQJn/SK\nI33Svbw8oH17qe03bqw6mvLl5l6P00jPIMjY7M2dTPpkCC++KBuRzJypOpLyjR8PNGgAJCaqjoSs\nhEmfTOnXX6VWvncvEBCgOprbZWYC998PHDkCNGyoOhqyEtb0yZR8fIBnnwVeeUV1JGV75RVZfcuE\nT3rHkT4ZxrlzQEgIsHo10Lmz6miu27YNiI+XGTu1a6uOhqyGI30yrbp1gddfB55/XjpY6kFJiYzw\nZ8xgwidjYNInQ3niCeDyZWDxYtWRiPnzgZo1gdGjVUdCVDUs75DhbNsmc/cPHFC76vXsWaBNG2DN\nGuC++9TFQdbG2TtkCc8+C1y9Csydqy6GJ56QfW/NsOELGReTPlnCuXNA27bAJ58Afft6/vqpqfKL\n58cfgTp1PH99olJ8kEuWULcu8NFHwNNPA+fPe/bav/8uC7HmzmXCJ+PhSJ8M7amngCtXgAULPLND\nlabJ8wQfH2D2bPdfj6gyHOmTpcyaBezaJbNoPGHuXCAjw7ibuxBxpE+Gd+AA0KsXsH49EB7uvut8\n951s2r5tm2zuQqQHHOmT5bRpI/X96GggJ8c918jKAmJj5cExEz4ZmZfqAIhcIS4OyM4GBg8GtmwB\n6td33blPn5bzvvyyjPSJjIwjfTKNKVOA/v2BPn0kUbvCqVNyvthYYNIk15yTSCWHk/7nn3+ONm3a\noHr16ti9e3e5x6WmpiIkJARBQUGYMWOGo5cjqpTNJvvTDhwoNf5ffnHufEePAj17AjEx0vOHyAwc\nTvrt2rXDypUr0bNnz3KPKSkpwaRJk5CamoqDBw8iKSkJhw4dcvSSlpGWlqY6BN2w92dhswFvvAGM\nHQt07Qps3OjYdVNTge7dpZna6697ZjpoZfi+uI4/C8c5nPRDQkIQXMkTrfT0dAQGBqJFixbw9vZG\nfHw8Vq9e7eglLYNv6Osc/VlMmQIsWgSMGQM884z0yamK/Hxg3DhZbbt4MTBhgkOXdwu+L67jz8Jx\nbq3p5+XlIeCGbY78/f2Rl5fnzksSXdOnj0znrFEDCAwEXngB2LlTevbcSNOA9HRg8mTZnat2bWDf\nPiAyUknYRG5V4eydqKgonDx58rY/nz59OqKjoys9uU0P98RkafXry8rZqVOB998HHn9cRvOBgbKf\n7blzwMGDsuH6ww9Lsvf3Vx01kRtpToqMjNR27dpV5t99++232oABA659P336dC0xMbHMY1u3bq0B\n4Be/+MUvftnx1bp1a7tytkvm6WvlrAaLiIhARkYGsrOz0bRpUyxZsgRJSUllHpuZmemKUIiIqAIO\n1/RXrlyJgIAA7NixA4MHD8bAgQMBAMePH8fgwYMBAF5eXpg9ezYGDBiAsLAwjBo1CqGhoa6JnIiI\n7Kab3jtEROR+ylfkcvGWyMnJQe/evdGmTRu0bdsWs2bNUh2SciUlJQgPD6/SpAEzKygoQFxcHEJD\nQxEWFoYdO3aoDkmZN998E23atEG7du3wyCOP4PLly6pD8pixY8fC19cX7dq1u/ZnZ8+eRVRUFIKD\ng9G/f38UFBRUeh6lSZ+Lt67z9vbGu+++iwMHDmDHjh14//33LfuzKDVz5kyEhYVZfhbY5MmTMWjQ\nIBw6dAj79u2zbIk0OzsbH3/8MXbv3o39+/ejpKQEixcvVh2Wxzz55JNITU296c8SExMRFRWFI0eO\noG/fvkhMTKz0PEqTPhdvXde4cWN07NgRAFCnTh2Ehobi+PHjiqNSJzc3FykpKRg3bpylW27//vvv\n2Lp1K8aOHQtAnpPVU7kbvEJ169aFt7c3CgsLUVxcjMLCQvj5+akOy2N69OiBBg0a3PRnycnJSEhI\nAAAkJCRg1apVlZ5HadLn4q2yZWdnY8+ePejatavqUJR54YUX8Pe//x3VqimvQCqVlZWFe+65B08+\n+SQ6deqEp59+GoWFharDUqJhw4aYMmUKmjVrhqZNm6J+/fro16+f6rCUys/Ph6+vLwDA19cX+fn5\nlb5G6SfK6rftZTl//jzi4uIwc+ZM1LHoBqxr1qyBj48PwsPDLT3KB4Di4mLs3r0bEydOxO7du1G7\ndu0q3cKb0dGjR/HPf/4T2dnZOH78OM6fP49FixapDks3bDZblXKq0qTv5+eHnBt2vcjJyYG/hZdD\nXrlyBSNGjMBjjz2GYcOGqQ5Hme3btyM5ORktW7bE6NGjsWnTJowZM0Z1WEr4+/vD398fnTt3BgDE\nxcVV2NXWzHbu3Ilu3brh7rvvhpeXF4YPH47t27erDkspX1/fa10TTpw4AR8fn0pfozTp37h4q6io\nCEuWLEFMTIzKkJTRNA1PPfUUwsLC8Pzzz6sOR6np06cjJycHWVlZWLx4Mfr06YOFCxeqDkuJxo0b\nIyAgAEeOHAEAbNiwAW3atFEclRohISHYsWMHLl68CE3TsGHDBoSFhakOS6mYmBgsWLAAALBgwYKq\nDRbtWr/rBikpKVpwcLDWunVrbfr06arDUWbr1q2azWbTOnTooHXs2FHr2LGjtnbtWtVhKZeWlqZF\nR0erDkOpvXv3ahEREVr79u212NhYraCgQHVIysyYMUMLCwvT2rZtq40ZM0YrKipSHZLHxMfHa02a\nNNG8vb01f39/7V//+pd25swZrW/fvlpQUJAWFRWl/fbbb5Weh4uziIgsxNpTI4iILIZJn4jIQpj0\niYgshEmfiMhCmPSJiCyESZ+IyEKY9ImILIRJn4jIQv4fvo4w8qUQNX8AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x107c07b10>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(np.random)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on package numpy.random in numpy:\n",
        "\n",
        "NAME\n",
        "    numpy.random\n",
        "\n",
        "FILE\n",
        "    /Users/bri/anaconda/lib/python2.7/site-packages/numpy/random/__init__.py\n",
        "\n",
        "DESCRIPTION\n",
        "    ========================\n",
        "    Random Number Generation\n",
        "    ========================\n",
        "    \n",
        "    ==================== =========================================================\n",
        "    Utility functions\n",
        "    ==============================================================================\n",
        "    random               Uniformly distributed values of a given shape.\n",
        "    bytes                Uniformly distributed random bytes.\n",
        "    random_integers      Uniformly distributed integers in a given range.\n",
        "    random_sample        Uniformly distributed floats in a given range.\n",
        "    permutation          Randomly permute a sequence / generate a random sequence.\n",
        "    shuffle              Randomly permute a sequence in place.\n",
        "    seed                 Seed the random number generator.\n",
        "    ==================== =========================================================\n",
        "    \n",
        "    ==================== =========================================================\n",
        "    Compatibility functions\n",
        "    ==============================================================================\n",
        "    rand                 Uniformly distributed values.\n",
        "    randn                Normally distributed values.\n",
        "    ranf                 Uniformly distributed floating point numbers.\n",
        "    randint              Uniformly distributed integers in a given range.\n",
        "    ==================== =========================================================\n",
        "    \n",
        "    ==================== =========================================================\n",
        "    Univariate distributions\n",
        "    ==============================================================================\n",
        "    beta                 Beta distribution over ``[0, 1]``.\n",
        "    binomial             Binomial distribution.\n",
        "    chisquare            :math:`\\chi^2` distribution.\n",
        "    exponential          Exponential distribution.\n",
        "    f                    F (Fisher-Snedecor) distribution.\n",
        "    gamma                Gamma distribution.\n",
        "    geometric            Geometric distribution.\n",
        "    gumbel               Gumbel distribution.\n",
        "    hypergeometric       Hypergeometric distribution.\n",
        "    laplace              Laplace distribution.\n",
        "    logistic             Logistic distribution.\n",
        "    lognormal            Log-normal distribution.\n",
        "    logseries            Logarithmic series distribution.\n",
        "    negative_binomial    Negative binomial distribution.\n",
        "    noncentral_chisquare Non-central chi-square distribution.\n",
        "    noncentral_f         Non-central F distribution.\n",
        "    normal               Normal / Gaussian distribution.\n",
        "    pareto               Pareto distribution.\n",
        "    poisson              Poisson distribution.\n",
        "    power                Power distribution.\n",
        "    rayleigh             Rayleigh distribution.\n",
        "    triangular           Triangular distribution.\n",
        "    uniform              Uniform distribution.\n",
        "    vonmises             Von Mises circular distribution.\n",
        "    wald                 Wald (inverse Gaussian) distribution.\n",
        "    weibull              Weibull distribution.\n",
        "    zipf                 Zipf's distribution over ranked data.\n",
        "    ==================== =========================================================\n",
        "    \n",
        "    ==================== =========================================================\n",
        "    Multivariate distributions\n",
        "    ==============================================================================\n",
        "    dirichlet            Multivariate generalization of Beta distribution.\n",
        "    multinomial          Multivariate generalization of the binomial distribution.\n",
        "    multivariate_normal  Multivariate generalization of the normal distribution.\n",
        "    ==================== =========================================================\n",
        "    \n",
        "    ==================== =========================================================\n",
        "    Standard distributions\n",
        "    ==============================================================================\n",
        "    standard_cauchy      Standard Cauchy-Lorentz distribution.\n",
        "    standard_exponential Standard exponential distribution.\n",
        "    standard_gamma       Standard Gamma distribution.\n",
        "    standard_normal      Standard normal distribution.\n",
        "    standard_t           Standard Student's t-distribution.\n",
        "    ==================== =========================================================\n",
        "    \n",
        "    ==================== =========================================================\n",
        "    Internal functions\n",
        "    ==============================================================================\n",
        "    get_state            Get tuple representing internal state of generator.\n",
        "    set_state            Set state of generator.\n",
        "    ==================== =========================================================\n",
        "\n",
        "PACKAGE CONTENTS\n",
        "    info\n",
        "    mtrand\n",
        "    setup\n",
        "    setupscons\n",
        "\n",
        "FUNCTIONS\n",
        "    beta(...)\n",
        "        beta(a, b, size=None)\n",
        "        \n",
        "        The Beta distribution over ``[0, 1]``.\n",
        "        \n",
        "        The Beta distribution is a special case of the Dirichlet distribution,\n",
        "        and is related to the Gamma distribution.  It has the probability\n",
        "        distribution function\n",
        "        \n",
        "        .. math:: f(x; a,b) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha - 1}\n",
        "                                                         (1 - x)^{\\beta - 1},\n",
        "        \n",
        "        where the normalisation, B, is the beta function,\n",
        "        \n",
        "        .. math:: B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha - 1}\n",
        "                                     (1 - t)^{\\beta - 1} dt.\n",
        "        \n",
        "        It is often seen in Bayesian inference and order statistics.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        a : float\n",
        "            Alpha, non-negative.\n",
        "        b : float\n",
        "            Beta, non-negative.\n",
        "        size : tuple of ints, optional\n",
        "            The number of samples to draw.  The ouput is packed according to\n",
        "            the size given.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Array of the given shape, containing values drawn from a\n",
        "            Beta distribution.\n",
        "    \n",
        "    binomial(...)\n",
        "        binomial(n, p, size=None)\n",
        "        \n",
        "        Draw samples from a binomial distribution.\n",
        "        \n",
        "        Samples are drawn from a Binomial distribution with specified\n",
        "        parameters, n trials and p probability of success where\n",
        "        n an integer > 0 and p is in the interval [0,1]. (n may be\n",
        "        input as a float, but it is truncated to an integer in use)\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n : float (but truncated to an integer)\n",
        "                parameter, > 0.\n",
        "        p : float\n",
        "                parameter, >= 0 and <=1.\n",
        "        size : {tuple, int}\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : {ndarray, scalar}\n",
        "                  where the values are all integers in  [0, n].\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.binom : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Binomial distribution is\n",
        "        \n",
        "        .. math:: P(N) = \\binom{n}{N}p^N(1-p)^{n-N},\n",
        "        \n",
        "        where :math:`n` is the number of trials, :math:`p` is the probability\n",
        "        of success, and :math:`N` is the number of successes.\n",
        "        \n",
        "        When estimating the standard error of a proportion in a population by\n",
        "        using a random sample, the normal distribution works well unless the\n",
        "        product p*n <=5, where p = population proportion estimate, and n =\n",
        "        number of samples, in which case the binomial distribution is used\n",
        "        instead. For example, a sample of 15 people shows 4 who are left\n",
        "        handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27*15 = 4,\n",
        "        so the binomial distribution should be used in this case.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Dalgaard, Peter, \"Introductory Statistics with R\",\n",
        "               Springer-Verlag, 2002.\n",
        "        .. [2] Glantz, Stanton A. \"Primer of Biostatistics.\", McGraw-Hill,\n",
        "               Fifth Edition, 2002.\n",
        "        .. [3] Lentner, Marvin, \"Elementary Applied Statistics\", Bogden\n",
        "               and Quigley, 1972.\n",
        "        .. [4] Weisstein, Eric W. \"Binomial Distribution.\" From MathWorld--A\n",
        "               Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/BinomialDistribution.html\n",
        "        .. [5] Wikipedia, \"Binomial-distribution\",\n",
        "               http://en.wikipedia.org/wiki/Binomial_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> n, p = 10, .5 # number of trials, probability of each trial\n",
        "        >>> s = np.random.binomial(n, p, 1000)\n",
        "        # result of flipping a coin 10 times, tested 1000 times.\n",
        "        \n",
        "        A real world example. A company drills 9 wild-cat oil exploration\n",
        "        wells, each with an estimated probability of success of 0.1. All nine\n",
        "        wells fail. What is the probability of that happening?\n",
        "        \n",
        "        Let's do 20,000 trials of the model, and count the number that\n",
        "        generate zero positive results.\n",
        "        \n",
        "        >>> sum(np.random.binomial(9,0.1,20000)==0)/20000.\n",
        "        answer = 0.38885, or 38%.\n",
        "    \n",
        "    bytes(...)\n",
        "        bytes(length)\n",
        "        \n",
        "        Return random bytes.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        length : int\n",
        "            Number of random bytes.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : str\n",
        "            String of length `length`.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.bytes(10)\n",
        "        ' eh\\x85\\x022SZ\\xbf\\xa4' #random\n",
        "    \n",
        "    chisquare(...)\n",
        "        chisquare(df, size=None)\n",
        "        \n",
        "        Draw samples from a chi-square distribution.\n",
        "        \n",
        "        When `df` independent random variables, each with standard normal\n",
        "        distributions (mean 0, variance 1), are squared and summed, the\n",
        "        resulting distribution is chi-square (see Notes).  This distribution\n",
        "        is often used in hypothesis testing.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        df : int\n",
        "             Number of degrees of freedom.\n",
        "        size : tuple of ints, int, optional\n",
        "             Size of the returned array.  By default, a scalar is\n",
        "             returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        output : ndarray\n",
        "            Samples drawn from the distribution, packed in a `size`-shaped\n",
        "            array.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            When `df` <= 0 or when an inappropriate `size` (e.g. ``size=-1``)\n",
        "            is given.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The variable obtained by summing the squares of `df` independent,\n",
        "        standard normally distributed random variables:\n",
        "        \n",
        "        .. math:: Q = \\sum_{i=0}^{\\mathtt{df}} X^2_i\n",
        "        \n",
        "        is chi-square distributed, denoted\n",
        "        \n",
        "        .. math:: Q \\sim \\chi^2_k.\n",
        "        \n",
        "        The probability density function of the chi-squared distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{(1/2)^{k/2}}{\\Gamma(k/2)}\n",
        "                         x^{k/2 - 1} e^{-x/2},\n",
        "        \n",
        "        where :math:`\\Gamma` is the gamma function,\n",
        "        \n",
        "        .. math:: \\Gamma(x) = \\int_0^{-\\infty} t^{x - 1} e^{-t} dt.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        `NIST/SEMATECH e-Handbook of Statistical Methods\n",
        "        <http://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm>`_\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.chisquare(2,4)\n",
        "        array([ 1.89920014,  9.00867716,  3.13710533,  5.62318272])\n",
        "    \n",
        "    exponential(...)\n",
        "        exponential(scale=1.0, size=None)\n",
        "        \n",
        "        Exponential distribution.\n",
        "        \n",
        "        Its probability density function is\n",
        "        \n",
        "        .. math:: f(x; \\frac{1}{\\beta}) = \\frac{1}{\\beta} \\exp(-\\frac{x}{\\beta}),\n",
        "        \n",
        "        for ``x > 0`` and 0 elsewhere. :math:`\\beta` is the scale parameter,\n",
        "        which is the inverse of the rate parameter :math:`\\lambda = 1/\\beta`.\n",
        "        The rate parameter is an alternative, widely used parameterization\n",
        "        of the exponential distribution [3]_.\n",
        "        \n",
        "        The exponential distribution is a continuous analogue of the\n",
        "        geometric distribution.  It describes many common situations, such as\n",
        "        the size of raindrops measured over many rainstorms [1]_, or the time\n",
        "        between page requests to Wikipedia [2]_.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        scale : float\n",
        "            The scale parameter, :math:`\\beta = 1/\\lambda`.\n",
        "        size : tuple of ints\n",
        "            Number of samples to draw.  The output is shaped\n",
        "            according to `size`.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Peyton Z. Peebles Jr., \"Probability, Random Variables and\n",
        "               Random Signal Principles\", 4th ed, 2001, p. 57.\n",
        "        .. [2] \"Poisson Process\", Wikipedia,\n",
        "               http://en.wikipedia.org/wiki/Poisson_process\n",
        "        .. [3] \"Exponential Distribution, Wikipedia,\n",
        "               http://en.wikipedia.org/wiki/Exponential_distribution\n",
        "    \n",
        "    f(...)\n",
        "        f(dfnum, dfden, size=None)\n",
        "        \n",
        "        Draw samples from a F distribution.\n",
        "        \n",
        "        Samples are drawn from an F distribution with specified parameters,\n",
        "        `dfnum` (degrees of freedom in numerator) and `dfden` (degrees of freedom\n",
        "        in denominator), where both parameters should be greater than zero.\n",
        "        \n",
        "        The random variate of the F distribution (also known as the\n",
        "        Fisher distribution) is a continuous probability distribution\n",
        "        that arises in ANOVA tests, and is the ratio of two chi-square\n",
        "        variates.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dfnum : float\n",
        "            Degrees of freedom in numerator. Should be greater than zero.\n",
        "        dfden : float\n",
        "            Degrees of freedom in denominator. Should be greater than zero.\n",
        "        size : {tuple, int}, optional\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``,\n",
        "            then ``m * n * k`` samples are drawn. By default only one sample\n",
        "            is returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : {ndarray, scalar}\n",
        "            Samples from the Fisher distribution.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.f : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \n",
        "        The F statistic is used to compare in-group variances to between-group\n",
        "        variances. Calculating the distribution depends on the sampling, and\n",
        "        so it is a function of the respective degrees of freedom in the\n",
        "        problem.  The variable `dfnum` is the number of samples minus one, the\n",
        "        between-groups degrees of freedom, while `dfden` is the within-groups\n",
        "        degrees of freedom, the sum of the number of samples in each group\n",
        "        minus the number of groups.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Glantz, Stanton A. \"Primer of Biostatistics.\", McGraw-Hill,\n",
        "               Fifth Edition, 2002.\n",
        "        .. [2] Wikipedia, \"F-distribution\",\n",
        "               http://en.wikipedia.org/wiki/F-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        An example from Glantz[1], pp 47-40.\n",
        "        Two groups, children of diabetics (25 people) and children from people\n",
        "        without diabetes (25 controls). Fasting blood glucose was measured,\n",
        "        case group had a mean value of 86.1, controls had a mean value of\n",
        "        82.2. Standard deviations were 2.09 and 2.49 respectively. Are these\n",
        "        data consistent with the null hypothesis that the parents diabetic\n",
        "        status does not affect their children's blood glucose levels?\n",
        "        Calculating the F statistic from the data gives a value of 36.01.\n",
        "        \n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> dfnum = 1. # between group degrees of freedom\n",
        "        >>> dfden = 48. # within groups degrees of freedom\n",
        "        >>> s = np.random.f(dfnum, dfden, 1000)\n",
        "        \n",
        "        The lower bound for the top 1% of the samples is :\n",
        "        \n",
        "        >>> sort(s)[-10]\n",
        "        7.61988120985\n",
        "        \n",
        "        So there is about a 1% chance that the F statistic will exceed 7.62,\n",
        "        the measured value is 36, so the null hypothesis is rejected at the 1%\n",
        "        level.\n",
        "    \n",
        "    gamma(...)\n",
        "        gamma(shape, scale=1.0, size=None)\n",
        "        \n",
        "        Draw samples from a Gamma distribution.\n",
        "        \n",
        "        Samples are drawn from a Gamma distribution with specified parameters,\n",
        "        `shape` (sometimes designated \"k\") and `scale` (sometimes designated\n",
        "        \"theta\"), where both parameters are > 0.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : scalar > 0\n",
        "            The shape of the gamma distribution.\n",
        "        scale : scalar > 0, optional\n",
        "            The scale of the gamma distribution.  Default is equal to 1.\n",
        "        size : shape_tuple, optional\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray, float\n",
        "            Returns one sample unless `size` parameter is specified.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.gamma : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Gamma distribution is\n",
        "        \n",
        "        .. math:: p(x) = x^{k-1}\\frac{e^{-x/\\theta}}{\\theta^k\\Gamma(k)},\n",
        "        \n",
        "        where :math:`k` is the shape and :math:`\\theta` the scale,\n",
        "        and :math:`\\Gamma` is the Gamma function.\n",
        "        \n",
        "        The Gamma distribution is often used to model the times to failure of\n",
        "        electronic components, and arises naturally in processes for which the\n",
        "        waiting times between Poisson distributed events are relevant.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Weisstein, Eric W. \"Gamma Distribution.\" From MathWorld--A\n",
        "               Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/GammaDistribution.html\n",
        "        .. [2] Wikipedia, \"Gamma-distribution\",\n",
        "               http://en.wikipedia.org/wiki/Gamma-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> shape, scale = 2., 2. # mean and dispersion\n",
        "        >>> s = np.random.gamma(shape, scale, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> import scipy.special as sps\n",
        "        >>> count, bins, ignored = plt.hist(s, 50, normed=True)\n",
        "        >>> y = bins**(shape-1)*(np.exp(-bins/scale) /\n",
        "        ...                      (sps.gamma(shape)*scale**shape))\n",
        "        >>> plt.plot(bins, y, linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    geometric(...)\n",
        "        geometric(p, size=None)\n",
        "        \n",
        "        Draw samples from the geometric distribution.\n",
        "        \n",
        "        Bernoulli trials are experiments with one of two outcomes:\n",
        "        success or failure (an example of such an experiment is flipping\n",
        "        a coin).  The geometric distribution models the number of trials\n",
        "        that must be run in order to achieve success.  It is therefore\n",
        "        supported on the positive integers, ``k = 1, 2, ...``.\n",
        "        \n",
        "        The probability mass function of the geometric distribution is\n",
        "        \n",
        "        .. math:: f(k) = (1 - p)^{k - 1} p\n",
        "        \n",
        "        where `p` is the probability of success of an individual trial.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        p : float\n",
        "            The probability of success of an individual trial.\n",
        "        size : tuple of ints\n",
        "            Number of values to draw from the distribution.  The output\n",
        "            is shaped according to `size`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Samples from the geometric distribution, shaped according to\n",
        "            `size`.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw ten thousand values from the geometric distribution,\n",
        "        with the probability of an individual success equal to 0.35:\n",
        "        \n",
        "        >>> z = np.random.geometric(p=0.35, size=10000)\n",
        "        \n",
        "        How many trials succeeded after a single run?\n",
        "        \n",
        "        >>> (z == 1).sum() / 10000.\n",
        "        0.34889999999999999 #random\n",
        "    \n",
        "    get_state(...)\n",
        "        get_state()\n",
        "        \n",
        "        Return a tuple representing the internal state of the generator.\n",
        "        \n",
        "        For more details, see `set_state`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : tuple(str, ndarray of 624 uints, int, int, float)\n",
        "            The returned tuple has the following items:\n",
        "        \n",
        "            1. the string 'MT19937'.\n",
        "            2. a 1-D array of 624 unsigned integer keys.\n",
        "            3. an integer ``pos``.\n",
        "            4. an integer ``has_gauss``.\n",
        "            5. a float ``cached_gaussian``.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        set_state\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        `set_state` and `get_state` are not needed to work with any of the\n",
        "        random distributions in NumPy. If the internal state is manually altered,\n",
        "        the user should know exactly what he/she is doing.\n",
        "    \n",
        "    gumbel(...)\n",
        "        gumbel(loc=0.0, scale=1.0, size=None)\n",
        "        \n",
        "        Gumbel distribution.\n",
        "        \n",
        "        Draw samples from a Gumbel distribution with specified location and scale.\n",
        "        For more information on the Gumbel distribution, see Notes and References\n",
        "        below.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        loc : float\n",
        "            The location of the mode of the distribution.\n",
        "        scale : float\n",
        "            The scale parameter of the distribution.\n",
        "        size : tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            The samples\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.gumbel_l\n",
        "        scipy.stats.gumbel_r\n",
        "        scipy.stats.genextreme\n",
        "            probability density function, distribution, or cumulative density\n",
        "            function, etc. for each of the above\n",
        "        weibull\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The Gumbel (or Smallest Extreme Value (SEV) or the Smallest Extreme Value\n",
        "        Type I) distribution is one of a class of Generalized Extreme Value (GEV)\n",
        "        distributions used in modeling extreme value problems.  The Gumbel is a\n",
        "        special case of the Extreme Value Type I distribution for maximums from\n",
        "        distributions with \"exponential-like\" tails.\n",
        "        \n",
        "        The probability density for the Gumbel distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{e^{-(x - \\mu)/ \\beta}}{\\beta} e^{ -e^{-(x - \\mu)/\n",
        "                  \\beta}},\n",
        "        \n",
        "        where :math:`\\mu` is the mode, a location parameter, and :math:`\\beta` is\n",
        "        the scale parameter.\n",
        "        \n",
        "        The Gumbel (named for German mathematician Emil Julius Gumbel) was used\n",
        "        very early in the hydrology literature, for modeling the occurrence of\n",
        "        flood events. It is also used for modeling maximum wind speed and rainfall\n",
        "        rates.  It is a \"fat-tailed\" distribution - the probability of an event in\n",
        "        the tail of the distribution is larger than if one used a Gaussian, hence\n",
        "        the surprisingly frequent occurrence of 100-year floods. Floods were\n",
        "        initially modeled as a Gaussian process, which underestimated the frequency\n",
        "        of extreme events.\n",
        "        \n",
        "        \n",
        "        It is one of a class of extreme value distributions, the Generalized\n",
        "        Extreme Value (GEV) distributions, which also includes the Weibull and\n",
        "        Frechet.\n",
        "        \n",
        "        The function has a mean of :math:`\\mu + 0.57721\\beta` and a variance of\n",
        "        :math:`\\frac{\\pi^2}{6}\\beta^2`.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Gumbel, E. J., *Statistics of Extremes*, New York: Columbia University\n",
        "        Press, 1958.\n",
        "        \n",
        "        Reiss, R.-D. and Thomas, M., *Statistical Analysis of Extreme Values from\n",
        "        Insurance, Finance, Hydrology and Other Fields*, Basel: Birkhauser Verlag,\n",
        "        2001.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> mu, beta = 0, 0.1 # location and scale\n",
        "        >>> s = np.random.gumbel(mu, beta, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 30, normed=True)\n",
        "        >>> plt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n",
        "        ...          * np.exp( -np.exp( -(bins - mu) /beta) ),\n",
        "        ...          linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "        \n",
        "        Show how an extreme value distribution can arise from a Gaussian process\n",
        "        and compare to a Gaussian:\n",
        "        \n",
        "        >>> means = []\n",
        "        >>> maxima = []\n",
        "        >>> for i in range(0,1000) :\n",
        "        ...    a = np.random.normal(mu, beta, 1000)\n",
        "        ...    means.append(a.mean())\n",
        "        ...    maxima.append(a.max())\n",
        "        >>> count, bins, ignored = plt.hist(maxima, 30, normed=True)\n",
        "        >>> beta = np.std(maxima)*np.pi/np.sqrt(6)\n",
        "        >>> mu = np.mean(maxima) - 0.57721*beta\n",
        "        >>> plt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n",
        "        ...          * np.exp(-np.exp(-(bins - mu)/beta)),\n",
        "        ...          linewidth=2, color='r')\n",
        "        >>> plt.plot(bins, 1/(beta * np.sqrt(2 * np.pi))\n",
        "        ...          * np.exp(-(bins - mu)**2 / (2 * beta**2)),\n",
        "        ...          linewidth=2, color='g')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    hypergeometric(...)\n",
        "        hypergeometric(ngood, nbad, nsample, size=None)\n",
        "        \n",
        "        Draw samples from a Hypergeometric distribution.\n",
        "        \n",
        "        Samples are drawn from a Hypergeometric distribution with specified\n",
        "        parameters, ngood (ways to make a good selection), nbad (ways to make\n",
        "        a bad selection), and nsample = number of items sampled, which is less\n",
        "        than or equal to the sum ngood + nbad.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        ngood : float (but truncated to an integer)\n",
        "                parameter, > 0.\n",
        "        nbad  : float\n",
        "                parameter, >= 0.\n",
        "        nsample  : float\n",
        "                   parameter, > 0 and <= ngood+nbad\n",
        "        size : {tuple, int}\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : {ndarray, scalar}\n",
        "                  where the values are all integers in  [0, n].\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.hypergeom : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Hypergeometric distribution is\n",
        "        \n",
        "        .. math:: P(x) = \\frac{\\binom{m}{n}\\binom{N-m}{n-x}}{\\binom{N}{n}},\n",
        "        \n",
        "        where :math:`0 \\le x \\le m` and :math:`n+m-N \\le x \\le n`\n",
        "        \n",
        "        for P(x) the probability of x successes, n = ngood, m = nbad, and\n",
        "        N = number of samples.\n",
        "        \n",
        "        Consider an urn with black and white marbles in it, ngood of them\n",
        "        black and nbad are white. If you draw nsample balls without\n",
        "        replacement, then the Hypergeometric distribution describes the\n",
        "        distribution of black balls in the drawn sample.\n",
        "        \n",
        "        Note that this distribution is very similar to the Binomial\n",
        "        distribution, except that in this case, samples are drawn without\n",
        "        replacement, whereas in the Binomial case samples are drawn with\n",
        "        replacement (or the sample space is infinite). As the sample space\n",
        "        becomes large, this distribution approaches the Binomial.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Lentner, Marvin, \"Elementary Applied Statistics\", Bogden\n",
        "               and Quigley, 1972.\n",
        "        .. [2] Weisstein, Eric W. \"Hypergeometric Distribution.\" From\n",
        "               MathWorld--A Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/HypergeometricDistribution.html\n",
        "        .. [3] Wikipedia, \"Hypergeometric-distribution\",\n",
        "               http://en.wikipedia.org/wiki/Hypergeometric-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> ngood, nbad, nsamp = 100, 2, 10\n",
        "        # number of good, number of bad, and number of samples\n",
        "        >>> s = np.random.hypergeometric(ngood, nbad, nsamp, 1000)\n",
        "        >>> hist(s)\n",
        "        #   note that it is very unlikely to grab both bad items\n",
        "        \n",
        "        Suppose you have an urn with 15 white and 15 black marbles.\n",
        "        If you pull 15 marbles at random, how likely is it that\n",
        "        12 or more of them are one color?\n",
        "        \n",
        "        >>> s = np.random.hypergeometric(15, 15, 15, 100000)\n",
        "        >>> sum(s>=12)/100000. + sum(s<=3)/100000.\n",
        "        #   answer = 0.003 ... pretty unlikely!\n",
        "    \n",
        "    laplace(...)\n",
        "        laplace(loc=0.0, scale=1.0, size=None)\n",
        "        \n",
        "        Draw samples from the Laplace or double exponential distribution with\n",
        "        specified location (or mean) and scale (decay).\n",
        "        \n",
        "        The Laplace distribution is similar to the Gaussian/normal distribution,\n",
        "        but is sharper at the peak and has fatter tails. It represents the\n",
        "        difference between two independent, identically distributed exponential\n",
        "        random variables.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        loc : float\n",
        "            The position, :math:`\\mu`, of the distribution peak.\n",
        "        scale : float\n",
        "            :math:`\\lambda`, the exponential decay.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        It has the probability density function\n",
        "        \n",
        "        .. math:: f(x; \\mu, \\lambda) = \\frac{1}{2\\lambda}\n",
        "                                       \\exp\\left(-\\frac{|x - \\mu|}{\\lambda}\\right).\n",
        "        \n",
        "        The first law of Laplace, from 1774, states that the frequency of an error\n",
        "        can be expressed as an exponential function of the absolute magnitude of\n",
        "        the error, which leads to the Laplace distribution. For many problems in\n",
        "        Economics and Health sciences, this distribution seems to model the data\n",
        "        better than the standard Gaussian distribution\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Abramowitz, M. and Stegun, I. A. (Eds.). Handbook of Mathematical\n",
        "               Functions with Formulas, Graphs, and Mathematical Tables, 9th\n",
        "               printing.  New York: Dover, 1972.\n",
        "        \n",
        "        .. [2] The Laplace distribution and generalizations\n",
        "               By Samuel Kotz, Tomasz J. Kozubowski, Krzysztof Podgorski,\n",
        "               Birkhauser, 2001.\n",
        "        \n",
        "        .. [3] Weisstein, Eric W. \"Laplace Distribution.\"\n",
        "               From MathWorld--A Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/LaplaceDistribution.html\n",
        "        \n",
        "        .. [4] Wikipedia, \"Laplace distribution\",\n",
        "               http://en.wikipedia.org/wiki/Laplace_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution\n",
        "        \n",
        "        >>> loc, scale = 0., 1.\n",
        "        >>> s = np.random.laplace(loc, scale, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 30, normed=True)\n",
        "        >>> x = np.arange(-8., 8., .01)\n",
        "        >>> pdf = np.exp(-abs(x-loc/scale))/(2.*scale)\n",
        "        >>> plt.plot(x, pdf)\n",
        "        \n",
        "        Plot Gaussian for comparison:\n",
        "        \n",
        "        >>> g = (1/(scale * np.sqrt(2 * np.pi)) * \n",
        "        ...      np.exp( - (x - loc)**2 / (2 * scale**2) ))\n",
        "        >>> plt.plot(x,g)\n",
        "    \n",
        "    logistic(...)\n",
        "        logistic(loc=0.0, scale=1.0, size=None)\n",
        "        \n",
        "        Draw samples from a Logistic distribution.\n",
        "        \n",
        "        Samples are drawn from a Logistic distribution with specified\n",
        "        parameters, loc (location or mean, also median), and scale (>0).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        loc : float\n",
        "        \n",
        "        scale : float > 0.\n",
        "        \n",
        "        size : {tuple, int}\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : {ndarray, scalar}\n",
        "                  where the values are all integers in  [0, n].\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.logistic : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Logistic distribution is\n",
        "        \n",
        "        .. math:: P(x) = P(x) = \\frac{e^{-(x-\\mu)/s}}{s(1+e^{-(x-\\mu)/s})^2},\n",
        "        \n",
        "        where :math:`\\mu` = location and :math:`s` = scale.\n",
        "        \n",
        "        The Logistic distribution is used in Extreme Value problems where it\n",
        "        can act as a mixture of Gumbel distributions, in Epidemiology, and by\n",
        "        the World Chess Federation (FIDE) where it is used in the Elo ranking\n",
        "        system, assuming the performance of each player is a logistically\n",
        "        distributed random variable.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Reiss, R.-D. and Thomas M. (2001), Statistical Analysis of Extreme\n",
        "               Values, from Insurance, Finance, Hydrology and Other Fields,\n",
        "               Birkhauser Verlag, Basel, pp 132-133.\n",
        "        .. [2] Weisstein, Eric W. \"Logistic Distribution.\" From\n",
        "               MathWorld--A Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/LogisticDistribution.html\n",
        "        .. [3] Wikipedia, \"Logistic-distribution\",\n",
        "               http://en.wikipedia.org/wiki/Logistic-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> loc, scale = 10, 1\n",
        "        >>> s = np.random.logistic(loc, scale, 10000)\n",
        "        >>> count, bins, ignored = plt.hist(s, bins=50)\n",
        "        \n",
        "        #   plot against distribution\n",
        "        \n",
        "        >>> def logist(x, loc, scale):\n",
        "        ...     return exp((loc-x)/scale)/(scale*(1+exp((loc-x)/scale))**2)\n",
        "        >>> plt.plot(bins, logist(bins, loc, scale)*count.max()/\\\n",
        "        ... logist(bins, loc, scale).max())\n",
        "        >>> plt.show()\n",
        "    \n",
        "    lognormal(...)\n",
        "        lognormal(mean=0.0, sigma=1.0, size=None)\n",
        "        \n",
        "        Return samples drawn from a log-normal distribution.\n",
        "        \n",
        "        Draw samples from a log-normal distribution with specified mean, standard\n",
        "        deviation, and shape. Note that the mean and standard deviation are not the\n",
        "        values for the distribution itself, but of the underlying normal\n",
        "        distribution it is derived from.\n",
        "        \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : float\n",
        "            Mean value of the underlying normal distribution\n",
        "        sigma : float, >0.\n",
        "            Standard deviation of the underlying normal distribution\n",
        "        size : tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.lognorm : probability density function, distribution,\n",
        "            cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        A variable `x` has a log-normal distribution if `log(x)` is normally\n",
        "        distributed.\n",
        "        \n",
        "        The probability density function for the log-normal distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{1}{\\sigma x \\sqrt{2\\pi}}\n",
        "                         e^{(-\\frac{(ln(x)-\\mu)^2}{2\\sigma^2})}\n",
        "        \n",
        "        where :math:`\\mu` is the mean and :math:`\\sigma` is the standard deviation\n",
        "        of the normally distributed logarithm of the variable.\n",
        "        \n",
        "        A log-normal distribution results if a random variable is the *product* of\n",
        "        a large number of independent, identically-distributed variables in the\n",
        "        same way that a normal distribution results if the variable is the *sum*\n",
        "        of a large number of independent, identically-distributed variables\n",
        "        (see the last example). It is one of the so-called \"fat-tailed\"\n",
        "        distributions.\n",
        "        \n",
        "        The log-normal distribution is commonly used to model the lifespan of units\n",
        "        with fatigue-stress failure modes. Since this includes\n",
        "        most mechanical systems, the log-normal distribution has widespread\n",
        "        application.\n",
        "        \n",
        "        It is also commonly used to model oil field sizes, species abundance, and\n",
        "        latent periods of infectious diseases.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Eckhard Limpert, Werner A. Stahel, and Markus Abbt, \"Log-normal\n",
        "               Distributions across the Sciences: Keys and Clues\", May 2001\n",
        "               Vol. 51 No. 5 BioScience\n",
        "               http://stat.ethz.ch/~stahel/lognormal/bioscience.pdf\n",
        "        .. [2] Reiss, R.D., Thomas, M.(2001), Statistical Analysis of Extreme\n",
        "               Values, Birkhauser Verlag, Basel, pp 31-32.\n",
        "        .. [3] Wikipedia, \"Lognormal distribution\",\n",
        "               http://en.wikipedia.org/wiki/Lognormal_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> mu, sigma = 3., 1. # mean and standard deviation\n",
        "        >>> s = np.random.lognormal(mu, sigma, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 100, normed=True, align='mid')\n",
        "        \n",
        "        >>> x = np.linspace(min(bins), max(bins), 10000)\n",
        "        >>> pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
        "        ...        / (x * sigma * np.sqrt(2 * np.pi)))\n",
        "        \n",
        "        >>> plt.plot(x, pdf, linewidth=2, color='r')\n",
        "        >>> plt.axis('tight')\n",
        "        >>> plt.show()\n",
        "        \n",
        "        Demonstrate that taking the products of random samples from a uniform\n",
        "        distribution can be fit well by a log-normal probability density function.\n",
        "        \n",
        "        >>> # Generate a thousand samples: each is the product of 100 random\n",
        "        >>> # values, drawn from a normal distribution.\n",
        "        >>> b = []\n",
        "        >>> for i in range(1000):\n",
        "        ...    a = 10. + np.random.random(100)\n",
        "        ...    b.append(np.product(a))\n",
        "        \n",
        "        >>> b = np.array(b) / np.min(b) # scale values to be positive\n",
        "        \n",
        "        >>> count, bins, ignored = plt.hist(b, 100, normed=True, align='center')\n",
        "        \n",
        "        >>> sigma = np.std(np.log(b))\n",
        "        >>> mu = np.mean(np.log(b))\n",
        "        \n",
        "        >>> x = np.linspace(min(bins), max(bins), 10000)\n",
        "        >>> pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
        "        ...        / (x * sigma * np.sqrt(2 * np.pi)))\n",
        "        \n",
        "        >>> plt.plot(x, pdf, color='r', linewidth=2)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    logseries(...)\n",
        "        logseries(p, size=None)\n",
        "        \n",
        "        Draw samples from a Logarithmic Series distribution.\n",
        "        \n",
        "        Samples are drawn from a Log Series distribution with specified\n",
        "        parameter, p (probability, 0 < p < 1).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        loc : float\n",
        "        \n",
        "        scale : float > 0.\n",
        "        \n",
        "        size : {tuple, int}\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : {ndarray, scalar}\n",
        "                  where the values are all integers in  [0, n].\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.logser : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Log Series distribution is\n",
        "        \n",
        "        .. math:: P(k) = \\frac{-p^k}{k \\ln(1-p)},\n",
        "        \n",
        "        where p = probability.\n",
        "        \n",
        "        The Log Series distribution is frequently used to represent species\n",
        "        richness and occurrence, first proposed by Fisher, Corbet, and\n",
        "        Williams in 1943 [2].  It may also be used to model the numbers of\n",
        "        occupants seen in cars [3].\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Buzas, Martin A.; Culver, Stephen J.,  Understanding regional\n",
        "               species diversity through the log series distribution of\n",
        "               occurrences: BIODIVERSITY RESEARCH Diversity & Distributions,\n",
        "               Volume 5, Number 5, September 1999 , pp. 187-195(9).\n",
        "        .. [2] Fisher, R.A,, A.S. Corbet, and C.B. Williams. 1943. The\n",
        "               relation between the number of species and the number of\n",
        "               individuals in a random sample of an animal population.\n",
        "               Journal of Animal Ecology, 12:42-58.\n",
        "        .. [3] D. J. Hand, F. Daly, D. Lunn, E. Ostrowski, A Handbook of Small\n",
        "               Data Sets, CRC Press, 1994.\n",
        "        .. [4] Wikipedia, \"Logarithmic-distribution\",\n",
        "               http://en.wikipedia.org/wiki/Logarithmic-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> a = .6\n",
        "        >>> s = np.random.logseries(a, 10000)\n",
        "        >>> count, bins, ignored = plt.hist(s)\n",
        "        \n",
        "        #   plot against distribution\n",
        "        \n",
        "        >>> def logseries(k, p):\n",
        "        ...     return -p**k/(k*log(1-p))\n",
        "        >>> plt.plot(bins, logseries(bins, a)*count.max()/\n",
        "                     logseries(bins, a).max(), 'r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    multinomial(...)\n",
        "        multinomial(n, pvals, size=None)\n",
        "        \n",
        "        Draw samples from a multinomial distribution.\n",
        "        \n",
        "        The multinomial distribution is a multivariate generalisation of the\n",
        "        binomial distribution.  Take an experiment with one of ``p``\n",
        "        possible outcomes.  An example of such an experiment is throwing a dice,\n",
        "        where the outcome can be 1 through 6.  Each sample drawn from the\n",
        "        distribution represents `n` such experiments.  Its values,\n",
        "        ``X_i = [X_0, X_1, ..., X_p]``, represent the number of times the outcome\n",
        "        was ``i``.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            Number of experiments.\n",
        "        pvals : sequence of floats, length p\n",
        "            Probabilities of each of the ``p`` different outcomes.  These\n",
        "            should sum to 1 (however, the last element is always assumed to\n",
        "            account for the remaining probability, as long as\n",
        "            ``sum(pvals[:-1]) <= 1)``.\n",
        "        size : tuple of ints\n",
        "            Given a `size` of ``(M, N, K)``, then ``M*N*K`` samples are drawn,\n",
        "            and the output shape becomes ``(M, N, K, p)``, since each sample\n",
        "            has shape ``(p,)``.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Throw a dice 20 times:\n",
        "        \n",
        "        >>> np.random.multinomial(20, [1/6.]*6, size=1)\n",
        "        array([[4, 1, 7, 5, 2, 1]])\n",
        "        \n",
        "        It landed 4 times on 1, once on 2, etc.\n",
        "        \n",
        "        Now, throw the dice 20 times, and 20 times again:\n",
        "        \n",
        "        >>> np.random.multinomial(20, [1/6.]*6, size=2)\n",
        "        array([[3, 4, 3, 3, 4, 3],\n",
        "               [2, 4, 3, 4, 0, 7]])\n",
        "        \n",
        "        For the first run, we threw 3 times 1, 4 times 2, etc.  For the second,\n",
        "        we threw 2 times 1, 4 times 2, etc.\n",
        "        \n",
        "        A loaded dice is more likely to land on number 6:\n",
        "        \n",
        "        >>> np.random.multinomial(100, [1/7.]*5)\n",
        "        array([13, 16, 13, 16, 42])\n",
        "    \n",
        "    multivariate_normal(...)\n",
        "        multivariate_normal(mean, cov[, size])\n",
        "        \n",
        "        Draw random samples from a multivariate normal distribution.\n",
        "        \n",
        "        The multivariate normal, multinormal or Gaussian distribution is a\n",
        "        generalization of the one-dimensional normal distribution to higher\n",
        "        dimensions.  Such a distribution is specified by its mean and\n",
        "        covariance matrix.  These parameters are analogous to the mean\n",
        "        (average or \"center\") and variance (standard deviation, or \"width,\"\n",
        "        squared) of the one-dimensional normal distribution.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : 1-D array_like, of length N\n",
        "            Mean of the N-dimensional distribution.\n",
        "        cov : 2-D array_like, of shape (N, N)\n",
        "            Covariance matrix of the distribution.  Must be symmetric and\n",
        "            positive semi-definite for \"physically meaningful\" results.\n",
        "        size : tuple of ints, optional\n",
        "            Given a shape of, for example, ``(m,n,k)``, ``m*n*k`` samples are\n",
        "            generated, and packed in an `m`-by-`n`-by-`k` arrangement.  Because\n",
        "            each sample is `N`-dimensional, the output shape is ``(m,n,k,N)``.\n",
        "            If no shape is specified, a single (`N`-D) sample is returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            The drawn samples, of shape *size*, if that was provided.  If not,\n",
        "            the shape is ``(N,)``.\n",
        "        \n",
        "            In other words, each entry ``out[i,j,...,:]`` is an N-dimensional\n",
        "            value drawn from the distribution.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The mean is a coordinate in N-dimensional space, which represents the\n",
        "        location where samples are most likely to be generated.  This is\n",
        "        analogous to the peak of the bell curve for the one-dimensional or\n",
        "        univariate normal distribution.\n",
        "        \n",
        "        Covariance indicates the level to which two variables vary together.\n",
        "        From the multivariate normal distribution, we draw N-dimensional\n",
        "        samples, :math:`X = [x_1, x_2, ... x_N]`.  The covariance matrix\n",
        "        element :math:`C_{ij}` is the covariance of :math:`x_i` and :math:`x_j`.\n",
        "        The element :math:`C_{ii}` is the variance of :math:`x_i` (i.e. its\n",
        "        \"spread\").\n",
        "        \n",
        "        Instead of specifying the full covariance matrix, popular\n",
        "        approximations include:\n",
        "        \n",
        "          - Spherical covariance (*cov* is a multiple of the identity matrix)\n",
        "          - Diagonal covariance (*cov* has non-negative elements, and only on\n",
        "            the diagonal)\n",
        "        \n",
        "        This geometrical property can be seen in two dimensions by plotting\n",
        "        generated data-points:\n",
        "        \n",
        "        >>> mean = [0,0]\n",
        "        >>> cov = [[1,0],[0,100]] # diagonal covariance, points lie on x or y-axis\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> x,y = np.random.multivariate_normal(mean,cov,5000).T\n",
        "        >>> plt.plot(x,y,'x'); plt.axis('equal'); plt.show()\n",
        "        \n",
        "        Note that the covariance matrix must be non-negative definite.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Papoulis, A., *Probability, Random Variables, and Stochastic Processes*,\n",
        "        3rd ed., New York: McGraw-Hill, 1991.\n",
        "        \n",
        "        Duda, R. O., Hart, P. E., and Stork, D. G., *Pattern Classification*,\n",
        "        2nd ed., New York: Wiley, 2001.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> mean = (1,2)\n",
        "        >>> cov = [[1,0],[1,0]]\n",
        "        >>> x = np.random.multivariate_normal(mean,cov,(3,3))\n",
        "        >>> x.shape\n",
        "        (3, 3, 2)\n",
        "        \n",
        "        The following is probably true, given that 0.6 is roughly twice the\n",
        "        standard deviation:\n",
        "        \n",
        "        >>> print list( (x[0,0,:] - mean) < 0.6 )\n",
        "        [True, True]\n",
        "    \n",
        "    negative_binomial(...)\n",
        "        negative_binomial(n, p, size=None)\n",
        "        \n",
        "        Draw samples from a negative_binomial distribution.\n",
        "        \n",
        "        Samples are drawn from a negative_Binomial distribution with specified\n",
        "        parameters, `n` trials and `p` probability of success where `n` is an\n",
        "        integer > 0 and `p` is in the interval [0, 1].\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            Parameter, > 0.\n",
        "        p : float\n",
        "            Parameter, >= 0 and <=1.\n",
        "        size : int or tuple of ints\n",
        "            Output shape. If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : int or ndarray of ints\n",
        "            Drawn samples.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Negative Binomial distribution is\n",
        "        \n",
        "        .. math:: P(N;n,p) = \\binom{N+n-1}{n-1}p^{n}(1-p)^{N},\n",
        "        \n",
        "        where :math:`n-1` is the number of successes, :math:`p` is the probability\n",
        "        of success, and :math:`N+n-1` is the number of trials.\n",
        "        \n",
        "        The negative binomial distribution gives the probability of n-1 successes\n",
        "        and N failures in N+n-1 trials, and success on the (N+n)th trial.\n",
        "        \n",
        "        If one throws a die repeatedly until the third time a \"1\" appears, then the\n",
        "        probability distribution of the number of non-\"1\"s that appear before the\n",
        "        third \"1\" is a negative binomial distribution.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Weisstein, Eric W. \"Negative Binomial Distribution.\" From\n",
        "               MathWorld--A Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/NegativeBinomialDistribution.html\n",
        "        .. [2] Wikipedia, \"Negative binomial distribution\",\n",
        "               http://en.wikipedia.org/wiki/Negative_binomial_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        A real world example. A company drills wild-cat oil exploration wells, each\n",
        "        with an estimated probability of success of 0.1.  What is the probability\n",
        "        of having one success for each successive well, that is what is the\n",
        "        probability of a single success after drilling 5 wells, after 6 wells,\n",
        "        etc.?\n",
        "        \n",
        "        >>> s = np.random.negative_binomial(1, 0.1, 100000)\n",
        "        >>> for i in range(1, 11):\n",
        "        ...    probability = sum(s<i) / 100000.\n",
        "        ...    print i, \"wells drilled, probability of one success =\", probability\n",
        "    \n",
        "    noncentral_chisquare(...)\n",
        "        noncentral_chisquare(df, nonc, size=None)\n",
        "        \n",
        "        Draw samples from a noncentral chi-square distribution.\n",
        "        \n",
        "        The noncentral :math:`\\chi^2` distribution is a generalisation of\n",
        "        the :math:`\\chi^2` distribution.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        df : int\n",
        "            Degrees of freedom, should be >= 1.\n",
        "        nonc : float\n",
        "            Non-centrality, should be > 0.\n",
        "        size : int or tuple of ints\n",
        "            Shape of the output.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function for the noncentral Chi-square distribution\n",
        "        is\n",
        "        \n",
        "        .. math:: P(x;df,nonc) = \\sum^{\\infty}_{i=0}\n",
        "                               \\frac{e^{-nonc/2}(nonc/2)^{i}}{i!}P_{Y_{df+2i}}(x),\n",
        "        \n",
        "        where :math:`Y_{q}` is the Chi-square with q degrees of freedom.\n",
        "        \n",
        "        In Delhi (2007), it is noted that the noncentral chi-square is useful in\n",
        "        bombing and coverage problems, the probability of killing the point target\n",
        "        given by the noncentral chi-squared distribution.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Delhi, M.S. Holla, \"On a noncentral chi-square distribution in the\n",
        "               analysis of weapon systems effectiveness\", Metrika, Volume 15,\n",
        "               Number 1 / December, 1970.\n",
        "        .. [2] Wikipedia, \"Noncentral chi-square distribution\"\n",
        "               http://en.wikipedia.org/wiki/Noncentral_chi-square_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw values from the distribution and plot the histogram\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> values = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),\n",
        "        ...                   bins=200, normed=True)\n",
        "        >>> plt.show()\n",
        "        \n",
        "        Draw values from a noncentral chisquare with very small noncentrality,\n",
        "        and compare to a chisquare.\n",
        "        \n",
        "        >>> plt.figure()\n",
        "        >>> values = plt.hist(np.random.noncentral_chisquare(3, .0000001, 100000),\n",
        "        ...                   bins=np.arange(0., 25, .1), normed=True)\n",
        "        >>> values2 = plt.hist(np.random.chisquare(3, 100000),\n",
        "        ...                    bins=np.arange(0., 25, .1), normed=True)\n",
        "        >>> plt.plot(values[1][0:-1], values[0]-values2[0], 'ob')\n",
        "        >>> plt.show()\n",
        "        \n",
        "        Demonstrate how large values of non-centrality lead to a more symmetric\n",
        "        distribution.\n",
        "        \n",
        "        >>> plt.figure()\n",
        "        >>> values = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),\n",
        "        ...                   bins=200, normed=True)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    noncentral_f(...)\n",
        "        noncentral_f(dfnum, dfden, nonc, size=None)\n",
        "        \n",
        "        Draw samples from the noncentral F distribution.\n",
        "        \n",
        "        Samples are drawn from an F distribution with specified parameters,\n",
        "        `dfnum` (degrees of freedom in numerator) and `dfden` (degrees of\n",
        "        freedom in denominator), where both parameters > 1.\n",
        "        `nonc` is the non-centrality parameter.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dfnum : int\n",
        "            Parameter, should be > 1.\n",
        "        dfden : int\n",
        "            Parameter, should be > 1.\n",
        "        nonc : float\n",
        "            Parameter, should be >= 0.\n",
        "        size : int or tuple of ints\n",
        "            Output shape. If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : scalar or ndarray\n",
        "            Drawn samples.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        When calculating the power of an experiment (power = probability of\n",
        "        rejecting the null hypothesis when a specific alternative is true) the\n",
        "        non-central F statistic becomes important.  When the null hypothesis is\n",
        "        true, the F statistic follows a central F distribution. When the null\n",
        "        hypothesis is not true, then it follows a non-central F statistic.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Weisstein, Eric W. \"Noncentral F-Distribution.\" From MathWorld--A Wolfram\n",
        "        Web Resource.  http://mathworld.wolfram.com/NoncentralF-Distribution.html\n",
        "        \n",
        "        Wikipedia, \"Noncentral F distribution\",\n",
        "        http://en.wikipedia.org/wiki/Noncentral_F-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        In a study, testing for a specific alternative to the null hypothesis\n",
        "        requires use of the Noncentral F distribution. We need to calculate the\n",
        "        area in the tail of the distribution that exceeds the value of the F\n",
        "        distribution for the null hypothesis.  We'll plot the two probability\n",
        "        distributions for comparison.\n",
        "        \n",
        "        >>> dfnum = 3 # between group deg of freedom\n",
        "        >>> dfden = 20 # within groups degrees of freedom\n",
        "        >>> nonc = 3.0\n",
        "        >>> nc_vals = np.random.noncentral_f(dfnum, dfden, nonc, 1000000)\n",
        "        >>> NF = np.histogram(nc_vals, bins=50, normed=True)\n",
        "        >>> c_vals = np.random.f(dfnum, dfden, 1000000)\n",
        "        >>> F = np.histogram(c_vals, bins=50, normed=True)\n",
        "        >>> plt.plot(F[1][1:], F[0])\n",
        "        >>> plt.plot(NF[1][1:], NF[0])\n",
        "        >>> plt.show()\n",
        "    \n",
        "    normal(...)\n",
        "        normal(loc=0.0, scale=1.0, size=None)\n",
        "        \n",
        "        Draw random samples from a normal (Gaussian) distribution.\n",
        "        \n",
        "        The probability density function of the normal distribution, first\n",
        "        derived by De Moivre and 200 years later by both Gauss and Laplace\n",
        "        independently [2]_, is often called the bell curve because of\n",
        "        its characteristic shape (see the example below).\n",
        "        \n",
        "        The normal distributions occurs often in nature.  For example, it\n",
        "        describes the commonly occurring distribution of samples influenced\n",
        "        by a large number of tiny, random disturbances, each with its own\n",
        "        unique distribution [2]_.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        loc : float\n",
        "            Mean (\"centre\") of the distribution.\n",
        "        scale : float\n",
        "            Standard deviation (spread or \"width\") of the distribution.\n",
        "        size : tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.norm : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Gaussian distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n",
        "                         e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} },\n",
        "        \n",
        "        where :math:`\\mu` is the mean and :math:`\\sigma` the standard deviation.\n",
        "        The square of the standard deviation, :math:`\\sigma^2`, is called the\n",
        "        variance.\n",
        "        \n",
        "        The function has its peak at the mean, and its \"spread\" increases with\n",
        "        the standard deviation (the function reaches 0.607 times its maximum at\n",
        "        :math:`x + \\sigma` and :math:`x - \\sigma` [2]_).  This implies that\n",
        "        `numpy.random.normal` is more likely to return samples lying close to the\n",
        "        mean, rather than those far away.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wikipedia, \"Normal distribution\",\n",
        "               http://en.wikipedia.org/wiki/Normal_distribution\n",
        "        .. [2] P. R. Peebles Jr., \"Central Limit Theorem\" in \"Probability, Random\n",
        "               Variables and Random Signal Principles\", 4th ed., 2001,\n",
        "               pp. 51, 51, 125.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> mu, sigma = 0, 0.1 # mean and standard deviation\n",
        "        >>> s = np.random.normal(mu, sigma, 1000)\n",
        "        \n",
        "        Verify the mean and the variance:\n",
        "        \n",
        "        >>> abs(mu - np.mean(s)) < 0.01\n",
        "        True\n",
        "        \n",
        "        >>> abs(sigma - np.std(s, ddof=1)) < 0.01\n",
        "        True\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 30, normed=True)\n",
        "        >>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
        "        ...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
        "        ...          linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    pareto(...)\n",
        "        pareto(a, size=None)\n",
        "        \n",
        "        Draw samples from a Pareto II or Lomax distribution with specified shape.\n",
        "        \n",
        "        The Lomax or Pareto II distribution is a shifted Pareto distribution. The\n",
        "        classical Pareto distribution can be obtained from the Lomax distribution\n",
        "        by adding the location parameter m, see below. The smallest value of the\n",
        "        Lomax distribution is zero while for the classical Pareto distribution it\n",
        "        is m, where the standard Pareto distribution has location m=1.\n",
        "        Lomax can also be considered as a simplified version of the Generalized\n",
        "        Pareto distribution (available in SciPy), with the scale set to one and\n",
        "        the location set to zero.\n",
        "        \n",
        "        The Pareto distribution must be greater than zero, and is unbounded above.\n",
        "        It is also known as the \"80-20 rule\".  In this distribution, 80 percent of\n",
        "        the weights are in the lowest 20 percent of the range, while the other 20\n",
        "        percent fill the remaining 80 percent of the range.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : float, > 0.\n",
        "            Shape of the distribution.\n",
        "        size : tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.lomax.pdf : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        scipy.stats.distributions.genpareto.pdf : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Pareto distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{am^a}{x^{a+1}}\n",
        "        \n",
        "        where :math:`a` is the shape and :math:`m` the location\n",
        "        \n",
        "        The Pareto distribution, named after the Italian economist Vilfredo Pareto,\n",
        "        is a power law probability distribution useful in many real world problems.\n",
        "        Outside the field of economics it is generally referred to as the Bradford\n",
        "        distribution. Pareto developed the distribution to describe the\n",
        "        distribution of wealth in an economy.  It has also found use in insurance,\n",
        "        web page access statistics, oil field sizes, and many other problems,\n",
        "        including the download frequency for projects in Sourceforge [1].  It is\n",
        "        one of the so-called \"fat-tailed\" distributions.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Francis Hunt and Paul Johnson, On the Pareto Distribution of\n",
        "               Sourceforge projects.\n",
        "        .. [2] Pareto, V. (1896). Course of Political Economy. Lausanne.\n",
        "        .. [3] Reiss, R.D., Thomas, M.(2001), Statistical Analysis of Extreme\n",
        "               Values, Birkhauser Verlag, Basel, pp 23-30.\n",
        "        .. [4] Wikipedia, \"Pareto distribution\",\n",
        "               http://en.wikipedia.org/wiki/Pareto_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> a, m = 3., 1. # shape and mode\n",
        "        >>> s = np.random.pareto(a, 1000) + m\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 100, normed=True, align='center')\n",
        "        >>> fit = a*m**a/bins**(a+1)\n",
        "        >>> plt.plot(bins, max(count)*fit/max(fit),linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    permutation(...)\n",
        "        permutation(x)\n",
        "        \n",
        "        Randomly permute a sequence, or return a permuted range.\n",
        "        \n",
        "        If `x` is a multi-dimensional array, it is only shuffled along its\n",
        "        first index.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : int or array_like\n",
        "            If `x` is an integer, randomly permute ``np.arange(x)``.\n",
        "            If `x` is an array, make a copy and shuffle the elements\n",
        "            randomly.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Permuted sequence or array range.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.permutation(10)\n",
        "        array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6])\n",
        "        \n",
        "        >>> np.random.permutation([1, 4, 9, 12, 15])\n",
        "        array([15,  1,  9,  4, 12])\n",
        "        \n",
        "        >>> arr = np.arange(9).reshape((3, 3))\n",
        "        >>> np.random.permutation(arr)\n",
        "        array([[6, 7, 8],\n",
        "               [0, 1, 2],\n",
        "               [3, 4, 5]])\n",
        "    \n",
        "    poisson(...)\n",
        "        poisson(lam=1.0, size=None)\n",
        "        \n",
        "        Draw samples from a Poisson distribution.\n",
        "        \n",
        "        The Poisson distribution is the limit of the Binomial\n",
        "        distribution for large N.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        lam : float\n",
        "            Expectation of interval, should be >= 0.\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The Poisson distribution\n",
        "        \n",
        "        .. math:: f(k; \\lambda)=\\frac{\\lambda^k e^{-\\lambda}}{k!}\n",
        "        \n",
        "        For events with an expected separation :math:`\\lambda` the Poisson\n",
        "        distribution :math:`f(k; \\lambda)` describes the probability of\n",
        "        :math:`k` events occurring within the observed interval :math:`\\lambda`.\n",
        "        \n",
        "        Because the output is limited to the range of the C long type, a\n",
        "        ValueError is raised when `lam` is within 10 sigma of the maximum\n",
        "        representable value.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Weisstein, Eric W. \"Poisson Distribution.\" From MathWorld--A Wolfram\n",
        "               Web Resource. http://mathworld.wolfram.com/PoissonDistribution.html\n",
        "        .. [2] Wikipedia, \"Poisson distribution\",\n",
        "           http://en.wikipedia.org/wiki/Poisson_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> import numpy as np\n",
        "        >>> s = np.random.poisson(5, 10000)\n",
        "        \n",
        "        Display histogram of the sample:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 14, normed=True)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    power(...)\n",
        "        power(a, size=None)\n",
        "        \n",
        "        Draws samples in [0, 1] from a power distribution with positive\n",
        "        exponent a - 1.\n",
        "        \n",
        "        Also known as the power function distribution.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        a : float\n",
        "            parameter, > 0\n",
        "        size : tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "                    ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : {ndarray, scalar}\n",
        "            The returned samples lie in [0, 1].\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If a<1.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function is\n",
        "        \n",
        "        .. math:: P(x; a) = ax^{a-1}, 0 \\le x \\le 1, a>0.\n",
        "        \n",
        "        The power function distribution is just the inverse of the Pareto\n",
        "        distribution. It may also be seen as a special case of the Beta\n",
        "        distribution.\n",
        "        \n",
        "        It is used, for example, in modeling the over-reporting of insurance\n",
        "        claims.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Christian Kleiber, Samuel Kotz, \"Statistical size distributions\n",
        "               in economics and actuarial sciences\", Wiley, 2003.\n",
        "        .. [2] Heckert, N. A. and Filliben, James J. (2003). NIST Handbook 148:\n",
        "               Dataplot Reference Manual, Volume 2: Let Subcommands and Library\n",
        "               Functions\", National Institute of Standards and Technology Handbook\n",
        "               Series, June 2003.\n",
        "               http://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/powpdf.pdf\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> a = 5. # shape\n",
        "        >>> samples = 1000\n",
        "        >>> s = np.random.power(a, samples)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, bins=30)\n",
        "        >>> x = np.linspace(0, 1, 100)\n",
        "        >>> y = a*x**(a-1.)\n",
        "        >>> normed_y = samples*np.diff(bins)[0]*y\n",
        "        >>> plt.plot(x, normed_y)\n",
        "        >>> plt.show()\n",
        "        \n",
        "        Compare the power function distribution to the inverse of the Pareto.\n",
        "        \n",
        "        >>> from scipy import stats\n",
        "        >>> rvs = np.random.power(5, 1000000)\n",
        "        >>> rvsp = np.random.pareto(5, 1000000)\n",
        "        >>> xx = np.linspace(0,1,100)\n",
        "        >>> powpdf = stats.powerlaw.pdf(xx,5)\n",
        "        \n",
        "        >>> plt.figure()\n",
        "        >>> plt.hist(rvs, bins=50, normed=True)\n",
        "        >>> plt.plot(xx,powpdf,'r-')\n",
        "        >>> plt.title('np.random.power(5)')\n",
        "        \n",
        "        >>> plt.figure()\n",
        "        >>> plt.hist(1./(1.+rvsp), bins=50, normed=True)\n",
        "        >>> plt.plot(xx,powpdf,'r-')\n",
        "        >>> plt.title('inverse of 1 + np.random.pareto(5)')\n",
        "        \n",
        "        >>> plt.figure()\n",
        "        >>> plt.hist(1./(1.+rvsp), bins=50, normed=True)\n",
        "        >>> plt.plot(xx,powpdf,'r-')\n",
        "        >>> plt.title('inverse of stats.pareto(5)')\n",
        "    \n",
        "    rand(...)\n",
        "        rand(d0, d1, ..., dn)\n",
        "        \n",
        "        Random values in a given shape.\n",
        "        \n",
        "        Create an array of the given shape and propagate it with\n",
        "        random samples from a uniform distribution\n",
        "        over ``[0, 1)``.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        d0, d1, ..., dn : int\n",
        "            Shape of the output.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray, shape ``(d0, d1, ..., dn)``\n",
        "            Random values.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        random\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This is a convenience function. If you want an interface that\n",
        "        takes a shape-tuple as the first argument, refer to\n",
        "        `random`.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.rand(3,2)\n",
        "        array([[ 0.14022471,  0.96360618],  #random\n",
        "               [ 0.37601032,  0.25528411],  #random\n",
        "               [ 0.49313049,  0.94909878]]) #random\n",
        "    \n",
        "    randint(...)\n",
        "        randint(low, high=None, size=None)\n",
        "        \n",
        "        Return random integers from `low` (inclusive) to `high` (exclusive).\n",
        "        \n",
        "        Return random integers from the \"discrete uniform\" distribution in the\n",
        "        \"half-open\" interval [`low`, `high`). If `high` is None (the default),\n",
        "        then results are from [0, `low`).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        low : int\n",
        "            Lowest (signed) integer to be drawn from the distribution (unless\n",
        "            ``high=None``, in which case this parameter is the *highest* such\n",
        "            integer).\n",
        "        high : int, optional\n",
        "            If provided, one above the largest (signed) integer to be drawn\n",
        "            from the distribution (see above for behavior if ``high=None``).\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. Default is None, in which case a single int is\n",
        "            returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : int or ndarray of ints\n",
        "            `size`-shaped array of random integers from the appropriate\n",
        "            distribution, or a single such random int if `size` not provided.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        random.random_integers : similar to `randint`, only for the closed\n",
        "            interval [`low`, `high`], and 1 is the lowest value if `high` is\n",
        "            omitted. In particular, this other one is the one to use to generate\n",
        "            uniformly distributed discrete non-integers.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.randint(2, size=10)\n",
        "        array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
        "        >>> np.random.randint(1, size=10)\n",
        "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "        \n",
        "        Generate a 2 x 4 array of ints between 0 and 4, inclusive:\n",
        "        \n",
        "        >>> np.random.randint(5, size=(2, 4))\n",
        "        array([[4, 0, 2, 1],\n",
        "               [3, 2, 2, 0]])\n",
        "    \n",
        "    randn(...)\n",
        "        randn([d1, ..., dn])\n",
        "        \n",
        "        Return a sample (or samples) from the \"standard normal\" distribution.\n",
        "        \n",
        "        If positive, int_like or int-convertible arguments are provided,\n",
        "        `randn` generates an array of shape ``(d1, ..., dn)``, filled\n",
        "        with random floats sampled from a univariate \"normal\" (Gaussian)\n",
        "        distribution of mean 0 and variance 1 (if any of the :math:`d_i` are\n",
        "        floats, they are first converted to integers by truncation). A single\n",
        "        float randomly sampled from the distribution is returned if no\n",
        "        argument is provided.\n",
        "        \n",
        "        This is a convenience function.  If you want an interface that takes a\n",
        "        tuple as the first argument, use `numpy.random.standard_normal` instead.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        d1, ..., dn : `n` ints, optional\n",
        "            The dimensions of the returned array, should be all positive.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        Z : ndarray or float\n",
        "            A ``(d1, ..., dn)``-shaped array of floating-point samples from\n",
        "            the standard normal distribution, or a single such float if\n",
        "            no parameters were supplied.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        random.standard_normal : Similar, but takes a tuple as its argument.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        For random samples from :math:`N(\\mu, \\sigma^2)`, use:\n",
        "        \n",
        "        ``sigma * np.random.randn(...) + mu``\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.randn()\n",
        "        2.1923875335537315 #random\n",
        "        \n",
        "        Two-by-four array of samples from N(3, 6.25):\n",
        "        \n",
        "        >>> 2.5 * np.random.randn(2, 4) + 3\n",
        "        array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],  #random\n",
        "               [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]]) #random\n",
        "    \n",
        "    random = random_sample(...)\n",
        "        random_sample(size=None)\n",
        "        \n",
        "        Return random floats in the half-open interval [0.0, 1.0).\n",
        "        \n",
        "        Results are from the \"continuous uniform\" distribution over the\n",
        "        stated interval.  To sample :math:`Unif[a, b), b > a` multiply\n",
        "        the output of `random_sample` by `(b-a)` and add `a`::\n",
        "        \n",
        "          (b - a) * random_sample() + a\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints, optional\n",
        "            Defines the shape of the returned array of random floats. If None\n",
        "            (the default), returns a single float.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : float or ndarray of floats\n",
        "            Array of random floats of shape `size` (unless ``size=None``, in which\n",
        "            case a single float is returned).\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.random_sample()\n",
        "        0.47108547995356098\n",
        "        >>> type(np.random.random_sample())\n",
        "        <type 'float'>\n",
        "        >>> np.random.random_sample((5,))\n",
        "        array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428])\n",
        "        \n",
        "        Three-by-two array of random numbers from [-5, 0):\n",
        "        \n",
        "        >>> 5 * np.random.random_sample((3, 2)) - 5\n",
        "        array([[-3.99149989, -0.52338984],\n",
        "               [-2.99091858, -0.79479508],\n",
        "               [-1.23204345, -1.75224494]])\n",
        "    \n",
        "    random_integers(...)\n",
        "        random_integers(low, high=None, size=None)\n",
        "        \n",
        "        Return random integers between `low` and `high`, inclusive.\n",
        "        \n",
        "        Return random integers from the \"discrete uniform\" distribution in the\n",
        "        closed interval [`low`, `high`].  If `high` is None (the default),\n",
        "        then results are from [1, `low`].\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        low : int\n",
        "            Lowest (signed) integer to be drawn from the distribution (unless\n",
        "            ``high=None``, in which case this parameter is the *highest* such\n",
        "            integer).\n",
        "        high : int, optional\n",
        "            If provided, the largest (signed) integer to be drawn from the\n",
        "            distribution (see above for behavior if ``high=None``).\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. Default is None, in which case a single int is returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : int or ndarray of ints\n",
        "            `size`-shaped array of random integers from the appropriate\n",
        "            distribution, or a single such random int if `size` not provided.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        random.randint : Similar to `random_integers`, only for the half-open\n",
        "            interval [`low`, `high`), and 0 is the lowest value if `high` is\n",
        "            omitted.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        To sample from N evenly spaced floating-point numbers between a and b,\n",
        "        use::\n",
        "        \n",
        "          a + (b - a) * (np.random.random_integers(N) - 1) / (N - 1.)\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.random_integers(5)\n",
        "        4\n",
        "        >>> type(np.random.random_integers(5))\n",
        "        <type 'int'>\n",
        "        >>> np.random.random_integers(5, size=(3.,2.))\n",
        "        array([[5, 4],\n",
        "               [3, 3],\n",
        "               [4, 5]])\n",
        "        \n",
        "        Choose five random numbers from the set of five evenly-spaced\n",
        "        numbers between 0 and 2.5, inclusive (*i.e.*, from the set\n",
        "        :math:`{0, 5/8, 10/8, 15/8, 20/8}`):\n",
        "        \n",
        "        >>> 2.5 * (np.random.random_integers(5, size=(5,)) - 1) / 4.\n",
        "        array([ 0.625,  1.25 ,  0.625,  0.625,  2.5  ])\n",
        "        \n",
        "        Roll two six sided dice 1000 times and sum the results:\n",
        "        \n",
        "        >>> d1 = np.random.random_integers(1, 6, 1000)\n",
        "        >>> d2 = np.random.random_integers(1, 6, 1000)\n",
        "        >>> dsums = d1 + d2\n",
        "        \n",
        "        Display results as a histogram:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(dsums, 11, normed=True)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    random_sample(...)\n",
        "        random_sample(size=None)\n",
        "        \n",
        "        Return random floats in the half-open interval [0.0, 1.0).\n",
        "        \n",
        "        Results are from the \"continuous uniform\" distribution over the\n",
        "        stated interval.  To sample :math:`Unif[a, b), b > a` multiply\n",
        "        the output of `random_sample` by `(b-a)` and add `a`::\n",
        "        \n",
        "          (b - a) * random_sample() + a\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints, optional\n",
        "            Defines the shape of the returned array of random floats. If None\n",
        "            (the default), returns a single float.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : float or ndarray of floats\n",
        "            Array of random floats of shape `size` (unless ``size=None``, in which\n",
        "            case a single float is returned).\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.random_sample()\n",
        "        0.47108547995356098\n",
        "        >>> type(np.random.random_sample())\n",
        "        <type 'float'>\n",
        "        >>> np.random.random_sample((5,))\n",
        "        array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428])\n",
        "        \n",
        "        Three-by-two array of random numbers from [-5, 0):\n",
        "        \n",
        "        >>> 5 * np.random.random_sample((3, 2)) - 5\n",
        "        array([[-3.99149989, -0.52338984],\n",
        "               [-2.99091858, -0.79479508],\n",
        "               [-1.23204345, -1.75224494]])\n",
        "    \n",
        "    ranf = random_sample(...)\n",
        "        random_sample(size=None)\n",
        "        \n",
        "        Return random floats in the half-open interval [0.0, 1.0).\n",
        "        \n",
        "        Results are from the \"continuous uniform\" distribution over the\n",
        "        stated interval.  To sample :math:`Unif[a, b), b > a` multiply\n",
        "        the output of `random_sample` by `(b-a)` and add `a`::\n",
        "        \n",
        "          (b - a) * random_sample() + a\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints, optional\n",
        "            Defines the shape of the returned array of random floats. If None\n",
        "            (the default), returns a single float.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : float or ndarray of floats\n",
        "            Array of random floats of shape `size` (unless ``size=None``, in which\n",
        "            case a single float is returned).\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.random_sample()\n",
        "        0.47108547995356098\n",
        "        >>> type(np.random.random_sample())\n",
        "        <type 'float'>\n",
        "        >>> np.random.random_sample((5,))\n",
        "        array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428])\n",
        "        \n",
        "        Three-by-two array of random numbers from [-5, 0):\n",
        "        \n",
        "        >>> 5 * np.random.random_sample((3, 2)) - 5\n",
        "        array([[-3.99149989, -0.52338984],\n",
        "               [-2.99091858, -0.79479508],\n",
        "               [-1.23204345, -1.75224494]])\n",
        "    \n",
        "    rayleigh(...)\n",
        "        rayleigh(scale=1.0, size=None)\n",
        "        \n",
        "        Draw samples from a Rayleigh distribution.\n",
        "        \n",
        "        The :math:`\\chi` and Weibull distributions are generalizations of the\n",
        "        Rayleigh.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        scale : scalar\n",
        "            Scale, also equals the mode. Should be >= 0.\n",
        "        size : int or tuple of ints, optional\n",
        "            Shape of the output. Default is None, in which case a single\n",
        "            value is returned.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function for the Rayleigh distribution is\n",
        "        \n",
        "        .. math:: P(x;scale) = \\frac{x}{scale^2}e^{\\frac{-x^2}{2 \\cdotp scale^2}}\n",
        "        \n",
        "        The Rayleigh distribution arises if the wind speed and wind direction are\n",
        "        both gaussian variables, then the vector wind velocity forms a Rayleigh\n",
        "        distribution. The Rayleigh distribution is used to model the expected\n",
        "        output from wind turbines.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        ..[1] Brighton Webs Ltd., Rayleigh Distribution,\n",
        "              http://www.brighton-webs.co.uk/distributions/rayleigh.asp\n",
        "        ..[2] Wikipedia, \"Rayleigh distribution\"\n",
        "              http://en.wikipedia.org/wiki/Rayleigh_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw values from the distribution and plot the histogram\n",
        "        \n",
        "        >>> values = hist(np.random.rayleigh(3, 100000), bins=200, normed=True)\n",
        "        \n",
        "        Wave heights tend to follow a Rayleigh distribution. If the mean wave\n",
        "        height is 1 meter, what fraction of waves are likely to be larger than 3\n",
        "        meters?\n",
        "        \n",
        "        >>> meanvalue = 1\n",
        "        >>> modevalue = np.sqrt(2 / np.pi) * meanvalue\n",
        "        >>> s = np.random.rayleigh(modevalue, 1000000)\n",
        "        \n",
        "        The percentage of waves larger than 3 meters is:\n",
        "        \n",
        "        >>> 100.*sum(s>3)/1000000.\n",
        "        0.087300000000000003\n",
        "    \n",
        "    sample = random_sample(...)\n",
        "        random_sample(size=None)\n",
        "        \n",
        "        Return random floats in the half-open interval [0.0, 1.0).\n",
        "        \n",
        "        Results are from the \"continuous uniform\" distribution over the\n",
        "        stated interval.  To sample :math:`Unif[a, b), b > a` multiply\n",
        "        the output of `random_sample` by `(b-a)` and add `a`::\n",
        "        \n",
        "          (b - a) * random_sample() + a\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints, optional\n",
        "            Defines the shape of the returned array of random floats. If None\n",
        "            (the default), returns a single float.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : float or ndarray of floats\n",
        "            Array of random floats of shape `size` (unless ``size=None``, in which\n",
        "            case a single float is returned).\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> np.random.random_sample()\n",
        "        0.47108547995356098\n",
        "        >>> type(np.random.random_sample())\n",
        "        <type 'float'>\n",
        "        >>> np.random.random_sample((5,))\n",
        "        array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428])\n",
        "        \n",
        "        Three-by-two array of random numbers from [-5, 0):\n",
        "        \n",
        "        >>> 5 * np.random.random_sample((3, 2)) - 5\n",
        "        array([[-3.99149989, -0.52338984],\n",
        "               [-2.99091858, -0.79479508],\n",
        "               [-1.23204345, -1.75224494]])\n",
        "    \n",
        "    seed(...)\n",
        "        seed(seed=None)\n",
        "        \n",
        "        Seed the generator.\n",
        "        \n",
        "        This method is called when `RandomState` is initialized. It can be\n",
        "        called again to re-seed the generator. For details, see `RandomState`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        seed : int or array_like, optional\n",
        "            Seed for `RandomState`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        RandomState\n",
        "    \n",
        "    set_state(...)\n",
        "        set_state(state)\n",
        "        \n",
        "        Set the internal state of the generator from a tuple.\n",
        "        \n",
        "        For use if one has reason to manually (re-)set the internal state of the\n",
        "        \"Mersenne Twister\"[1]_ pseudo-random number generating algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        state : tuple(str, ndarray of 624 uints, int, int, float)\n",
        "            The `state` tuple has the following items:\n",
        "        \n",
        "            1. the string 'MT19937', specifying the Mersenne Twister algorithm.\n",
        "            2. a 1-D array of 624 unsigned integers ``keys``.\n",
        "            3. an integer ``pos``.\n",
        "            4. an integer ``has_gauss``.\n",
        "            5. a float ``cached_gaussian``.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : None\n",
        "            Returns 'None' on success.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        get_state\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        `set_state` and `get_state` are not needed to work with any of the\n",
        "        random distributions in NumPy. If the internal state is manually altered,\n",
        "        the user should know exactly what he/she is doing.\n",
        "        \n",
        "        For backwards compatibility, the form (str, array of 624 uints, int) is\n",
        "        also accepted although it is missing some information about the cached\n",
        "        Gaussian value: ``state = ('MT19937', keys, pos)``.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] M. Matsumoto and T. Nishimura, \"Mersenne Twister: A\n",
        "           623-dimensionally equidistributed uniform pseudorandom number\n",
        "           generator,\" *ACM Trans. on Modeling and Computer Simulation*,\n",
        "           Vol. 8, No. 1, pp. 3-30, Jan. 1998.\n",
        "    \n",
        "    shuffle(...)\n",
        "        shuffle(x)\n",
        "        \n",
        "        Modify a sequence in-place by shuffling its contents.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            The array or list to be shuffled.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> arr = np.arange(10)\n",
        "        >>> np.random.shuffle(arr)\n",
        "        >>> arr\n",
        "        [1 7 5 2 9 4 3 6 0 8]\n",
        "        \n",
        "        This function only shuffles the array along the first index of a\n",
        "        multi-dimensional array:\n",
        "        \n",
        "        >>> arr = np.arange(9).reshape((3, 3))\n",
        "        >>> np.random.shuffle(arr)\n",
        "        >>> arr\n",
        "        array([[3, 4, 5],\n",
        "               [6, 7, 8],\n",
        "               [0, 1, 2]])\n",
        "    \n",
        "    standard_cauchy(...)\n",
        "        standard_cauchy(size=None)\n",
        "        \n",
        "        Standard Cauchy distribution with mode = 0.\n",
        "        \n",
        "        Also known as the Lorentz distribution.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints\n",
        "            Shape of the output.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : ndarray or scalar\n",
        "            The drawn samples.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function for the full Cauchy distribution is\n",
        "        \n",
        "        .. math:: P(x; x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\bigl[ 1+\n",
        "                  (\\frac{x-x_0}{\\gamma})^2 \\bigr] }\n",
        "        \n",
        "        and the Standard Cauchy distribution just sets :math:`x_0=0` and\n",
        "        :math:`\\gamma=1`\n",
        "        \n",
        "        The Cauchy distribution arises in the solution to the driven harmonic\n",
        "        oscillator problem, and also describes spectral line broadening. It\n",
        "        also describes the distribution of values at which a line tilted at\n",
        "        a random angle will cut the x axis.\n",
        "        \n",
        "        When studying hypothesis tests that assume normality, seeing how the\n",
        "        tests perform on data from a Cauchy distribution is a good indicator of\n",
        "        their sensitivity to a heavy-tailed distribution, since the Cauchy looks\n",
        "        very much like a Gaussian distribution, but with heavier tails.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        ..[1] NIST/SEMATECH e-Handbook of Statistical Methods, \"Cauchy\n",
        "              Distribution\",\n",
        "              http://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm\n",
        "        ..[2] Weisstein, Eric W. \"Cauchy Distribution.\" From MathWorld--A\n",
        "              Wolfram Web Resource.\n",
        "              http://mathworld.wolfram.com/CauchyDistribution.html\n",
        "        ..[3] Wikipedia, \"Cauchy distribution\"\n",
        "              http://en.wikipedia.org/wiki/Cauchy_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples and plot the distribution:\n",
        "        \n",
        "        >>> s = np.random.standard_cauchy(1000000)\n",
        "        >>> s = s[(s>-25) & (s<25)]  # truncate distribution so it plots well\n",
        "        >>> plt.hist(s, bins=100)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    standard_exponential(...)\n",
        "        standard_exponential(size=None)\n",
        "        \n",
        "        Draw samples from the standard exponential distribution.\n",
        "        \n",
        "        `standard_exponential` is identical to the exponential distribution\n",
        "        with a scale parameter of 1.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints\n",
        "            Shape of the output.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : float or ndarray\n",
        "            Drawn samples.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Output a 3x8000 array:\n",
        "        \n",
        "        >>> n = np.random.standard_exponential((3, 8000))\n",
        "    \n",
        "    standard_gamma(...)\n",
        "        standard_gamma(shape, size=None)\n",
        "        \n",
        "        Draw samples from a Standard Gamma distribution.\n",
        "        \n",
        "        Samples are drawn from a Gamma distribution with specified parameters,\n",
        "        shape (sometimes designated \"k\") and scale=1.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : float\n",
        "            Parameter, should be > 0.\n",
        "        size : int or tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : ndarray or scalar\n",
        "            The drawn samples.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.gamma : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Gamma distribution is\n",
        "        \n",
        "        .. math:: p(x) = x^{k-1}\\frac{e^{-x/\\theta}}{\\theta^k\\Gamma(k)},\n",
        "        \n",
        "        where :math:`k` is the shape and :math:`\\theta` the scale,\n",
        "        and :math:`\\Gamma` is the Gamma function.\n",
        "        \n",
        "        The Gamma distribution is often used to model the times to failure of\n",
        "        electronic components, and arises naturally in processes for which the\n",
        "        waiting times between Poisson distributed events are relevant.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Weisstein, Eric W. \"Gamma Distribution.\" From MathWorld--A\n",
        "               Wolfram Web Resource.\n",
        "               http://mathworld.wolfram.com/GammaDistribution.html\n",
        "        .. [2] Wikipedia, \"Gamma-distribution\",\n",
        "               http://en.wikipedia.org/wiki/Gamma-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> shape, scale = 2., 1. # mean and width\n",
        "        >>> s = np.random.standard_gamma(shape, 1000000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> import scipy.special as sps\n",
        "        >>> count, bins, ignored = plt.hist(s, 50, normed=True)\n",
        "        >>> y = bins**(shape-1) * ((np.exp(-bins/scale))/ \\\n",
        "        ...                       (sps.gamma(shape) * scale**shape))\n",
        "        >>> plt.plot(bins, y, linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    standard_normal(...)\n",
        "        standard_normal(size=None)\n",
        "        \n",
        "        Returns samples from a Standard Normal distribution (mean=0, stdev=1).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. Default is None, in which case a single value is\n",
        "            returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : float or ndarray\n",
        "            Drawn samples.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> s = np.random.standard_normal(8000)\n",
        "        >>> s\n",
        "        array([ 0.6888893 ,  0.78096262, -0.89086505, ...,  0.49876311, #random\n",
        "               -0.38672696, -0.4685006 ])                               #random\n",
        "        >>> s.shape\n",
        "        (8000,)\n",
        "        >>> s = np.random.standard_normal(size=(3, 4, 2))\n",
        "        >>> s.shape\n",
        "        (3, 4, 2)\n",
        "    \n",
        "    standard_t(...)\n",
        "        standard_t(df, size=None)\n",
        "        \n",
        "        Standard Student's t distribution with df degrees of freedom.\n",
        "        \n",
        "        A special case of the hyperbolic distribution.\n",
        "        As `df` gets large, the result resembles that of the standard normal\n",
        "        distribution (`standard_normal`).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        df : int\n",
        "            Degrees of freedom, should be > 0.\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. Default is None, in which case a single value is\n",
        "            returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : ndarray or scalar\n",
        "            Drawn samples.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function for the t distribution is\n",
        "        \n",
        "        .. math:: P(x, df) = \\frac{\\Gamma(\\frac{df+1}{2})}{\\sqrt{\\pi df}\n",
        "                  \\Gamma(\\frac{df}{2})}\\Bigl( 1+\\frac{x^2}{df} \\Bigr)^{-(df+1)/2}\n",
        "        \n",
        "        The t test is based on an assumption that the data come from a Normal\n",
        "        distribution. The t test provides a way to test whether the sample mean\n",
        "        (that is the mean calculated from the data) is a good estimate of the true\n",
        "        mean.\n",
        "        \n",
        "        The derivation of the t-distribution was forst published in 1908 by William\n",
        "        Gisset while working for the Guinness Brewery in Dublin. Due to proprietary\n",
        "        issues, he had to publish under a pseudonym, and so he used the name\n",
        "        Student.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Dalgaard, Peter, \"Introductory Statistics With R\",\n",
        "               Springer, 2002.\n",
        "        .. [2] Wikipedia, \"Student's t-distribution\"\n",
        "               http://en.wikipedia.org/wiki/Student's_t-distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        From Dalgaard page 83 [1]_, suppose the daily energy intake for 11\n",
        "        women in Kj is:\n",
        "        \n",
        "        >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n",
        "        ...                    7515, 8230, 8770])\n",
        "        \n",
        "        Does their energy intake deviate systematically from the recommended\n",
        "        value of 7725 kJ?\n",
        "        \n",
        "        We have 10 degrees of freedom, so is the sample mean within 95% of the\n",
        "        recommended value?\n",
        "        \n",
        "        >>> s = np.random.standard_t(10, size=100000)\n",
        "        >>> np.mean(intake)\n",
        "        6753.636363636364\n",
        "        >>> intake.std(ddof=1)\n",
        "        1142.1232221373727\n",
        "        \n",
        "        Calculate the t statistic, setting the ddof parameter to the unbiased\n",
        "        value so the divisor in the standard deviation will be degrees of\n",
        "        freedom, N-1.\n",
        "        \n",
        "        >>> t = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))\n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> h = plt.hist(s, bins=100, normed=True)\n",
        "        \n",
        "        For a one-sided t-test, how far out in the distribution does the t\n",
        "        statistic appear?\n",
        "        \n",
        "        >>> >>> np.sum(s<t) / float(len(s))\n",
        "        0.0090699999999999999  #random\n",
        "        \n",
        "        So the p-value is about 0.009, which says the null hypothesis has a\n",
        "        probability of about 99% of being true.\n",
        "    \n",
        "    triangular(...)\n",
        "        triangular(left, mode, right, size=None)\n",
        "        \n",
        "        Draw samples from the triangular distribution.\n",
        "        \n",
        "        The triangular distribution is a continuous probability distribution with\n",
        "        lower limit left, peak at mode, and upper limit right. Unlike the other\n",
        "        distributions, these parameters directly define the shape of the pdf.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        left : scalar\n",
        "            Lower limit.\n",
        "        mode : scalar\n",
        "            The value where the peak of the distribution occurs.\n",
        "            The value should fulfill the condition ``left <= mode <= right``.\n",
        "        right : scalar\n",
        "            Upper limit, should be larger than `left`.\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. Default is None, in which case a single value is\n",
        "            returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : ndarray or scalar\n",
        "            The returned samples all lie in the interval [left, right].\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function for the Triangular distribution is\n",
        "        \n",
        "        .. math:: P(x;l, m, r) = \\begin{cases}\n",
        "                  \\frac{2(x-l)}{(r-l)(m-l)}& \\text{for $l \\leq x \\leq m$},\\\\\n",
        "                  \\frac{2(m-x)}{(r-l)(r-m)}& \\text{for $m \\leq x \\leq r$},\\\\\n",
        "                  0& \\text{otherwise}.\n",
        "                  \\end{cases}\n",
        "        \n",
        "        The triangular distribution is often used in ill-defined problems where the\n",
        "        underlying distribution is not known, but some knowledge of the limits and\n",
        "        mode exists. Often it is used in simulations.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        ..[1] Wikipedia, \"Triangular distribution\"\n",
        "              http://en.wikipedia.org/wiki/Triangular_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw values from the distribution and plot the histogram:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> h = plt.hist(np.random.triangular(-3, 0, 8, 100000), bins=200,\n",
        "        ...              normed=True)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    uniform(...)\n",
        "        uniform(low=0.0, high=1.0, size=1)\n",
        "        \n",
        "        Draw samples from a uniform distribution.\n",
        "        \n",
        "        Samples are uniformly distributed over the half-open interval\n",
        "        ``[low, high)`` (includes low, but excludes high).  In other words,\n",
        "        any value within the given interval is equally likely to be drawn\n",
        "        by `uniform`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        low : float, optional\n",
        "            Lower boundary of the output interval.  All values generated will be\n",
        "            greater than or equal to low.  The default value is 0.\n",
        "        high : float\n",
        "            Upper boundary of the output interval.  All values generated will be\n",
        "            less than high.  The default value is 1.0.\n",
        "        size : int or tuple of ints, optional\n",
        "            Shape of output.  If the given size is, for example, (m,n,k),\n",
        "            m*n*k samples are generated.  If no shape is specified, a single sample\n",
        "            is returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Drawn samples, with shape `size`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        randint : Discrete uniform distribution, yielding integers.\n",
        "        random_integers : Discrete uniform distribution over the closed\n",
        "                          interval ``[low, high]``.\n",
        "        random_sample : Floats uniformly distributed over ``[0, 1)``.\n",
        "        random : Alias for `random_sample`.\n",
        "        rand : Convenience function that accepts dimensions as input, e.g.,\n",
        "               ``rand(2,2)`` would generate a 2-by-2 array of floats,\n",
        "               uniformly distributed over ``[0, 1)``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function of the uniform distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{1}{b - a}\n",
        "        \n",
        "        anywhere within the interval ``[a, b)``, and zero elsewhere.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> s = np.random.uniform(-1,0,1000)\n",
        "        \n",
        "        All values are within the given interval:\n",
        "        \n",
        "        >>> np.all(s >= -1)\n",
        "        True\n",
        "        >>> np.all(s < 0)\n",
        "        True\n",
        "        \n",
        "        Display the histogram of the samples, along with the\n",
        "        probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> count, bins, ignored = plt.hist(s, 15, normed=True)\n",
        "        >>> plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    vonmises(...)\n",
        "        vonmises(mu, kappa, size=None)\n",
        "        \n",
        "        Draw samples from a von Mises distribution.\n",
        "        \n",
        "        Samples are drawn from a von Mises distribution with specified mode\n",
        "        (mu) and dispersion (kappa), on the interval [-pi, pi].\n",
        "        \n",
        "        The von Mises distribution (also known as the circular normal\n",
        "        distribution) is a continuous probability distribution on the unit\n",
        "        circle.  It may be thought of as the circular analogue of the normal\n",
        "        distribution.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        mu : float\n",
        "            Mode (\"center\") of the distribution.\n",
        "        kappa : float\n",
        "            Dispersion of the distribution, has to be >=0.\n",
        "        size : int or tuple of int\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : scalar or ndarray\n",
        "            The returned samples, which are in the interval [-pi, pi].\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.vonmises : probability density function,\n",
        "            distribution, or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the von Mises distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{e^{\\kappa cos(x-\\mu)}}{2\\pi I_0(\\kappa)},\n",
        "        \n",
        "        where :math:`\\mu` is the mode and :math:`\\kappa` the dispersion,\n",
        "        and :math:`I_0(\\kappa)` is the modified Bessel function of order 0.\n",
        "        \n",
        "        The von Mises is named for Richard Edler von Mises, who was born in\n",
        "        Austria-Hungary, in what is now the Ukraine.  He fled to the United\n",
        "        States in 1939 and became a professor at Harvard.  He worked in\n",
        "        probability theory, aerodynamics, fluid mechanics, and philosophy of\n",
        "        science.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Abramowitz, M. and Stegun, I. A. (ed.), *Handbook of Mathematical\n",
        "        Functions*, New York: Dover, 1965.\n",
        "        \n",
        "        von Mises, R., *Mathematical Theory of Probability and Statistics*,\n",
        "        New York: Academic Press, 1964.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> mu, kappa = 0.0, 4.0 # mean and dispersion\n",
        "        >>> s = np.random.vonmises(mu, kappa, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> import scipy.special as sps\n",
        "        >>> count, bins, ignored = plt.hist(s, 50, normed=True)\n",
        "        >>> x = np.arange(-np.pi, np.pi, 2*np.pi/50.)\n",
        "        >>> y = -np.exp(kappa*np.cos(x-mu))/(2*np.pi*sps.jn(0,kappa))\n",
        "        >>> plt.plot(x, y/max(y), linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "    \n",
        "    wald(...)\n",
        "        wald(mean, scale, size=None)\n",
        "        \n",
        "        Draw samples from a Wald, or Inverse Gaussian, distribution.\n",
        "        \n",
        "        As the scale approaches infinity, the distribution becomes more like a\n",
        "        Gaussian.\n",
        "        \n",
        "        Some references claim that the Wald is an Inverse Gaussian with mean=1, but\n",
        "        this is by no means universal.\n",
        "        \n",
        "        The Inverse Gaussian distribution was first studied in relationship to\n",
        "        Brownian motion. In 1956 M.C.K. Tweedie used the name Inverse Gaussian\n",
        "        because there is an inverse relationship between the time to cover a unit\n",
        "        distance and distance covered in unit time.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : scalar\n",
        "            Distribution mean, should be > 0.\n",
        "        scale : scalar\n",
        "            Scale parameter, should be >= 0.\n",
        "        size : int or tuple of ints, optional\n",
        "            Output shape. Default is None, in which case a single value is\n",
        "            returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : ndarray or scalar\n",
        "            Drawn sample, all greater than zero.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density function for the Wald distribution is\n",
        "        \n",
        "        .. math:: P(x;mean,scale) = \\sqrt{\\frac{scale}{2\\pi x^3}}e^\n",
        "                                    \\frac{-scale(x-mean)^2}{2\\cdotp mean^2x}\n",
        "        \n",
        "        As noted above the Inverse Gaussian distribution first arise from attempts\n",
        "        to model Brownian Motion. It is also a competitor to the Weibull for use in\n",
        "        reliability modeling and modeling stock returns and interest rate\n",
        "        processes.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        ..[1] Brighton Webs Ltd., Wald Distribution,\n",
        "              http://www.brighton-webs.co.uk/distributions/wald.asp\n",
        "        ..[2] Chhikara, Raj S., and Folks, J. Leroy, \"The Inverse Gaussian\n",
        "              Distribution: Theory : Methodology, and Applications\", CRC Press,\n",
        "              1988.\n",
        "        ..[3] Wikipedia, \"Wald distribution\"\n",
        "              http://en.wikipedia.org/wiki/Wald_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw values from the distribution and plot the histogram:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> h = plt.hist(np.random.wald(3, 2, 100000), bins=200, normed=True)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    weibull(...)\n",
        "        weibull(a, size=None)\n",
        "        \n",
        "        Weibull distribution.\n",
        "        \n",
        "        Draw samples from a 1-parameter Weibull distribution with the given\n",
        "        shape parameter `a`.\n",
        "        \n",
        "        .. math:: X = (-ln(U))^{1/a}\n",
        "        \n",
        "        Here, U is drawn from the uniform distribution over (0,1].\n",
        "        \n",
        "        The more common 2-parameter Weibull, including a scale parameter\n",
        "        :math:`\\lambda` is just :math:`X = \\lambda(-ln(U))^{1/a}`.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        a : float\n",
        "            Shape of the distribution.\n",
        "        size : tuple of ints\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.weibull : probability density function,\n",
        "            distribution or cumulative density function, etc.\n",
        "        \n",
        "        gumbel, scipy.stats.distributions.genextreme\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The Weibull (or Type III asymptotic extreme value distribution for smallest\n",
        "        values, SEV Type III, or Rosin-Rammler distribution) is one of a class of\n",
        "        Generalized Extreme Value (GEV) distributions used in modeling extreme\n",
        "        value problems.  This class includes the Gumbel and Frechet distributions.\n",
        "        \n",
        "        The probability density for the Weibull distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{a}\n",
        "                         {\\lambda}(\\frac{x}{\\lambda})^{a-1}e^{-(x/\\lambda)^a},\n",
        "        \n",
        "        where :math:`a` is the shape and :math:`\\lambda` the scale.\n",
        "        \n",
        "        The function has its peak (the mode) at\n",
        "        :math:`\\lambda(\\frac{a-1}{a})^{1/a}`.\n",
        "        \n",
        "        When ``a = 1``, the Weibull distribution reduces to the exponential\n",
        "        distribution.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Waloddi Weibull, Professor, Royal Technical University, Stockholm,\n",
        "               1939 \"A Statistical Theory Of The Strength Of Materials\",\n",
        "               Ingeniorsvetenskapsakademiens Handlingar Nr 151, 1939,\n",
        "               Generalstabens Litografiska Anstalts Forlag, Stockholm.\n",
        "        .. [2] Waloddi Weibull, 1951 \"A Statistical Distribution Function of Wide\n",
        "               Applicability\",  Journal Of Applied Mechanics ASME Paper.\n",
        "        .. [3] Wikipedia, \"Weibull distribution\",\n",
        "               http://en.wikipedia.org/wiki/Weibull_distribution\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> a = 5. # shape\n",
        "        >>> s = np.random.weibull(a, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> x = np.arange(1,100.)/50.\n",
        "        >>> def weib(x,n,a):\n",
        "        ...     return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)\n",
        "        \n",
        "        >>> count, bins, ignored = plt.hist(np.random.weibull(5.,1000))\n",
        "        >>> x = np.arange(1,100.)/50.\n",
        "        >>> scale = count.max()/weib(x, 1., 5.).max()\n",
        "        >>> plt.plot(x, weib(x, 1., 5.)*scale)\n",
        "        >>> plt.show()\n",
        "    \n",
        "    zipf(...)\n",
        "        zipf(a, size=None)\n",
        "        \n",
        "        Draw samples from a Zipf distribution.\n",
        "        \n",
        "        Samples are drawn from a Zipf distribution with specified parameter\n",
        "        `a` > 1.\n",
        "        \n",
        "        The Zipf distribution (also known as the zeta distribution) is a\n",
        "        continuous probability distribution that satisfies Zipf's law: the\n",
        "        frequency of an item is inversely proportional to its rank in a\n",
        "        frequency table.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        a : float > 1\n",
        "            Distribution parameter.\n",
        "        size : int or tuple of int, optional\n",
        "            Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
        "            ``m * n * k`` samples are drawn; a single integer is equivalent in\n",
        "            its result to providing a mono-tuple, i.e., a 1-D array of length\n",
        "            *size* is returned.  The default is None, in which case a single\n",
        "            scalar is returned.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        samples : scalar or ndarray\n",
        "            The returned samples are greater than or equal to one.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.stats.distributions.zipf : probability density function,\n",
        "            distribution, or cumulative density function, etc.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The probability density for the Zipf distribution is\n",
        "        \n",
        "        .. math:: p(x) = \\frac{x^{-a}}{\\zeta(a)},\n",
        "        \n",
        "        where :math:`\\zeta` is the Riemann Zeta function.\n",
        "        \n",
        "        It is named for the American linguist George Kingsley Zipf, who noted\n",
        "        that the frequency of any word in a sample of a language is inversely\n",
        "        proportional to its rank in the frequency table.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Zipf, G. K., *Selected Studies of the Principle of Relative Frequency\n",
        "        in Language*, Cambridge, MA: Harvard Univ. Press, 1932.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Draw samples from the distribution:\n",
        "        \n",
        "        >>> a = 2. # parameter\n",
        "        >>> s = np.random.zipf(a, 1000)\n",
        "        \n",
        "        Display the histogram of the samples, along with\n",
        "        the probability density function:\n",
        "        \n",
        "        >>> import matplotlib.pyplot as plt\n",
        "        >>> import scipy.special as sps\n",
        "        Truncate s values at 50 so plot is interesting\n",
        "        >>> count, bins, ignored = plt.hist(s[s<50], 50, normed=True)\n",
        "        >>> x = np.arange(1., 50.)\n",
        "        >>> y = x**(-a)/sps.zetac(a)\n",
        "        >>> plt.plot(x, y/max(y), linewidth=2, color='r')\n",
        "        >>> plt.show()\n",
        "\n",
        "DATA\n",
        "    __all__ = ['beta', 'binomial', 'bytes', 'chisquare', 'exponential', 'f...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(np.random.randn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on built-in function randn:\n",
        "\n",
        "randn(...)\n",
        "    randn([d1, ..., dn])\n",
        "    \n",
        "    Return a sample (or samples) from the \"standard normal\" distribution.\n",
        "    \n",
        "    If positive, int_like or int-convertible arguments are provided,\n",
        "    `randn` generates an array of shape ``(d1, ..., dn)``, filled\n",
        "    with random floats sampled from a univariate \"normal\" (Gaussian)\n",
        "    distribution of mean 0 and variance 1 (if any of the :math:`d_i` are\n",
        "    floats, they are first converted to integers by truncation). A single\n",
        "    float randomly sampled from the distribution is returned if no\n",
        "    argument is provided.\n",
        "    \n",
        "    This is a convenience function.  If you want an interface that takes a\n",
        "    tuple as the first argument, use `numpy.random.standard_normal` instead.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    d1, ..., dn : `n` ints, optional\n",
        "        The dimensions of the returned array, should be all positive.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    Z : ndarray or float\n",
        "        A ``(d1, ..., dn)``-shaped array of floating-point samples from\n",
        "        the standard normal distribution, or a single such float if\n",
        "        no parameters were supplied.\n",
        "    \n",
        "    See Also\n",
        "    --------\n",
        "    random.standard_normal : Similar, but takes a tuple as its argument.\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    For random samples from :math:`N(\\mu, \\sigma^2)`, use:\n",
        "    \n",
        "    ``sigma * np.random.randn(...) + mu``\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> np.random.randn()\n",
        "    2.1923875335537315 #random\n",
        "    \n",
        "    Two-by-four array of samples from N(3, 6.25):\n",
        "    \n",
        "    >>> 2.5 * np.random.randn(2, 4) + 3\n",
        "    array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],  #random\n",
        "           [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]]) #random\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.randn(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([-0.51780114,  0.50728784])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_rand = y + np.random.randn(len(y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,y_rand)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[<matplotlib.lines.Line2D at 0x108056150>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc4ZhmDKAxZxhGKYMYDFnGIYpA/4/CZnBpVj3qssAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x108044dd0>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gaussian noise\n",
      "# zero mean and 0.1 variance\n",
      "y_rand = y + 0.1*np.random.randn(len(y))\n",
      "plb.plot(x,y_rand)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10821ee10>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x108066550>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# allowable methods\n",
      "dir()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "['ALLOW_THREADS',\n",
        " 'Annotation',\n",
        " 'Arrow',\n",
        " 'Artist',\n",
        " 'AutoLocator',\n",
        " 'Axes',\n",
        " 'BUFSIZE',\n",
        " 'Button',\n",
        " 'CLIP',\n",
        " 'Circle',\n",
        " 'ComplexWarning',\n",
        " 'DAILY',\n",
        " 'DataSource',\n",
        " 'DateFormatter',\n",
        " 'DateLocator',\n",
        " 'DayLocator',\n",
        " 'ERR_CALL',\n",
        " 'ERR_DEFAULT',\n",
        " 'ERR_DEFAULT2',\n",
        " 'ERR_IGNORE',\n",
        " 'ERR_LOG',\n",
        " 'ERR_PRINT',\n",
        " 'ERR_RAISE',\n",
        " 'ERR_WARN',\n",
        " 'FLOATING_POINT_SUPPORT',\n",
        " 'FPE_DIVIDEBYZERO',\n",
        " 'FPE_INVALID',\n",
        " 'FPE_OVERFLOW',\n",
        " 'FPE_UNDERFLOW',\n",
        " 'FR',\n",
        " 'False_',\n",
        " 'Figure',\n",
        " 'FigureCanvasBase',\n",
        " 'FixedFormatter',\n",
        " 'FixedLocator',\n",
        " 'FormatStrFormatter',\n",
        " 'Formatter',\n",
        " 'FuncFormatter',\n",
        " 'GridSpec',\n",
        " 'HOURLY',\n",
        " 'HourLocator',\n",
        " 'In',\n",
        " 'IndexDateFormatter',\n",
        " 'IndexLocator',\n",
        " 'Inf',\n",
        " 'Infinity',\n",
        " 'LinAlgError',\n",
        " 'Line2D',\n",
        " 'LinearLocator',\n",
        " 'Locator',\n",
        " 'LogFormatter',\n",
        " 'LogFormatterExponent',\n",
        " 'LogFormatterMathtext',\n",
        " 'LogLocator',\n",
        " 'MAXDIMS',\n",
        " 'MINUTELY',\n",
        " 'MO',\n",
        " 'MONTHLY',\n",
        " 'MachAr',\n",
        " 'MaxNLocator',\n",
        " 'MinuteLocator',\n",
        " 'MonthLocator',\n",
        " 'MultipleLocator',\n",
        " 'NAN',\n",
        " 'NINF',\n",
        " 'NZERO',\n",
        " 'NaN',\n",
        " 'Normalize',\n",
        " 'NullFormatter',\n",
        " 'NullLocator',\n",
        " 'Out',\n",
        " 'PINF',\n",
        " 'PZERO',\n",
        " 'PackageLoader',\n",
        " 'PolarAxes',\n",
        " 'Polygon',\n",
        " 'RAISE',\n",
        " 'RRuleLocator',\n",
        " 'RankWarning',\n",
        " 'Rectangle',\n",
        " 'SA',\n",
        " 'SECONDLY',\n",
        " 'SHIFT_DIVIDEBYZERO',\n",
        " 'SHIFT_INVALID',\n",
        " 'SHIFT_OVERFLOW',\n",
        " 'SHIFT_UNDERFLOW',\n",
        " 'SU',\n",
        " 'ScalarFormatter',\n",
        " 'ScalarType',\n",
        " 'SecondLocator',\n",
        " 'Slider',\n",
        " 'Subplot',\n",
        " 'SubplotTool',\n",
        " 'TH',\n",
        " 'TU',\n",
        " 'Tester',\n",
        " 'Text',\n",
        " 'TickHelper',\n",
        " 'True_',\n",
        " 'UFUNC_BUFSIZE_DEFAULT',\n",
        " 'UFUNC_PYVALS_NAME',\n",
        " 'WE',\n",
        " 'WEEKLY',\n",
        " 'WRAP',\n",
        " 'WeekdayLocator',\n",
        " 'Widget',\n",
        " 'YEARLY',\n",
        " 'YearLocator',\n",
        " '_',\n",
        " '_10',\n",
        " '_13',\n",
        " '_15',\n",
        " '_16',\n",
        " '_17',\n",
        " '_6',\n",
        " '__',\n",
        " '___',\n",
        " '__builtin__',\n",
        " '__builtins__',\n",
        " '__doc__',\n",
        " '__name__',\n",
        " '__package__',\n",
        " '__version__',\n",
        " '_dh',\n",
        " '_i',\n",
        " '_i1',\n",
        " '_i10',\n",
        " '_i11',\n",
        " '_i12',\n",
        " '_i13',\n",
        " '_i14',\n",
        " '_i15',\n",
        " '_i16',\n",
        " '_i17',\n",
        " '_i18',\n",
        " '_i19',\n",
        " '_i2',\n",
        " '_i3',\n",
        " '_i4',\n",
        " '_i5',\n",
        " '_i6',\n",
        " '_i7',\n",
        " '_i8',\n",
        " '_i9',\n",
        " '_ih',\n",
        " '_ii',\n",
        " '_iii',\n",
        " '_oh',\n",
        " '_sh',\n",
        " 'absolute',\n",
        " 'acorr',\n",
        " 'add',\n",
        " 'add_docstring',\n",
        " 'add_newdoc',\n",
        " 'add_newdocs',\n",
        " 'alen',\n",
        " 'all',\n",
        " 'allclose',\n",
        " 'alltrue',\n",
        " 'alterdot',\n",
        " 'amap',\n",
        " 'amax',\n",
        " 'amin',\n",
        " 'angle',\n",
        " 'annotate',\n",
        " 'any',\n",
        " 'append',\n",
        " 'apply_along_axis',\n",
        " 'apply_over_axes',\n",
        " 'arange',\n",
        " 'arccos',\n",
        " 'arccosh',\n",
        " 'arcsin',\n",
        " 'arcsinh',\n",
        " 'arctan',\n",
        " 'arctan2',\n",
        " 'arctanh',\n",
        " 'argmax',\n",
        " 'argmin',\n",
        " 'argsort',\n",
        " 'argwhere',\n",
        " 'around',\n",
        " 'array',\n",
        " 'array2string',\n",
        " 'array_equal',\n",
        " 'array_equiv',\n",
        " 'array_repr',\n",
        " 'array_split',\n",
        " 'array_str',\n",
        " 'arrow',\n",
        " 'asanyarray',\n",
        " 'asarray',\n",
        " 'asarray_chkfinite',\n",
        " 'ascontiguousarray',\n",
        " 'asfarray',\n",
        " 'asfortranarray',\n",
        " 'asmatrix',\n",
        " 'asscalar',\n",
        " 'atleast_1d',\n",
        " 'atleast_2d',\n",
        " 'atleast_3d',\n",
        " 'autoscale',\n",
        " 'autumn',\n",
        " 'average',\n",
        " 'axes',\n",
        " 'axhline',\n",
        " 'axhspan',\n",
        " 'axis',\n",
        " 'axvline',\n",
        " 'axvspan',\n",
        " 'bar',\n",
        " 'barbs',\n",
        " 'barh',\n",
        " 'bartlett',\n",
        " 'base_repr',\n",
        " 'bench',\n",
        " 'beta',\n",
        " 'binary_repr',\n",
        " 'bincount',\n",
        " 'binomial',\n",
        " 'bitwise_and',\n",
        " 'bitwise_not',\n",
        " 'bitwise_or',\n",
        " 'bitwise_xor',\n",
        " 'bivariate_normal',\n",
        " 'blackman',\n",
        " 'bmat',\n",
        " 'bone',\n",
        " 'bool8',\n",
        " 'bool_',\n",
        " 'box',\n",
        " 'boxplot',\n",
        " 'broadcast',\n",
        " 'broadcast_arrays',\n",
        " 'broken_barh',\n",
        " 'byte',\n",
        " 'byte_bounds',\n",
        " 'bytes',\n",
        " 'bytes_',\n",
        " 'c_',\n",
        " 'can_cast',\n",
        " 'cast',\n",
        " 'cbook',\n",
        " 'cdouble',\n",
        " 'ceil',\n",
        " 'center_matrix',\n",
        " 'cfloat',\n",
        " 'char',\n",
        " 'character',\n",
        " 'chararray',\n",
        " 'chisquare',\n",
        " 'cholesky',\n",
        " 'choose',\n",
        " 'cla',\n",
        " 'clabel',\n",
        " 'clf',\n",
        " 'clim',\n",
        " 'clip',\n",
        " 'clongdouble',\n",
        " 'clongfloat',\n",
        " 'close',\n",
        " 'cm',\n",
        " 'cohere',\n",
        " 'colorbar',\n",
        " 'colormaps',\n",
        " 'colors',\n",
        " 'column_stack',\n",
        " 'common_type',\n",
        " 'compare_chararrays',\n",
        " 'complex128',\n",
        " 'complex256',\n",
        " 'complex64',\n",
        " 'complex_',\n",
        " 'complexfloating',\n",
        " 'compress',\n",
        " 'concatenate',\n",
        " 'cond',\n",
        " 'conj',\n",
        " 'conjugate',\n",
        " 'connect',\n",
        " 'contour',\n",
        " 'contourf',\n",
        " 'convolve',\n",
        " 'cool',\n",
        " 'copper',\n",
        " 'copy',\n",
        " 'copysign',\n",
        " 'corrcoef',\n",
        " 'correlate',\n",
        " 'cos',\n",
        " 'cosh',\n",
        " 'count_nonzero',\n",
        " 'cov',\n",
        " 'cross',\n",
        " 'csd',\n",
        " 'csingle',\n",
        " 'csv2rec',\n",
        " 'ctypeslib',\n",
        " 'cumprod',\n",
        " 'cumproduct',\n",
        " 'cumsum',\n",
        " 'date2num',\n",
        " 'datestr2num',\n",
        " 'datetime',\n",
        " 'datetime64',\n",
        " 'datetime_',\n",
        " 'datetime_data',\n",
        " 'dedent',\n",
        " 'deg2rad',\n",
        " 'degrees',\n",
        " 'delaxes',\n",
        " 'delete',\n",
        " 'demean',\n",
        " 'deprecate',\n",
        " 'deprecate_with_doc',\n",
        " 'det',\n",
        " 'detrend',\n",
        " 'detrend_linear',\n",
        " 'detrend_mean',\n",
        " 'detrend_none',\n",
        " 'diag',\n",
        " 'diag_indices',\n",
        " 'diag_indices_from',\n",
        " 'diagflat',\n",
        " 'diagonal',\n",
        " 'diff',\n",
        " 'digitize',\n",
        " 'disconnect',\n",
        " 'disp',\n",
        " 'display',\n",
        " 'dist',\n",
        " 'dist_point_to_segment',\n",
        " 'distances_along_curve',\n",
        " 'divide',\n",
        " 'docstring',\n",
        " 'dot',\n",
        " 'double',\n",
        " 'drange',\n",
        " 'draw',\n",
        " 'draw_if_interactive',\n",
        " 'dsplit',\n",
        " 'dstack',\n",
        " 'dtype',\n",
        " 'e',\n",
        " 'ediff1d',\n",
        " 'eig',\n",
        " 'eigh',\n",
        " 'eigvals',\n",
        " 'eigvalsh',\n",
        " 'einsum',\n",
        " 'emath',\n",
        " 'empty',\n",
        " 'empty_like',\n",
        " 'entropy',\n",
        " 'epoch2num',\n",
        " 'equal',\n",
        " 'errorbar',\n",
        " 'errstate',\n",
        " 'eventplot',\n",
        " 'exception_to_str',\n",
        " 'exit',\n",
        " 'exp',\n",
        " 'exp2',\n",
        " 'exp_safe',\n",
        " 'expand_dims',\n",
        " 'expm1',\n",
        " 'exponential',\n",
        " 'extract',\n",
        " 'eye',\n",
        " 'f',\n",
        " 'fabs',\n",
        " 'fastCopyAndTranspose',\n",
        " 'fft',\n",
        " 'fft2',\n",
        " 'fftfreq',\n",
        " 'fftn',\n",
        " 'fftpack',\n",
        " 'fftpack_lite',\n",
        " 'fftshift',\n",
        " 'fftsurr',\n",
        " 'figaspect',\n",
        " 'figimage',\n",
        " 'figlegend',\n",
        " 'fignum_exists',\n",
        " 'figsize',\n",
        " 'figtext',\n",
        " 'figure',\n",
        " 'fill',\n",
        " 'fill_between',\n",
        " 'fill_betweenx',\n",
        " 'fill_diagonal',\n",
        " 'find',\n",
        " 'find_common_type',\n",
        " 'findobj',\n",
        " 'finfo',\n",
        " 'fix',\n",
        " 'flag',\n",
        " 'flatiter',\n",
        " 'flatnonzero',\n",
        " 'flatten',\n",
        " 'flexible',\n",
        " 'fliplr',\n",
        " 'flipud',\n",
        " 'float128',\n",
        " 'float16',\n",
        " 'float32',\n",
        " 'float64',\n",
        " 'float_',\n",
        " 'floating',\n",
        " 'floor',\n",
        " 'floor_divide',\n",
        " 'fmax',\n",
        " 'fmin',\n",
        " 'fmod',\n",
        " 'format_parser',\n",
        " 'frange',\n",
        " 'frexp',\n",
        " 'frombuffer',\n",
        " 'fromfile',\n",
        " 'fromfunction',\n",
        " 'fromiter',\n",
        " 'frompyfunc',\n",
        " 'fromregex',\n",
        " 'fromstring',\n",
        " 'fv',\n",
        " 'gamma',\n",
        " 'gca',\n",
        " 'gcf',\n",
        " 'gci',\n",
        " 'generic',\n",
        " 'genfromtxt',\n",
        " 'geometric',\n",
        " 'get',\n",
        " 'get_array_wrap',\n",
        " 'get_backend',\n",
        " 'get_cmap',\n",
        " 'get_current_fig_manager',\n",
        " 'get_figlabels',\n",
        " 'get_fignums',\n",
        " 'get_include',\n",
        " 'get_ipython',\n",
        " 'get_numarray_include',\n",
        " 'get_plot_commands',\n",
        " 'get_printoptions',\n",
        " 'get_scale_docs',\n",
        " 'get_scale_names',\n",
        " 'get_sparse_matrix',\n",
        " 'get_state',\n",
        " 'get_xyz_where',\n",
        " 'getbuffer',\n",
        " 'getbufsize',\n",
        " 'geterr',\n",
        " 'geterrcall',\n",
        " 'geterrobj',\n",
        " 'getfigs',\n",
        " 'getp',\n",
        " 'ginput',\n",
        " 'gradient',\n",
        " 'gray',\n",
        " 'greater',\n",
        " 'greater_equal',\n",
        " 'grid',\n",
        " 'griddata',\n",
        " 'gumbel',\n",
        " 'half',\n",
        " 'hamming',\n",
        " 'hanning',\n",
        " 'help',\n",
        " 'helper',\n",
        " 'hexbin',\n",
        " 'hfft',\n",
        " 'hist',\n",
        " 'hist2d',\n",
        " 'histogram',\n",
        " 'histogram2d',\n",
        " 'histogramdd',\n",
        " 'hlines',\n",
        " 'hold',\n",
        " 'hot',\n",
        " 'hsplit',\n",
        " 'hstack',\n",
        " 'hsv',\n",
        " 'hypergeometric',\n",
        " 'hypot',\n",
        " 'i0',\n",
        " 'identity',\n",
        " 'ifft',\n",
        " 'ifft2',\n",
        " 'ifftn',\n",
        " 'ifftshift',\n",
        " 'ihfft',\n",
        " 'iinfo',\n",
        " 'imag',\n",
        " 'imread',\n",
        " 'imsave',\n",
        " 'imshow',\n",
        " 'in1d',\n",
        " 'index_exp',\n",
        " 'indices',\n",
        " 'inexact',\n",
        " 'inf',\n",
        " 'info',\n",
        " 'infty',\n",
        " 'inner',\n",
        " 'insert',\n",
        " 'inside_poly',\n",
        " 'int0',\n",
        " 'int16',\n",
        " 'int32',\n",
        " 'int64',\n",
        " 'int8',\n",
        " 'int_',\n",
        " 'int_asbuffer',\n",
        " 'intc',\n",
        " 'integer',\n",
        " 'interactive',\n",
        " 'interp',\n",
        " 'intersect1d',\n",
        " 'intp',\n",
        " 'inv',\n",
        " 'invert',\n",
        " 'ioff',\n",
        " 'ion',\n",
        " 'ipmt',\n",
        " 'irfft',\n",
        " 'irfft2',\n",
        " 'irfftn',\n",
        " 'irr',\n",
        " 'is_closed_polygon',\n",
        " 'is_numlike',\n",
        " 'is_string_like',\n",
        " 'iscomplex',\n",
        " 'iscomplexobj',\n",
        " 'isfinite',\n",
        " 'isfortran',\n",
        " 'ishold',\n",
        " 'isinf',\n",
        " 'isinteractive',\n",
        " 'isnan',\n",
        " 'isneginf',\n",
        " 'isposinf',\n",
        " 'ispower2',\n",
        " 'isreal',\n",
        " 'isrealobj',\n",
        " 'isscalar',\n",
        " 'issctype',\n",
        " 'issubclass_',\n",
        " 'issubdtype',\n",
        " 'issubsctype',\n",
        " 'isvector',\n",
        " 'iterable',\n",
        " 'ix_',\n",
        " 'jet',\n",
        " 'kaiser',\n",
        " 'kron',\n",
        " 'l1norm',\n",
        " 'l2norm',\n",
        " 'lapack_lite',\n",
        " 'laplace',\n",
        " 'ldexp',\n",
        " 'left_shift',\n",
        " 'legend',\n",
        " 'less',\n",
        " 'less_equal',\n",
        " 'levypdf',\n",
        " 'lexsort',\n",
        " 'linalg',\n",
        " 'linspace',\n",
        " 'little_endian',\n",
        " 'load',\n",
        " 'loads',\n",
        " 'loadtxt',\n",
        " 'locator_params',\n",
        " 'log',\n",
        " 'log10',\n",
        " 'log1p',\n",
        " 'log2',\n",
        " 'logaddexp',\n",
        " 'logaddexp2',\n",
        " 'logical_and',\n",
        " 'logical_not',\n",
        " 'logical_or',\n",
        " 'logical_xor',\n",
        " 'logistic',\n",
        " 'loglog',\n",
        " 'lognormal',\n",
        " 'logseries',\n",
        " 'logspace',\n",
        " 'longcomplex',\n",
        " 'longdouble',\n",
        " 'longest_contiguous_ones',\n",
        " 'longest_ones',\n",
        " 'longfloat',\n",
        " 'longlong',\n",
        " 'lookfor',\n",
        " 'lstsq',\n",
        " 'ma',\n",
        " 'mafromtxt',\n",
        " 'margins',\n",
        " 'mask_indices',\n",
        " 'mat',\n",
        " 'math',\n",
        " 'matplotlib',\n",
        " 'matrix',\n",
        " 'matrix_power',\n",
        " 'matrix_rank',\n",
        " 'matshow',\n",
        " 'maximum',\n",
        " 'maximum_sctype',\n",
        " 'may_share_memory',\n",
        " 'mean',\n",
        " 'median',\n",
        " 'memmap',\n",
        " 'meshgrid',\n",
        " 'mgrid',\n",
        " 'min_scalar_type',\n",
        " 'minimum',\n",
        " 'minorticks_off',\n",
        " 'minorticks_on',\n",
        " 'mintypecode',\n",
        " 'mirr',\n",
        " 'mlab',\n",
        " 'mod',\n",
        " 'modf',\n",
        " 'movavg',\n",
        " 'mpl',\n",
        " 'msort',\n",
        " 'multinomial',\n",
        " 'multiply',\n",
        " 'multivariate_normal',\n",
        " 'mx2num',\n",
        " 'nan',\n",
        " 'nan_to_num',\n",
        " 'nanargmax',\n",
        " 'nanargmin',\n",
        " 'nanmax',\n",
        " 'nanmin',\n",
        " 'nansum',\n",
        " 'nbytes',\n",
        " 'ndarray',\n",
        " 'ndenumerate',\n",
        " 'ndfromtxt',\n",
        " 'ndim',\n",
        " 'ndindex',\n",
        " 'nditer',\n",
        " 'negative',\n",
        " 'negative_binomial',\n",
        " 'nested_iters',\n",
        " 'new_figure_manager',\n",
        " 'newaxis',\n",
        " 'newbuffer',\n",
        " 'nextafter',\n",
        " 'noncentral_chisquare',\n",
        " 'noncentral_f',\n",
        " 'nonzero',\n",
        " 'norm',\n",
        " 'norm_flat',\n",
        " 'normal',\n",
        " 'normalize',\n",
        " 'normpdf',\n",
        " 'not_equal',\n",
        " 'np',\n",
        " 'nper',\n",
        " 'npv',\n",
        " 'num2date',\n",
        " 'num2epoch',\n",
        " 'number',\n",
        " 'numpy',\n",
        " 'obj2sctype',\n",
        " 'object0',\n",
        " 'object_',\n",
        " 'ogrid',\n",
        " 'ones',\n",
        " 'ones_like',\n",
        " 'outer',\n",
        " 'over',\n",
        " 'packbits',\n",
        " 'pareto',\n",
        " 'path_length',\n",
        " 'pause',\n",
        " 'pcolor',\n",
        " 'pcolormesh',\n",
        " 'percentile',\n",
        " 'permutation',\n",
        " 'pi',\n",
        " 'pie',\n",
        " 'piecewise',\n",
        " 'pink',\n",
        " 'pinv',\n",
        " 'pkgload',\n",
        " 'place',\n",
        " 'plb',\n",
        " 'plot',\n",
        " 'plot_date',\n",
        " 'plotfile',\n",
        " 'plotting',\n",
        " 'plt',\n",
        " 'pmt',\n",
        " 'poisson',\n",
        " 'polar',\n",
        " 'poly',\n",
        " 'poly1d',\n",
        " 'poly_below',\n",
        " 'poly_between',\n",
        " 'polyadd',\n",
        " 'polyder',\n",
        " 'polydiv',\n",
        " 'polyfit',\n",
        " 'polyint',\n",
        " 'polymul',\n",
        " 'polysub',\n",
        " 'polyval',\n",
        " 'power',\n",
        " 'ppmt',\n",
        " 'prctile',\n",
        " 'prctile_rank',\n",
        " 'prepca',\n",
        " 'print_function',\n",
        " 'prism',\n",
        " 'prod',\n",
        " 'product',\n",
        " 'promote_types',\n",
        " 'psd',\n",
        " 'ptp',\n",
        " 'put',\n",
        " 'putmask',\n",
        " 'pv',\n",
        " 'pylab',\n",
        " 'pylab_setup',\n",
        " 'pyplot',\n",
        " 'qr',\n",
        " 'quit',\n",
        " 'quiver',\n",
        " 'quiverkey',\n",
        " 'r_',\n",
        " 'rad2deg',\n",
        " 'radians',\n",
        " 'rand',\n",
        " 'randint',\n",
        " 'randn',\n",
        " 'random',\n",
        " 'random_integers',\n",
        " 'random_sample',\n",
        " 'ranf',\n",
        " 'rank',\n",
        " 'rate',\n",
        " 'ravel',\n",
        " 'ravel_multi_index',\n",
        " 'rayleigh',\n",
        " 'rc',\n",
        " 'rcParams',\n",
        " 'rcParamsDefault',\n",
        " 'rc_context',\n",
        " 'rcdefaults',\n",
        " 'real',\n",
        " 'real_if_close',\n",
        " 'rec',\n",
        " 'rec2csv',\n",
        " 'rec_append_fields',\n",
        " 'rec_drop_fields',\n",
        " 'rec_join',\n",
        " 'recarray',\n",
        " 'recfromcsv',\n",
        " 'recfromtxt',\n",
        " 'reciprocal',\n",
        " 'record',\n",
        " 'register_cmap',\n",
        " 'relativedelta',\n",
        " 'remainder',\n",
        " 'repeat',\n",
        " 'require',\n",
        " 'reshape',\n",
        " 'resize',\n",
        " 'restoredot',\n",
        " 'result_type',\n",
        " 'rfft',\n",
        " 'rfft2',\n",
        " 'rfftn',\n",
        " 'rgrids',\n",
        " 'right_shift',\n",
        " 'rint',\n",
        " 'rk4',\n",
        " 'rms_flat',\n",
        " 'roll',\n",
        " 'rollaxis',\n",
        " 'roots',\n",
        " 'rot90',\n",
        " 'round_',\n",
        " 'row_stack',\n",
        " 'rrule',\n",
        " 's_',\n",
        " 'safe_eval',\n",
        " 'sample',\n",
        " 'save',\n",
        " 'savefig',\n",
        " 'savetxt',\n",
        " 'savez',\n",
        " 'savez_compressed',\n",
        " 'sca',\n",
        " 'scatter',\n",
        " 'sci',\n",
        " 'sctype2char',\n",
        " 'sctypeDict',\n",
        " 'sctypeNA',\n",
        " 'sctypes',\n",
        " 'searchsorted',\n",
        " 'seed',\n",
        " 'segments_intersect',\n",
        " 'select',\n",
        " 'semilogx',\n",
        " 'semilogy',\n",
        " 'set_cmap',\n",
        " 'set_numeric_ops',\n",
        " 'set_printoptions',\n",
        " 'set_state',\n",
        " 'set_string_function',\n",
        " 'setbufsize',\n",
        " 'setdiff1d',\n",
        " 'seterr',\n",
        " 'seterrcall',\n",
        " 'seterrobj',\n",
        " 'setp',\n",
        " 'setxor1d',\n",
        " 'shape',\n",
        " 'short',\n",
        " 'show',\n",
        " 'show_config',\n",
        " 'shuffle',\n",
        " 'sign',\n",
        " 'signbit',\n",
        " 'signedinteger',\n",
        " 'silent_list',\n",
        " 'sin',\n",
        " 'sinc',\n",
        " 'single',\n",
        " 'singlecomplex',\n",
        " 'sinh',\n",
        " 'size',\n",
        " 'slogdet',\n",
        " 'slopes',\n",
        " 'solve',\n",
        " 'sometrue',\n",
        " 'sort',\n",
        " 'sort_complex',\n",
        " 'source',\n",
        " 'spacing',\n",
        " 'specgram',\n",
        " 'spectral',\n",
        " 'split',\n",
        " 'spring',\n",
        " 'spy',\n",
        " 'sqrt',\n",
        " 'square',\n",
        " 'squeeze',\n",
        " 'stackplot',\n",
        " 'standard_cauchy',\n",
        " 'standard_exponential',\n",
        " 'standard_gamma',\n",
        " 'standard_normal',\n",
        " 'standard_t',\n",
        " 'std',\n",
        " 'stem',\n",
        " 'step',\n",
        " 'stineman_interp',\n",
        " 'str_',\n",
        " 'streamplot',\n",
        " 'string0',\n",
        " 'string_',\n",
        " 'strpdate2num',\n",
        " 'subplot',\n",
        " 'subplot2grid',\n",
        " 'subplot_tool',\n",
        " 'subplots',\n",
        " 'subplots_adjust',\n",
        " 'subtract',\n",
        " 'sum',\n",
        " 'summer',\n",
        " 'suptitle',\n",
        " 'svd',\n",
        " 'swapaxes',\n",
        " 'switch_backend',\n",
        " 'sys',\n",
        " 'table',\n",
        " 'take',\n",
        " 'tan',\n",
        " 'tanh',\n",
        " 'tensordot',\n",
        " 'tensorinv',\n",
        " 'tensorsolve',\n",
        " 'test',\n",
        " 'text',\n",
        " 'thetagrids',\n",
        " 'tick_params',\n",
        " 'ticklabel_format',\n",
        " 'tight_layout',\n",
        " 'tile',\n",
        " 'timedelta64',\n",
        " 'timedelta_',\n",
        " 'timeinteger',\n",
        " 'title',\n",
        " 'trace',\n",
        " 'transpose',\n",
        " 'trapz',\n",
        " 'tri',\n",
        " 'triangular',\n",
        " 'tricontour',\n",
        " 'tricontourf',\n",
        " 'tril',\n",
        " 'tril_indices',\n",
        " 'tril_indices_from',\n",
        " 'trim_zeros',\n",
        " 'tripcolor',\n",
        " 'triplot',\n",
        " 'triu',\n",
        " 'triu_indices',\n",
        " 'triu_indices_from',\n",
        " 'true_divide',\n",
        " 'trunc',\n",
        " 'twinx',\n",
        " 'twiny',\n",
        " 'typeDict',\n",
        " 'typeNA',\n",
        " 'typecodes',\n",
        " 'typename',\n",
        " 'ubyte',\n",
        " 'ufunc',\n",
        " 'uint',\n",
        " 'uint0',\n",
        " 'uint16',\n",
        " 'uint32',\n",
        " 'uint64',\n",
        " 'uint8',\n",
        " 'uintc',\n",
        " 'uintp',\n",
        " 'ulonglong',\n",
        " 'unicode0',\n",
        " 'unicode_',\n",
        " 'uniform',\n",
        " 'union1d',\n",
        " 'unique',\n",
        " 'unpackbits',\n",
        " 'unravel_index',\n",
        " 'unsignedinteger',\n",
        " 'unwrap',\n",
        " 'ushort',\n",
        " 'vander',\n",
        " 'var',\n",
        " 'vdot',\n",
        " 'vector_lengths',\n",
        " 'vectorize',\n",
        " 'vlines',\n",
        " 'void',\n",
        " 'void0',\n",
        " 'vonmises',\n",
        " 'vsplit',\n",
        " 'vstack',\n",
        " 'waitforbuttonpress',\n",
        " 'wald',\n",
        " 'warnings',\n",
        " 'weibull',\n",
        " 'where',\n",
        " 'who',\n",
        " 'window_hanning',\n",
        " 'window_none',\n",
        " 'winter',\n",
        " 'x',\n",
        " 'xcorr',\n",
        " 'xkcd',\n",
        " 'xlabel',\n",
        " 'xlim',\n",
        " 'xscale',\n",
        " 'xticks',\n",
        " 'y',\n",
        " 'y_rand',\n",
        " 'ylabel',\n",
        " 'ylim',\n",
        " 'yscale',\n",
        " 'yticks',\n",
        " 'zeros',\n",
        " 'zeros_like',\n",
        " 'zipf']"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(optimize)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on package scipy.optimize in scipy:\n",
        "\n",
        "NAME\n",
        "    scipy.optimize\n",
        "\n",
        "FILE\n",
        "    /Users/bri/anaconda/lib/python2.7/site-packages/scipy/optimize/__init__.py\n",
        "\n",
        "DESCRIPTION\n",
        "    =====================================================\n",
        "    Optimization and root finding (:mod:`scipy.optimize`)\n",
        "    =====================================================\n",
        "    \n",
        "    .. currentmodule:: scipy.optimize\n",
        "    \n",
        "    Optimization\n",
        "    ============\n",
        "    \n",
        "    General-purpose\n",
        "    ---------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize - Unified interface for minimizers of multivariate functions\n",
        "       fmin - Nelder-Mead Simplex algorithm\n",
        "       fmin_powell - Powell's (modified) level set method\n",
        "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm\n",
        "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno)\n",
        "       fmin_ncg - Line-search Newton Conjugate Gradient\n",
        "       leastsq - Minimize the sum of squares of M equations in N unknowns\n",
        "    \n",
        "    Constrained (multivariate)\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer\n",
        "       fmin_tnc - Truncated Newton code\n",
        "       fmin_cobyla - Constrained optimization by linear approximation\n",
        "       fmin_slsqp - Minimization using sequential least-squares programming\n",
        "       nnls - Linear least-squares problem with non-negativity constraint\n",
        "    \n",
        "    Global\n",
        "    ------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       anneal - Simulated annealing\n",
        "       basinhopping - Basinhopping stochastic optimizer\n",
        "       brute - Brute force searching optimizer\n",
        "    \n",
        "    Scalar function minimizers\n",
        "    --------------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       minimize_scalar - Unified interface for minimizers of univariate functions\n",
        "       fminbound - Bounded minimization of a scalar function\n",
        "       brent - 1-D function minimization using Brent method\n",
        "       golden - 1-D function minimization using Golden Section method\n",
        "       bracket - Bracket a minimum, given two starting points\n",
        "    \n",
        "    Rosenbrock function\n",
        "    -------------------\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       rosen - The Rosenbrock function.\n",
        "       rosen_der - The derivative of the Rosenbrock function.\n",
        "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
        "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
        "    \n",
        "    Fitting\n",
        "    =======\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       curve_fit -- Fit curve to a set of points\n",
        "    \n",
        "    Root finding\n",
        "    ============\n",
        "    \n",
        "    Scalar functions\n",
        "    ----------------\n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       brentq - quadratic interpolation Brent method\n",
        "       brenth - Brent method, modified by Harris with hyperbolic extrapolation\n",
        "       ridder - Ridder's method\n",
        "       bisect - Bisection method\n",
        "       newton - Secant method or Newton's method\n",
        "    \n",
        "    Fixed point finding:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       fixed_point - Single-variable fixed-point solver\n",
        "    \n",
        "    Multidimensional\n",
        "    ----------------\n",
        "    \n",
        "    General nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       root - Unified interface for nonlinear solvers of multivariate functions\n",
        "       fsolve - Non-linear multi-variable equation solver\n",
        "       broyden1 - Broyden's first method\n",
        "       broyden2 - Broyden's second method\n",
        "    \n",
        "    Large-scale nonlinear solvers:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       newton_krylov\n",
        "       anderson\n",
        "    \n",
        "    Simple iterations:\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       excitingmixing\n",
        "       linearmixing\n",
        "       diagbroyden\n",
        "    \n",
        "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
        "    \n",
        "    Utility Functions\n",
        "    =================\n",
        "    \n",
        "    .. autosummary::\n",
        "       :toctree: generated/\n",
        "    \n",
        "       line_search - Return a step that satisfies the strong Wolfe conditions\n",
        "       check_grad - Check the supplied derivative using finite differences\n",
        "    \n",
        "       show_options - Show specific options optimization solvers\n",
        "\n",
        "PACKAGE CONTENTS\n",
        "    _basinhopping\n",
        "    _cobyla\n",
        "    _lbfgsb\n",
        "    _minimize\n",
        "    _minpack\n",
        "    _nnls\n",
        "    _root\n",
        "    _slsqp\n",
        "    _trustregion\n",
        "    _trustregion_dogleg\n",
        "    _trustregion_ncg\n",
        "    _tstutils\n",
        "    _zeros\n",
        "    anneal\n",
        "    cobyla\n",
        "    lbfgsb\n",
        "    linesearch\n",
        "    minpack\n",
        "    minpack2\n",
        "    moduleTNC\n",
        "    nnls\n",
        "    nonlin\n",
        "    optimize\n",
        "    setup\n",
        "    slsqp\n",
        "    tnc\n",
        "    zeros\n",
        "\n",
        "CLASSES\n",
        "    __builtin__.dict(__builtin__.object)\n",
        "        scipy.optimize.optimize.Result\n",
        "    exceptions.UserWarning(exceptions.Warning)\n",
        "        scipy.optimize.optimize.OptimizeWarning\n",
        "    \n",
        "    class OptimizeWarning(exceptions.UserWarning)\n",
        "     |  Method resolution order:\n",
        "     |      OptimizeWarning\n",
        "     |      exceptions.UserWarning\n",
        "     |      exceptions.Warning\n",
        "     |      exceptions.Exception\n",
        "     |      exceptions.BaseException\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from exceptions.UserWarning:\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __delattr__(...)\n",
        "     |      x.__delattr__('name') <==> del x.name\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __getslice__(...)\n",
        "     |      x.__getslice__(i, j) <==> x[i:j]\n",
        "     |      \n",
        "     |      Use of negative indices is not supported.\n",
        "     |  \n",
        "     |  __reduce__(...)\n",
        "     |  \n",
        "     |  __repr__(...)\n",
        "     |      x.__repr__() <==> repr(x)\n",
        "     |  \n",
        "     |  __setattr__(...)\n",
        "     |      x.__setattr__('name', value) <==> x.name = value\n",
        "     |  \n",
        "     |  __setstate__(...)\n",
        "     |  \n",
        "     |  __str__(...)\n",
        "     |      x.__str__() <==> str(x)\n",
        "     |  \n",
        "     |  __unicode__(...)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |  \n",
        "     |  args\n",
        "     |  \n",
        "     |  message\n",
        "    \n",
        "    class Result(__builtin__.dict)\n",
        "     |  Represents the optimization result.\n",
        "     |  \n",
        "     |  Attributes\n",
        "     |  ----------\n",
        "     |  x : ndarray\n",
        "     |      The solution of the optimization.\n",
        "     |  success : bool\n",
        "     |      Whether or not the optimizer exited successfully.\n",
        "     |  status : int\n",
        "     |      Termination status of the optimizer. Its value depends on the\n",
        "     |      underlying solver. Refer to `message` for details.\n",
        "     |  message : str\n",
        "     |      Description of the cause of the termination.\n",
        "     |  fun, jac, hess, hess_inv : ndarray\n",
        "     |      Values of objective function, Jacobian, Hessian or its inverse (if\n",
        "     |      available). The Hessians may be approximations, see the documentation\n",
        "     |      of the function in question.\n",
        "     |  nfev, njev, nhev : int\n",
        "     |      Number of evaluations of the objective functions and of its\n",
        "     |      Jacobian and Hessian.\n",
        "     |  nit : int\n",
        "     |      Number of iterations performed by the optimizer.\n",
        "     |  maxcv : float\n",
        "     |      The maximum constraint violation.\n",
        "     |  \n",
        "     |  Notes\n",
        "     |  -----\n",
        "     |  There may be additional attributes not listed above depending of the\n",
        "     |  specific solver. Since this class is essentially a subclass of dict\n",
        "     |  with attribute accessors, one can see which attributes are available\n",
        "     |  using the `keys()` method.\n",
        "     |  \n",
        "     |  Method resolution order:\n",
        "     |      Result\n",
        "     |      __builtin__.dict\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Methods defined here:\n",
        "     |  \n",
        "     |  __delattr__ = __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __getattr__(self, name)\n",
        "     |  \n",
        "     |  __repr__(self)\n",
        "     |  \n",
        "     |  __setattr__ = __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |      dictionary for instance variables (if defined)\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __cmp__(...)\n",
        "     |      x.__cmp__(y) <==> cmp(x,y)\n",
        "     |  \n",
        "     |  __contains__(...)\n",
        "     |      D.__contains__(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  __delitem__(...)\n",
        "     |      x.__delitem__(y) <==> del x[y]\n",
        "     |  \n",
        "     |  __eq__(...)\n",
        "     |      x.__eq__(y) <==> x==y\n",
        "     |  \n",
        "     |  __ge__(...)\n",
        "     |      x.__ge__(y) <==> x>=y\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __gt__(...)\n",
        "     |      x.__gt__(y) <==> x>y\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  __iter__(...)\n",
        "     |      x.__iter__() <==> iter(x)\n",
        "     |  \n",
        "     |  __le__(...)\n",
        "     |      x.__le__(y) <==> x<=y\n",
        "     |  \n",
        "     |  __len__(...)\n",
        "     |      x.__len__() <==> len(x)\n",
        "     |  \n",
        "     |  __lt__(...)\n",
        "     |      x.__lt__(y) <==> x<y\n",
        "     |  \n",
        "     |  __ne__(...)\n",
        "     |      x.__ne__(y) <==> x!=y\n",
        "     |  \n",
        "     |  __setitem__(...)\n",
        "     |      x.__setitem__(i, y) <==> x[i]=y\n",
        "     |  \n",
        "     |  __sizeof__(...)\n",
        "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
        "     |  \n",
        "     |  clear(...)\n",
        "     |      D.clear() -> None.  Remove all items from D.\n",
        "     |  \n",
        "     |  copy(...)\n",
        "     |      D.copy() -> a shallow copy of D\n",
        "     |  \n",
        "     |  fromkeys(...)\n",
        "     |      dict.fromkeys(S[,v]) -> New dict with keys from S and values equal to v.\n",
        "     |      v defaults to None.\n",
        "     |  \n",
        "     |  get(...)\n",
        "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
        "     |  \n",
        "     |  has_key(...)\n",
        "     |      D.has_key(k) -> True if D has a key k, else False\n",
        "     |  \n",
        "     |  items(...)\n",
        "     |      D.items() -> list of D's (key, value) pairs, as 2-tuples\n",
        "     |  \n",
        "     |  iteritems(...)\n",
        "     |      D.iteritems() -> an iterator over the (key, value) items of D\n",
        "     |  \n",
        "     |  iterkeys(...)\n",
        "     |      D.iterkeys() -> an iterator over the keys of D\n",
        "     |  \n",
        "     |  itervalues(...)\n",
        "     |      D.itervalues() -> an iterator over the values of D\n",
        "     |  \n",
        "     |  keys(...)\n",
        "     |      D.keys() -> list of D's keys\n",
        "     |  \n",
        "     |  pop(...)\n",
        "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
        "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
        "     |  \n",
        "     |  popitem(...)\n",
        "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
        "     |      2-tuple; but raise KeyError if D is empty.\n",
        "     |  \n",
        "     |  setdefault(...)\n",
        "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
        "     |  \n",
        "     |  update(...)\n",
        "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
        "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
        "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
        "     |      In either case, this is followed by: for k in F: D[k] = F[k]\n",
        "     |  \n",
        "     |  values(...)\n",
        "     |      D.values() -> list of D's values\n",
        "     |  \n",
        "     |  viewitems(...)\n",
        "     |      D.viewitems() -> a set-like object providing a view on D's items\n",
        "     |  \n",
        "     |  viewkeys(...)\n",
        "     |      D.viewkeys() -> a set-like object providing a view on D's keys\n",
        "     |  \n",
        "     |  viewvalues(...)\n",
        "     |      D.viewvalues() -> an object providing a view on D's values\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from __builtin__.dict:\n",
        "     |  \n",
        "     |  __hash__ = None\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "\n",
        "FUNCTIONS\n",
        "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using (extended) Anderson mixing.\n",
        "        \n",
        "        The Jacobian is formed by for a 'best' solution in the space\n",
        "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
        "        inversions and MxN multiplications are required. [Ey]_\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        M : float, optional\n",
        "            Number of previous vectors to retain. Defaults to 5.\n",
        "        w0 : float, optional\n",
        "            Regularization parameter for numerical stability.\n",
        "            Compared to unity, good values of the order of 0.01.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
        "    \n",
        "    anneal(func, x0, args=(), schedule='fast', full_output=0, T0=None, Tf=1e-12, maxeval=None, maxaccept=None, maxiter=400, boltzmann=1.0, learn_rate=0.5, feps=1e-06, quench=1.0, m=1.0, n=1.0, lower=-100, upper=100, dwell=50, disp=True)\n",
        "        Minimize a function using simulated annealing.\n",
        "        \n",
        "        Uses simulated annealing, a random algorithm that uses no derivative\n",
        "        information from the function being optimized. Other names for this\n",
        "        family of approaches include: \"Monte Carlo\", \"Metropolis\",\n",
        "        \"Metropolis-Hastings\", `etc`. They all involve (a) evaluating the\n",
        "        objective function on a random set of points, (b) keeping those that\n",
        "        pass their randomized evaluation critera, (c) cooling (`i.e.`,\n",
        "        tightening) the evaluation critera, and (d) repeating until their\n",
        "        termination critera are met.  In practice they have been used mainly in\n",
        "        discrete rather than in continuous optimization.\n",
        "        \n",
        "        Available annealing schedules are 'fast', 'cauchy' and 'boltzmann'.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized.  Must be in the form\n",
        "            `f(x, *args)`, where `x` is the argument in the form of a 1-D array\n",
        "            and `args` is a  tuple of any additional fixed parameters needed to\n",
        "            completely specify the function.\n",
        "        x0: 1-D array\n",
        "            An initial guess at the optimizing argument of `func`.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely\n",
        "            specify the objective function.\n",
        "        schedule : str, optional\n",
        "            The annealing schedule to use.  Must be one of 'fast', 'cauchy' or\n",
        "            'boltzmann'.  See `Notes`.\n",
        "        full_output : bool, optional\n",
        "            If `full_output`, then return all values listed in the Returns\n",
        "            section. Otherwise, return just the `xmin` and `status` values.\n",
        "        T0 : float, optional\n",
        "            The initial \"temperature\".  If None, then estimate it as 1.2 times\n",
        "            the largest cost-function deviation over random points in the\n",
        "            box-shaped region specified by the `lower, upper` input parameters.\n",
        "        Tf : float, optional\n",
        "            Final goal temperature.  Cease iterations if the temperature\n",
        "            falls below `Tf`.\n",
        "        maxeval : int, optional\n",
        "            Cease iterations if the number of function evaluations exceeds\n",
        "            `maxeval`.\n",
        "        maxaccept : int, optional\n",
        "            Cease iterations if the number of points accepted exceeds `maxaccept`.\n",
        "            See `Notes` for the probabilistic acceptance criteria used.\n",
        "        maxiter : int, optional\n",
        "            Cease iterations if the number of cooling iterations exceeds `maxiter`.\n",
        "        learn_rate : float, optional\n",
        "            Scale constant for tuning the probabilistc acceptance criteria.\n",
        "        boltzmann : float, optional\n",
        "            Boltzmann constant in the probabilistic acceptance criteria\n",
        "            (increase for less stringent criteria at each temperature).\n",
        "        feps : float, optional\n",
        "            Cease iterations if the relative errors in the function value over the\n",
        "            last four coolings is below `feps`.\n",
        "        quench, m, n : floats, optional\n",
        "            Parameters to alter the `fast` simulated annealing schedule.\n",
        "            See `Notes`.\n",
        "        lower, upper : floats or 1-D arrays, optional\n",
        "            Lower and upper bounds on the argument `x`.  If floats are provided,\n",
        "            they apply to all components of `x`.\n",
        "        dwell : int, optional\n",
        "            The number of times to execute the inner loop at each value of the\n",
        "            temperature.  See `Notes`.\n",
        "        disp : bool, optional\n",
        "            Print a descriptive convergence message if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            The point where the lowest function value was found.\n",
        "        Jmin : float\n",
        "            The objective function value at `xmin`.\n",
        "        T : float\n",
        "            The temperature at termination of the iterations.\n",
        "        feval : int\n",
        "            Number of function evaluations used.\n",
        "        iters : int\n",
        "            Number of cooling iterations used.\n",
        "        accept : int\n",
        "            Number of tests accepted.\n",
        "        status : int\n",
        "            A code indicating the reason for termination:\n",
        "        \n",
        "            - 0 : Points no longer changing.\n",
        "            - 1 : Cooled to final temperature.\n",
        "            - 2 : Maximum function evaluations reached.\n",
        "            - 3 : Maximum cooling iterations reached.\n",
        "            - 4 : Maximum accepted query locations reached.\n",
        "            - 5 : Final point not the minimum amongst encountered points.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        basinhopping : another (more performant) global optimizer\n",
        "        brute : brute-force global optimizer\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Simulated annealing is a random algorithm which uses no derivative\n",
        "        information from the function being optimized. In practice it has\n",
        "        been more useful in discrete optimization than continuous\n",
        "        optimization, as there are usually better algorithms for continuous\n",
        "        optimization problems.\n",
        "        \n",
        "        Some experimentation by trying the different temperature\n",
        "        schedules and altering their parameters is likely required to\n",
        "        obtain good performance.\n",
        "        \n",
        "        The randomness in the algorithm comes from random sampling in numpy.\n",
        "        To obtain the same results you can call `numpy.random.seed` with the\n",
        "        same seed immediately before calling `anneal`.\n",
        "        \n",
        "        We give a brief description of how the three temperature schedules\n",
        "        generate new points and vary their temperature.  Temperatures are\n",
        "        only updated with iterations in the outer loop.  The inner loop is\n",
        "        over loop over ``xrange(dwell)``, and new points are generated for\n",
        "        every iteration in the inner loop.  Whether the proposed new points\n",
        "        are accepted is probabilistic.\n",
        "        \n",
        "        For readability, let ``d`` denote the dimension of the inputs to func.\n",
        "        Also, let ``x_old`` denote the previous state, and ``k`` denote the\n",
        "        iteration number of the outer loop.  All other variables not\n",
        "        defined below are input variables to `anneal` itself.\n",
        "        \n",
        "        In the 'fast' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(0, 1, size = d)\n",
        "            y = sgn(u - 0.5) * T * ((1 + 1/T)**abs(2*u - 1) - 1.0)\n",
        "        \n",
        "            xc = y * (upper - lower)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            c = n * exp(-n * quench)\n",
        "            T_new = T0 * exp(-c * k**quench)\n",
        "        \n",
        "        In the 'cauchy' schedule the updates are::\n",
        "        \n",
        "            u ~ Uniform(-pi/2, pi/2, size=d)\n",
        "            xc = learn_rate * T * tan(u)\n",
        "            x_new = x_old + xc\n",
        "        \n",
        "            T_new = T0 / (1 + k)\n",
        "        \n",
        "        In the 'boltzmann' schedule the updates are::\n",
        "        \n",
        "            std = minimum(sqrt(T) * ones(d), (upper - lower) / (3*learn_rate))\n",
        "            y ~ Normal(0, std, size = d)\n",
        "            x_new = x_old + learn_rate * y\n",
        "        \n",
        "            T_new = T0 / log(1 + k)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        [1] P. J. M. van Laarhoven and E. H. L. Aarts, \"Simulated Annealing: Theory\n",
        "            and Applications\", Kluwer Academic Publishers, 1987.\n",
        "        \n",
        "        [2] W.H. Press et al., \"Numerical Recipies: The Art of Scientific Computing\",\n",
        "            Cambridge U. Press, 1987.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        *Example 1.* We illustrate the use of `anneal` to seek the global minimum\n",
        "        of a function of two variables that is equal to the sum of a positive-\n",
        "        definite quadratic and two deep \"Gaussian-shaped\" craters.  Specifically,\n",
        "        define the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``, ``params``, and the functions are\n",
        "        as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        >>> x0 = np.array([2., 2.])     # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(555)   # Seeded to allow replication.\n",
        "        >>> res = optimize.anneal(f, x0, args=params, schedule='boltzmann',\n",
        "                                  full_output=True, maxiter=500, lower=-10,\n",
        "                                  upper=10, dwell=250, disp=True)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res[0]  # obtained minimum\n",
        "        array([-1.03914194,  1.81330654])\n",
        "        >>> res[1]  # function value at minimum\n",
        "        -3.3817...\n",
        "        \n",
        "        So this run settled on the point [-1.039, 1.813] with a minimum function\n",
        "        value of about -3.382.  The final temperature was about 212. The run used\n",
        "        125301 function evaluations, 501 iterations (including the initial guess as\n",
        "        a iteration), and accepted 61162 points. The status flag of 3 also\n",
        "        indicates that `maxiter` was reached.\n",
        "        \n",
        "        This problem's true global minimum lies near the point [-1.057, 1.808]\n",
        "        and has a value of about -3.409.  So these `anneal` results are pretty\n",
        "        good and could be used as the starting guess in a local optimizer to\n",
        "        seek a more exact local minimum.\n",
        "        \n",
        "        *Example 2.* To minimize the same objective function using\n",
        "        the `minimize` approach, we need to (a) convert the options to an\n",
        "        \"options dictionary\" using the keys prescribed for this method,\n",
        "        (b) call the `minimize` function with the name of the method (which\n",
        "        in this case is 'Anneal'), and (c) take account of the fact that\n",
        "        the returned value will be a `Result` object (`i.e.`, a dictionary,\n",
        "        as defined in `optimize.py`).\n",
        "        \n",
        "        All of the allowable options for 'Anneal' when using the `minimize`\n",
        "        approach are listed in the ``myopts`` dictionary given below, although\n",
        "        in practice only the non-default values would be needed.  Some of their\n",
        "        names differ from those used in the `anneal` approach.  We can proceed\n",
        "        as follows:\n",
        "        \n",
        "        >>> myopts = {\n",
        "                'schedule'     : 'boltzmann',   # Non-default value.\n",
        "                'maxfev'       : None,  # Default, formerly `maxeval`.\n",
        "                'maxiter'      : 500,   # Non-default value.\n",
        "                'maxaccept'    : None,  # Default value.\n",
        "                'ftol'         : 1e-6,  # Default, formerly `feps`.\n",
        "                'T0'           : None,  # Default value.\n",
        "                'Tf'           : 1e-12, # Default value.\n",
        "                'boltzmann'    : 1.0,   # Default value.\n",
        "                'learn_rate'   : 0.5,   # Default value.\n",
        "                'quench'       : 1.0,   # Default value.\n",
        "                'm'            : 1.0,   # Default value.\n",
        "                'n'            : 1.0,   # Default value.\n",
        "                'lower'        : -10,   # Non-default value.\n",
        "                'upper'        : +10,   # Non-default value.\n",
        "                'dwell'        : 250,   # Non-default value.\n",
        "                'disp'         : True   # Default value.\n",
        "                }\n",
        "        >>> from scipy import optimize\n",
        "        >>> np.random.seed(777)  # Seeded to allow replication.\n",
        "        >>> res2 = optimize.minimize(f, x0, args=params, method='Anneal',\n",
        "                                     options=myopts)\n",
        "        Warning: Maximum number of iterations exceeded.\n",
        "        >>> res2\n",
        "          status: 3\n",
        "         success: False\n",
        "          accept: 61742\n",
        "            nfev: 125301\n",
        "               T: 214.20624873839623\n",
        "             fun: -3.4084065576676053\n",
        "               x: array([-1.05757366,  1.8071427 ])\n",
        "         message: 'Maximum cooling iterations reached'\n",
        "         nit: 501\n",
        "    \n",
        "    approx_fprime(xk, f, epsilon, *args)\n",
        "        Finite-difference approximation of the gradient of a scalar function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        xk : array_like\n",
        "            The coordinate vector at which to determine the gradient of `f`.\n",
        "        f : callable\n",
        "            The function of which to determine the gradient (partial derivatives).\n",
        "            Should take `xk` as first argument, other arguments to `f` can be\n",
        "            supplied in ``*args``.  Should return a scalar, the value of the\n",
        "            function at `xk`.\n",
        "        epsilon : array_like\n",
        "            Increment to `xk` to use for determining the function gradient.\n",
        "            If a scalar, uses the same finite difference delta for all partial\n",
        "            derivatives.  If an array, should contain one value per element of\n",
        "            `xk`.\n",
        "        \\*args : args, optional\n",
        "            Any other arguments that are to be passed to `f`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        grad : ndarray\n",
        "            The partial derivatives of `f` to `xk`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        check_grad : Check correctness of gradient function against approx_fprime.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The function gradient is determined by the forward finite difference\n",
        "        formula::\n",
        "        \n",
        "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
        "            f'[i] = ---------------------------------\n",
        "                                epsilon[i]\n",
        "        \n",
        "        The main use of `approx_fprime` is in scalar function optimizers like\n",
        "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c0, c1):\n",
        "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
        "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
        "        \n",
        "        >>> x = np.ones(2)\n",
        "        >>> c0, c1 = (1, 200)\n",
        "        >>> eps = np.sqrt(np.finfo(np.float).eps)\n",
        "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
        "        array([   2.        ,  400.00004198])\n",
        "    \n",
        "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None)\n",
        "        Find the global minimum of a function using the basin-hopping algorithm\n",
        "        \n",
        "        .. versionadded:: 0.12.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            Function to be optimized.  ``args`` can be passed as an optional item\n",
        "            in the dict ``minimizer_kwargs``\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        niter : integer, optional\n",
        "            The number of basin hopping iterations\n",
        "        T : float, optional\n",
        "            The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
        "            \"temperatures\" mean that larger jumps in function value will be\n",
        "            accepted.  For best results ``T`` should be comparable to the\n",
        "            separation\n",
        "            (in function value) between local minima.\n",
        "        stepsize : float, optional\n",
        "            initial step size for use in the random displacement.\n",
        "        minimizer_kwargs : dict, optional\n",
        "            Extra keyword arguments to be passed to the minimizer\n",
        "            ``scipy.optimize.minimize()`` Some important options could be:\n",
        "                method : str\n",
        "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
        "                args : tuple\n",
        "                    Extra arguments passed to the objective function (``func``) and\n",
        "                    its derivatives (Jacobian, Hessian).\n",
        "        \n",
        "        take_step : callable ``take_step(x)``, optional\n",
        "            Replace the default step taking routine with this routine.  The default\n",
        "            step taking routine is a random displacement of the coordinates, but\n",
        "            other step taking algorithms may be better for some systems.\n",
        "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
        "            If this attribute exists, then ``basinhopping`` will adjust\n",
        "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
        "            search.\n",
        "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
        "            Define a test which will be used to judge whether or not to accept the\n",
        "            step.  This will be used in addition to the Metropolis test based on\n",
        "            \"temperature\" ``T``.  The acceptable return values are True,\n",
        "            False, or ``\"force accept\"``.  If the latter, then this will\n",
        "            override any other tests in order to accept the step.  This can be\n",
        "            used, for example, to forcefully escape from a local minimum that\n",
        "            ``basinhopping`` is trapped in.\n",
        "        callback : callable, ``callback(x, f, accept)``, optional\n",
        "            A callback function which will be called for all minimum found.  ``x``\n",
        "            and ``f`` are the coordinates and function value of the trial minima,\n",
        "            and ``accept`` is whether or not that minima was accepted.  This can be\n",
        "            used, for example, to save the lowest N minima found.  Also,\n",
        "            ``callback`` can be used to specify a user defined stop criterion by\n",
        "            optionally returning True to stop the ``basinhopping`` routine.\n",
        "        interval : integer, optional\n",
        "            interval for how often to update the ``stepsize``\n",
        "        disp : bool, optional\n",
        "            Set to True to print status messages\n",
        "        niter_success : integer, optional\n",
        "            Stop the run if the global minimum candidate remains the same for this\n",
        "            number of iterations.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.  Important\n",
        "            attributes are: ``x`` the solution array, ``fun`` the value of the\n",
        "            function at the solution, and ``message`` which describes the cause of\n",
        "            the termination. See `Result` for a description of other attributes.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize :\n",
        "            The local minimization function called once for each basinhopping step.\n",
        "            ``minimizer_kwargs`` is passed to this routine.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
        "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
        "        [4]_.  The algorithm in its current form was described by David Wales and\n",
        "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
        "        \n",
        "        The algorithm is iterative with each cycle composed of the following\n",
        "        features\n",
        "        \n",
        "        1) random perturbation of the coordinates\n",
        "        \n",
        "        2) local minimization\n",
        "        \n",
        "        3) accept or reject the new coordinates based on the minimized function\n",
        "           value\n",
        "        \n",
        "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
        "        Carlo algorithms, although there are many other possibilities [3]_.\n",
        "        \n",
        "        This global minimization method has been shown to be extremely efficient\n",
        "        for a wide variety of problems in physics and chemistry.  It is\n",
        "        particularly useful when the function has many minima separated by large\n",
        "        barriers. See the Cambridge Cluster Database\n",
        "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
        "        that have been optimized primarily using basin-hopping.  This database\n",
        "        includes minimization problems exceeding 300 degrees of freedom.\n",
        "        \n",
        "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
        "        a Fortran implementation of basin-hopping.  This implementation has many\n",
        "        different variations of the procedure described above, including more\n",
        "        advanced step taking algorithms and alternate acceptance criterion.\n",
        "        \n",
        "        For stochastic global optimization there is no way to determine if the true\n",
        "        global minimum has actually been found. Instead, as a consistency check,\n",
        "        the algorithm can be run from a number of different random starting points\n",
        "        to ensure the lowest minimum found in each example has converged to the\n",
        "        global minimum.  For this reason ``basinhopping`` will by default simply\n",
        "        run for the number of iterations ``niter`` and return the lowest minimum\n",
        "        found.  It is left to the user to ensure that this is in fact the global\n",
        "        minimum.\n",
        "        \n",
        "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
        "        depends on the problem being solved.  Ideally it should be comparable to\n",
        "        the typical separation between local minima of the function being\n",
        "        optimized.  ``basinhopping`` will, by default, adjust ``stepsize`` to find\n",
        "        an optimal value, but this may take many iterations.  You will get quicker\n",
        "        results if you set a sensible value for ``stepsize``.\n",
        "        \n",
        "        Choosing ``T``: The parameter ``T`` is the temperature used in the\n",
        "        metropolis criterion.  Basinhopping steps are accepted with probability\n",
        "        ``1`` if ``func(xnew) < func(xold)``, or otherwise with probability::\n",
        "        \n",
        "            exp( -(func(xnew) - func(xold)) / T )\n",
        "        \n",
        "        So, for best results, ``T`` should to be comparable to the typical\n",
        "        difference in function value between between local minima\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
        "            Cambridge, UK.\n",
        "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
        "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
        "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
        "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
        "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
        "            1987, 84, 6611.\n",
        "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
        "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following example is a one-dimensional minimization problem,  with many\n",
        "        local minima superimposed on a parabola.\n",
        "        \n",
        "        >>> func = lambda x: cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
        "        >>> x0=[1.]\n",
        "        \n",
        "        Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
        "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
        "        use and how to set up that minimizer.  This parameter will be passed to\n",
        "        ``scipy.optimize.minimize()``.\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
        "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
        "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
        "        \n",
        "        Next consider a two-dimensional minimization problem. Also, this time we\n",
        "        will use gradient information to significantly speed up the search.\n",
        "        \n",
        "        >>> def func2d(x):\n",
        "        ...     f = cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
        "        ...                                                         0.2) * x[0]\n",
        "        ...     df = np.zeros(2)\n",
        "        ...     df[0] = -14.5 * sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
        "        ...     df[1] = 2. * x[1] + 0.2\n",
        "        ...     return f, df\n",
        "        \n",
        "        We'll also use a different local minimization algorithm.  Also we must tell\n",
        "        the minimizer that our function returns both energy and gradient (jacobian)\n",
        "        \n",
        "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
        "        >>> x0 = [1.0, 1.0]\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Here is an example using a custom step taking routine.  Imagine you want\n",
        "        the first coordinate to take larger steps then the rest of the coordinates.\n",
        "        This can be implemented like so:\n",
        "        \n",
        "        >>> class MyTakeStep(object):\n",
        "        ...    def __init__(self, stepsize=0.5):\n",
        "        ...        self.stepsize = stepsize\n",
        "        ...    def __call__(self, x):\n",
        "        ...        s = self.stepsize\n",
        "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
        "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
        "        ...        return x\n",
        "        \n",
        "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
        "        of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
        "        before\n",
        "        \n",
        "        >>> mytakestep = MyTakeStep()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=200, take_step=mytakestep)\n",
        "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
        "        ...                                                           ret.x[1],\n",
        "        ...                                                           ret.fun))\n",
        "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
        "        \n",
        "        \n",
        "        Now let's do an example using a custom callback function which prints the\n",
        "        value of every minimum found\n",
        "        \n",
        "        >>> def print_fun(x, f, accepted):\n",
        "        ...         print(\"at minima %.4f accepted %d\" % (f, int(accepted)))\n",
        "        \n",
        "        We'll run it for only 10 basinhopping steps this time.\n",
        "        \n",
        "        >>> np.random.seed(1)\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, callback=print_fun)\n",
        "        at minima 0.4159 accepted 1\n",
        "        at minima -0.9073 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 0.9102 accepted 1\n",
        "        at minima 2.2945 accepted 0\n",
        "        at minima -0.1021 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        at minima -1.0109 accepted 1\n",
        "        \n",
        "        \n",
        "        The minima at -1.0109 is actually the global minimum, found already on the\n",
        "        8th iteration.\n",
        "        \n",
        "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
        "        \n",
        "        >>> class MyBounds(object):\n",
        "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
        "        ...         self.xmax = np.array(xmax)\n",
        "        ...         self.xmin = np.array(xmin)\n",
        "        ...     def __call__(self, **kwargs):\n",
        "        ...         x = kwargs[\"x_new\"]\n",
        "        ...         tmax = bool(np.all(x <= self.xmax))\n",
        "        ...         tmin = bool(np.all(x >= self.xmin))\n",
        "        ...         return tmax and tmin\n",
        "        \n",
        "        >>> mybounds = MyBounds()\n",
        "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
        "        ...                    niter=10, accept_test=mybounds)\n",
        "    \n",
        "    bisect(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of a function within an interval.\n",
        "        \n",
        "        Basic bisection routine to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. `f(a)` and `f(b)` can not have the same signs.\n",
        "        Slow but sure.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  `f` must be continuous, and\n",
        "            f(a) and f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within `xtol` of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in `maxiter` iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
        "            a `RootResults` object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        fsolve : n-dimensional root-finding\n",
        "    \n",
        "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
        "        Bracket the minimum of the function.\n",
        "        \n",
        "        Given a function and distinct initial points, search in the\n",
        "        downhill direction (as defined by the initital points) and return\n",
        "        new points xa, xb, xc that bracket the minimum of the function\n",
        "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
        "        solution will satisfy xa<=x<=xb\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to minimize.\n",
        "        xa, xb : float, optional\n",
        "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
        "        args : tuple, optional\n",
        "            Additional arguments (if present), passed to `func`.\n",
        "        grow_limit : float, optional\n",
        "            Maximum grow limit.  Defaults to 110.0\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Defaults to 1000.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xa, xb, xc : float\n",
        "            Bracket.\n",
        "        fa, fb, fc : float\n",
        "            Objective function values in bracket.\n",
        "        funcalls : int\n",
        "            Number of function evaluations made.\n",
        "    \n",
        "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
        "        Given a function of one-variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        args\n",
        "            Additional arguments (if present).\n",
        "        brack : tuple\n",
        "            Triple (a,b,c) where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,c)\n",
        "            then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that the obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            Stop if between iteration change is less than `tol`.\n",
        "        full_output : bool\n",
        "            If True, return all output args (xmin, fval, iter,\n",
        "            funcalls).\n",
        "        maxiter : int\n",
        "            Maximum number of iterations in solution.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xmin : ndarray\n",
        "            Optimum point.\n",
        "        fval : float\n",
        "            Optimum value.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of objective function evaluations made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Brent' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses inverse parabolic interpolation when possible to speed up\n",
        "        convergence of golden section method.\n",
        "    \n",
        "    brenth(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find root of f in [a,b].\n",
        "        \n",
        "        A variation on the classic Brent routine to find a zero of the function f\n",
        "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
        "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
        "        f(a) and f(b) can not have the same signs. Generally on a par with the\n",
        "        brent routine, but not as heavily tested.  It is a safe version of the\n",
        "        secant method that uses hyperbolic extrapolation. The version here is by\n",
        "        Chuck Harris.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        fmin, fmin_powell, fmin_cg,\n",
        "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
        "        \n",
        "        leastsq : nonlinear least squares minimizer\n",
        "        \n",
        "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
        "        \n",
        "        anneal, brute : global optimizers\n",
        "        \n",
        "        fminbound, brent, golden, bracket : local scalar minimizers\n",
        "        \n",
        "        fsolve : n-dimensional root-finding\n",
        "        \n",
        "        brentq, brenth, ridder, bisect, newton : one-dimensional root-finding\n",
        "        \n",
        "        fixed_point : scalar fixed-point finder\n",
        "    \n",
        "    brentq(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in given interval.\n",
        "        \n",
        "        Return float, a zero of `f` between `a` and `b`.  `f` must be a continuous\n",
        "        function, and [a,b] must be a sign changing interval.\n",
        "        \n",
        "        Description:\n",
        "        Uses the classic Brent (1973) method to find a zero of the function `f` on\n",
        "        the sign changing interval [a , b].  Generally considered the best of the\n",
        "        rootfinding routines here.  It is a safe version of the secant method that\n",
        "        uses inverse quadratic extrapolation.  Brent's method combines root\n",
        "        bracketing, interval bisection, and inverse quadratic interpolation.  It is\n",
        "        sometimes known as the van Wijngaarden-Deker-Brent method.  Brent (1973)\n",
        "        claims convergence is guaranteed for functions computable within [a,b].\n",
        "        \n",
        "        [Brent1973]_ provides the classic description of the algorithm.  Another\n",
        "        description can be found in a recent edition of Numerical Recipes, including\n",
        "        [PressEtal1992]_.  Another description is at\n",
        "        http://mathworld.wolfram.com/BrentsMethod.html.  It should be easy to\n",
        "        understand the algorithm just by reading our code.  Our code diverges a bit\n",
        "        from standard presentations: we choose a different formula for the\n",
        "        extrapolation step.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.  In particular,\n",
        "            ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        multivariate local optimizers\n",
        "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
        "        nonlinear least squares minimizer\n",
        "          `leastsq`\n",
        "        constrained multivariate optimizers\n",
        "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
        "        global optimizers\n",
        "          `anneal`, `basinhopping`, `brute`\n",
        "        local scalar minimizers\n",
        "          `fminbound`, `brent`, `golden`, `bracket`\n",
        "        n-dimensional root-finding\n",
        "          `fsolve`\n",
        "        one-dimensional root-finding\n",
        "          `brentq`, `brenth`, `ridder`, `bisect`, `newton`\n",
        "        scalar fixed-point finder\n",
        "          `fixed_point`\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Brent1973]\n",
        "           Brent, R. P.,\n",
        "           *Algorithms for Minimization Without Derivatives*.\n",
        "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
        "        \n",
        "        .. [PressEtal1992]\n",
        "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
        "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
        "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
        "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
        "    \n",
        "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \\\"Broyden's good method\\\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
        "        \n",
        "        which corresponds to Broyden's first Jacobian update\n",
        "        \n",
        "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \\\"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
        "        \n",
        "        This method is also known as \"Broyden's bad method\".\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
        "        reduction_method : str or tuple, optional\n",
        "            Method used in ensuring that the rank of the Broyden matrix\n",
        "            stays low. Can either be a string giving the name of the method,\n",
        "            or a tuple of the form ``(method, param1, param2, ...)``\n",
        "            that gives the name of the method and values for additional parameters.\n",
        "        \n",
        "            Methods available:\n",
        "        \n",
        "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
        "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
        "                - ``svd``: keep only the most significant SVD components.\n",
        "                  Takes an extra parameter, ``to_retain`, which determines the\n",
        "                  number of SVD components to retain when rank reduction is done.\n",
        "                  Default is ``max_rank - 2``.\n",
        "        \n",
        "        max_rank : int, optional\n",
        "            Maximum rank for the Broyden matrix.\n",
        "            Default is infinity (ie., no rank reduction).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
        "        \n",
        "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
        "        \n",
        "        corresponding to Broyden's second method.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [vR] B.A. van der Rotten, PhD thesis,\n",
        "           \"A limited memory Broyden method to solve high-dimensional\n",
        "           systems of nonlinear equations\". Mathematisch Instituut,\n",
        "           Universiteit Leiden, The Netherlands (2003).\n",
        "        \n",
        "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
        "    \n",
        "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin>, disp=False)\n",
        "        Minimize a function over a given range by brute force.\n",
        "        \n",
        "        Uses the \"brute force\" method, i.e. computes the function's value\n",
        "        at each point of a multidimensional grid of points, to find the global\n",
        "        minimum of the function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            The objective function to be minimized. Must be in the\n",
        "            form ``f(x, *args)``, where ``x`` is the argument in\n",
        "            the form of a 1-D array and ``args`` is a tuple of any\n",
        "            additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        ranges : tuple\n",
        "            Each component of the `ranges` tuple must be either a\n",
        "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
        "            The program uses these to create the grid of points on which\n",
        "            the objective function will be computed. See `Note 2` for\n",
        "            more detail.\n",
        "        args : tuple, optional\n",
        "            Any additional fixed parameters needed to completely specify\n",
        "            the function.\n",
        "        Ns : int, optional\n",
        "            Number of grid points along the axes, if not otherwise\n",
        "            specified. See `Note2`.\n",
        "        full_output : bool, optional\n",
        "            If True, return the evaluation grid and the objective function's\n",
        "            values on it.\n",
        "        finish : callable, optional\n",
        "            An optimization function that is called with the result of brute force\n",
        "            minimization as initial guess.  `finish` should take the initial guess\n",
        "            as positional argument, and take `args`, `full_output` and `disp`\n",
        "            as keyword arguments.  Use None if no \"polishing\" function is to be\n",
        "            used.  See Notes for more details.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : ndarray\n",
        "            A 1-D array containing the coordinates of a point at which the\n",
        "            objective function had its minimum value. (See `Note 1` for\n",
        "            which point is returned.)\n",
        "        fval : float\n",
        "            Function value at the point `x0`.\n",
        "        grid : tuple\n",
        "            Representation of the evaluation grid.  It has the same\n",
        "            length as `x0`. (Returned when `full_output` is True.)\n",
        "        Jout : ndarray\n",
        "            Function values at each point of the evaluation\n",
        "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
        "            when `full_output` is True.)\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        anneal : Another approach to seeking the global minimum of\n",
        "        multivariate, multimodal functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
        "        of the objective function occurs.  If `finish` is None, that is the\n",
        "        point returned.  When the global minimum occurs within (or not very far\n",
        "        outside) the grid's boundaries, and the grid is fine enough, that\n",
        "        point will be in the neighborhood of the gobal minimum.\n",
        "        \n",
        "        However, users often employ some other optimization program to\n",
        "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
        "        (local) minimum near `brute's` best gridpoint.\n",
        "        The `brute` function's `finish` option provides a convenient way to do\n",
        "        that.  Any polishing program used must take `brute's` output as its\n",
        "        initial guess as a positional argument, and take `brute's` input values\n",
        "        for `args` and `full_output` as keyword arguments, otherwise an error\n",
        "        will be raised.\n",
        "        \n",
        "        `brute` assumes that the `finish` function returns a tuple in the form:\n",
        "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing value\n",
        "        of the argument, ``Jmin`` is the minimum value of the objective function,\n",
        "        \"...\" may be some other returned values (which are not used by `brute`),\n",
        "        and ``statuscode`` is the status code of the `finish` program.\n",
        "        \n",
        "        Note that when `finish` is not None, the values returned are those\n",
        "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
        "        while `brute` confines its search to the input grid points,\n",
        "        the `finish` program's results usually will not coincide with any\n",
        "        gridpoint, and may fall outside the grid's boundary.\n",
        "        \n",
        "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
        "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
        "        Each component of the `ranges` tuple can be either a slice object or a\n",
        "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
        "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
        "        range, `brute` internally converts it to a slice object that interpolates\n",
        "        `Ns` points from its low-value to its high-value, inclusive.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        We illustrate the use of `brute` to seek the global minimum of a function\n",
        "        of two variables that is given as the sum of a positive-definite\n",
        "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
        "        the objective function `f` as the sum of three other functions,\n",
        "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
        "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
        "        are as defined below.\n",
        "        \n",
        "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
        "        >>> def f1(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
        "        \n",
        "        >>> def f2(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
        "        \n",
        "        >>> def f3(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
        "        \n",
        "        >>> def f(z, *params):\n",
        "        ...     x, y = z\n",
        "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
        "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
        "        \n",
        "        Thus, the objective function may have local minima near the minimum\n",
        "        of each of the three functions of which it is composed.  To\n",
        "        use `fmin` to polish its gridpoint result, we may then continue as\n",
        "        follows:\n",
        "        \n",
        "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
        "        >>> from scipy import optimize\n",
        "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
        "                                      finish=optimize.fmin)\n",
        "        >>> resbrute[0]  # global minimum\n",
        "        array([-1.05665192,  1.80834843])\n",
        "        >>> resbrute[1]  # function value at global minimum\n",
        "        -3.4085818767\n",
        "        \n",
        "        Note that if `finish` had been set to None, we would have gotten the\n",
        "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
        "    \n",
        "    check_grad(func, grad, x0, *args)\n",
        "        Check the correctness of a gradient function by comparing it against a\n",
        "        (forward) finite-difference approximation of the gradient.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x0,*args)\n",
        "            Function whose derivative is to be checked.\n",
        "        grad : callable grad(x0, *args)\n",
        "            Gradient of `func`.\n",
        "        x0 : ndarray\n",
        "            Points to check `grad` against forward difference approximation of grad\n",
        "            using `func`.\n",
        "        args : \\*args, optional\n",
        "            Extra arguments passed to `func` and `grad`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        err : float\n",
        "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
        "            difference between ``grad(x0, *args)`` and the finite difference\n",
        "            approximation of `grad` using func at the points `x0`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        approx_fprime\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The step size used for the finite difference approximation is\n",
        "        `sqrt(numpy.finfo(float).eps)`, which is approximately 1.49e-08.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> def func(x): return x[0]**2 - 0.5 * x[1]**3\n",
        "        >>> def grad(x): return [2 * x[0], -1.5 * x[1]**2]\n",
        "        >>> check_grad(func, grad, [1.5, -1.5])\n",
        "        2.9802322387695312e-08\n",
        "    \n",
        "    curve_fit(f, xdata, ydata, p0=None, sigma=None, **kw)\n",
        "        Use non-linear least squares to fit a function, f, to data.\n",
        "        \n",
        "        Assumes ``ydata = f(xdata, *params) + eps``\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable\n",
        "            The model function, f(x, ...).  It must take the independent\n",
        "            variable as the first argument and the parameters to fit as\n",
        "            separate remaining arguments.\n",
        "        xdata : An N-length sequence or an (k,N)-shaped array\n",
        "            for functions with k predictors.\n",
        "            The independent variable where the data is measured.\n",
        "        ydata : N-length sequence\n",
        "            The dependent data --- nominally f(xdata, ...)\n",
        "        p0 : None, scalar, or M-length sequence\n",
        "            Initial guess for the parameters.  If None, then the initial\n",
        "            values will all be 1 (if the number of parameters for the function\n",
        "            can be determined using introspection, otherwise a ValueError\n",
        "            is raised).\n",
        "        sigma : None or N-length sequence\n",
        "            If not None, this vector will be used as relative weights in the\n",
        "            least-squares problem.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        popt : array\n",
        "            Optimal values for the parameters so that the sum of the squared error\n",
        "            of ``f(xdata, *popt) - ydata`` is minimized\n",
        "        pcov : 2d array\n",
        "            The estimated covariance of popt.  The diagonals provide the variance\n",
        "            of the parameter estimate.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        leastsq\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The algorithm uses the Levenberg-Marquardt algorithm through `leastsq`.\n",
        "        Additional keyword arguments are passed directly to that algorithm.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> import numpy as np\n",
        "        >>> from scipy.optimize import curve_fit\n",
        "        >>> def func(x, a, b, c):\n",
        "        ...     return a*np.exp(-b*x) + c\n",
        "        \n",
        "        >>> x = np.linspace(0,4,50)\n",
        "        >>> y = func(x, 2.5, 1.3, 0.5)\n",
        "        >>> yn = y + 0.2*np.random.normal(size=len(x))\n",
        "        \n",
        "        >>> popt, pcov = curve_fit(func, x, yn)\n",
        "    \n",
        "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
        "        \n",
        "        The Jacobian approximation is derived from previous iterations, by\n",
        "        retaining only the diagonal of Broyden matrices.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial guess for the Jacobian is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
        "        \n",
        "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            Initial Jacobian approximation is (-1/alpha).\n",
        "        alphamax : float, optional\n",
        "            The entries of the diagonal Jacobian are kept in the range\n",
        "            ``[alpha, alphamax]``.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500)\n",
        "        Find a fixed point of the function.\n",
        "        \n",
        "        Given a function of one or more variables and a starting point, find a\n",
        "        fixed-point of the function: i.e. where ``func(x0) == x0``.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            Function to evaluate.\n",
        "        x0 : array_like\n",
        "            Fixed point of function.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to `func`.\n",
        "        xtol : float, optional\n",
        "            Convergence tolerance, defaults to 1e-08.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations, defaults to 500.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses Steffensen's Method using Aitken's ``Del^2`` convergence acceleration.\n",
        "        See Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> from scipy import optimize\n",
        "        >>> def func(x, c1, c2):\n",
        "        ....    return np.sqrt(c1/(x+c2))\n",
        "        >>> c1 = np.array([10,12.])\n",
        "        >>> c2 = np.array([3, 5.])\n",
        "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
        "        array([ 1.4920333 ,  1.37228132])\n",
        "    \n",
        "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the downhill simplex algorithm.\n",
        "        \n",
        "        This algorithm only uses function values, not derivatives or second\n",
        "        derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            The objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        xtol : float, optional\n",
        "            Relative error in xopt acceptable for convergence.\n",
        "        ftol : number, optional\n",
        "            Relative error in func(xopt) acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : number, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            Set to True if fopt and warnflag outputs are desired.\n",
        "        disp : bool, optional\n",
        "            Set to True to print convergence messages.\n",
        "        retall : bool, optional\n",
        "            Set to True to return list of solutions at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter that minimizes function.\n",
        "        fopt : float\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        iter : int\n",
        "            Number of iterations performed.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            1 : Maximum number of function evaluations made.\n",
        "            2 : Maximum number of iterations reached.\n",
        "        allvecs : list\n",
        "            Solution at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Nelder-Mead' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "        one or more variables.\n",
        "        \n",
        "        This algorithm has a long history of successful use in applications.\n",
        "        But it will usually be slower than an algorithm that uses first or\n",
        "        second derivative information. In practice it can have poor\n",
        "        performance in high-dimensional problems and is not robust to\n",
        "        minimizing complicated functions. Additionally, there currently is no\n",
        "        complete theory describing when the algorithm will successfully\n",
        "        converge to the minimum, or how fast it will if it does.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "               minimization\", The Computer Journal, 7, pp. 308-313\n",
        "        \n",
        "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "               Harlow, UK, pp. 191-208.\n",
        "    \n",
        "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using the BFGS algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable f'(x,*args), optional\n",
        "            Gradient of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f and fprime.\n",
        "        gtol : float, optional\n",
        "            Gradient norm must be less than gtol before succesful termination.\n",
        "        norm : float, optional\n",
        "            Order of norm (Inf is max, -Inf is min)\n",
        "        epsilon : int or ndarray, optional\n",
        "            If fprime is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function to call after each\n",
        "            iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
        "            in addition to xopt.\n",
        "        disp : bool, optional\n",
        "            Print convergence message if True.\n",
        "        retall : bool, optional\n",
        "            Return a list of results at each iteration if True.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
        "        fopt : float\n",
        "            Minimum value.\n",
        "        gopt : ndarray\n",
        "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
        "        Bopt : ndarray\n",
        "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
        "        func_calls : int\n",
        "            Number of function_calls made.\n",
        "        grad_calls : int\n",
        "            Number of gradient calls made.\n",
        "        warnflag : integer\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "            2 : Gradient and/or function calls not changing.\n",
        "        allvecs  :  list\n",
        "            Results at each iteration.  Only returned if retall is True.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'BFGS' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Optimize the function, f, whose gradient is given by fprime\n",
        "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
        "        and Shanno (BFGS)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
        "    \n",
        "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable, ``f(x, *args)``\n",
        "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
        "            the variables that are to be changed in the search for a minimum, and\n",
        "            `args` are the other (fixed) parameters of `f`.\n",
        "        x0 : ndarray\n",
        "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
        "            It must be a 1-D array of values.\n",
        "        fprime : callable, ``fprime(x, *args)``, optional\n",
        "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
        "            are as described above for `f`. The returned value must be a 1-D array.\n",
        "            Defaults to None, in which case the gradient is approximated\n",
        "            numerically (see `epsilon`, below).\n",
        "        args : tuple, optional\n",
        "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
        "            additional fixed parameters are needed to completely specify the\n",
        "            functions `f` and `fprime`.\n",
        "        gtol : float, optional\n",
        "            Stop when the norm of the gradient is less than `gtol`.\n",
        "        norm : float, optional\n",
        "            Order to use for the norm of the gradient\n",
        "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
        "        epsilon : float or ndarray, optional\n",
        "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
        "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
        "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
        "            1.5e-8.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
        "        full_output : bool, optional\n",
        "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
        "            addition to `xopt`.  See the Returns section below for additional\n",
        "            information on optional return values.\n",
        "        disp : bool, optional\n",
        "            If True, return a convergence message, followed by `xopt`.\n",
        "        retall : bool, optional\n",
        "            If True, add to the returned values the results of each iteration.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each iteration.\n",
        "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float, optional\n",
        "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
        "        func_calls : int, optional\n",
        "            The number of function_calls made.  Only returned if `full_output`\n",
        "            is True.\n",
        "        grad_calls : int, optional\n",
        "            The number of gradient calls made. Only returned if `full_output` is\n",
        "            True.\n",
        "        warnflag : int, optional\n",
        "            Integer value with warning status, only returned if `full_output` is\n",
        "            True.\n",
        "        \n",
        "            0 : Success.\n",
        "        \n",
        "            1 : The maximum number of iterations was exceeded.\n",
        "        \n",
        "            2 : Gradient and/or function calls were not changing.  May indicate\n",
        "                that precision was lost, i.e., the routine did not converge.\n",
        "        \n",
        "        allvecs : list of ndarray, optional\n",
        "            List of arrays, containing the results at each iteration.\n",
        "            Only returned if `retall` is True.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        minimize : common interface to all `scipy.optimize` algorithms for\n",
        "                   unconstrained and constrained minimization of multivariate\n",
        "                   functions.  It provides an alternative way to call\n",
        "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
        "        [1]_.\n",
        "        \n",
        "        Conjugate gradient methods tend to work better when:\n",
        "        \n",
        "        1. `f` has a unique global minimizing point, and no local minima or\n",
        "           other stationary points,\n",
        "        2. `f` is, at least locally, reasonably well approximated by a\n",
        "           quadratic function of the variables,\n",
        "        3. `f` is continuous and has a continuous gradient,\n",
        "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
        "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
        "           minimizing point, `xopt`.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Example 1: seek the minimum value of the expression\n",
        "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
        "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
        "        \n",
        "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
        "        >>> def f(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
        "        >>> def gradf(x, *args):\n",
        "        ...     u, v = x\n",
        "        ...     a, b, c, d, e, f = args\n",
        "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
        "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
        "        ...     return np.asarray((gu, gv))\n",
        "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
        "        >>> from scipy import optimize\n",
        "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
        "        >>> print 'res1 = ', res1\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 1.617021\n",
        "                 Iterations: 2\n",
        "                 Function evaluations: 5\n",
        "                 Gradient evaluations: 5\n",
        "        res1 =  [-1.80851064 -0.25531915]\n",
        "        \n",
        "        Example 2: solve the same problem using the `minimize` function.\n",
        "        (This `myopts` dictionary shows all of the available options,\n",
        "        although in practice only non-default values would be needed.\n",
        "        The returned value will be a dictionary.)\n",
        "        \n",
        "        >>> opts = {'maxiter' : None,    # default value.\n",
        "        ...         'disp' : True,    # non-default value.\n",
        "        ...         'gtol' : 1e-5,    # default value.\n",
        "        ...         'norm' : np.inf,  # default value.\n",
        "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
        "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
        "        ...                          method='CG', options=opts)\n",
        "        Optimization terminated successfully.\n",
        "                Current function value: 1.617021\n",
        "                Iterations: 2\n",
        "                Function evaluations: 5\n",
        "                Gradient evaluations: 5\n",
        "        >>> res2.x  # minimum found\n",
        "        array([-1.80851064 -0.25531915])\n",
        "    \n",
        "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, iprint=1, maxfun=1000, disp=None)\n",
        "        Minimize a function using the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
        "        implentation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            Function to minimize. In the form func(x, \\*args).\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        cons : sequence\n",
        "            Constraint functions; must all be ``>=0`` (a single function\n",
        "            if only 1 constraint). Each function takes the parameters `x`\n",
        "            as its first argument.\n",
        "        args : tuple\n",
        "            Extra arguments to pass to function.\n",
        "        consargs : tuple\n",
        "            Extra arguments to pass to constraint functions (default of None means\n",
        "            use same extra arguments as those passed to func).\n",
        "            Use ``()`` for no extra arguments.\n",
        "        rhobeg :\n",
        "            Reasonable initial changes to the variables.\n",
        "        rhoend :\n",
        "            Final accuracy in the optimization (not precisely guaranteed). This\n",
        "            is a lower bound on the size of the trust region.\n",
        "        iprint : {0, 1, 2, 3}\n",
        "            Controls the frequency of output; 0 implies no output.  Deprecated.\n",
        "        disp : {0, 1, 2, 3}\n",
        "            Over-rides the iprint interface.  Preferred.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The argument that minimises `f`.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'COBYLA' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This algorithm is based on linear approximations to the objective\n",
        "        function and each constraint. We briefly describe the algorithm.\n",
        "        \n",
        "        Suppose the function is being minimized over k variables. At the\n",
        "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
        "        an approximate solution x_j, and a radius RHO_j.\n",
        "        (i.e. linear plus a constant) approximations to the objective\n",
        "        function and constraint functions such that their function values\n",
        "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
        "        This gives a linear program to solve (where the linear approximations\n",
        "        of the constraint functions are constrained to be non-negative).\n",
        "        \n",
        "        However the linear approximations are likely only good\n",
        "        approximations near the current simplex, so the linear program is\n",
        "        given the further requirement that the solution, which\n",
        "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
        "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
        "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
        "        like a trust region algorithm.\n",
        "        \n",
        "        Additionally, the linear program may be inconsistent, or the\n",
        "        approximation may give poor improvement. For details about\n",
        "        how these issues are resolved, as well as how the points v_i are\n",
        "        updated, refer to the source code or the references below.\n",
        "        \n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
        "        the objective and constraint functions by linear interpolation.\", in\n",
        "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
        "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
        "        \n",
        "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
        "        calculations\", Acta Numerica 7, 287-336\n",
        "        \n",
        "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
        "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
        "        \n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Minimize the objective function f(x,y) = x*y subject\n",
        "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
        "        \n",
        "            >>> def objective(x):\n",
        "            ...     return x[0]*x[1]\n",
        "            ...\n",
        "            >>> def constr1(x):\n",
        "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
        "            ...\n",
        "            >>> def constr2(x):\n",
        "            ...     return x[1]\n",
        "            ...\n",
        "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
        "        \n",
        "               Normal return from subroutine COBYLA\n",
        "        \n",
        "               NFVALS =   64   F =-5.000000E-01    MAXCV = 1.998401E-14\n",
        "               X =-7.071069E-01   7.071067E-01\n",
        "            array([-0.70710685,  0.70710671])\n",
        "        \n",
        "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
        "    \n",
        "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None)\n",
        "        Minimize a function func using the L-BFGS-B algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Function to minimise.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable fprime(x,*args)\n",
        "            The gradient of `func`.  If None, then `func` returns the function\n",
        "            value and the gradient (``f, g = func(x, *args)``), unless\n",
        "            `approx_grad` is True in which case `func` returns only ``f``.\n",
        "        args : sequence\n",
        "            Arguments to pass to `func` and `fprime`.\n",
        "        approx_grad : bool\n",
        "            Whether to approximate the gradient numerically (in which case\n",
        "            `func` returns only the function value).\n",
        "        bounds : list\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        m : int\n",
        "            The maximum number of variable metric corrections\n",
        "            used to define the limited memory matrix. (The limited memory BFGS\n",
        "            method does not store the full hessian but uses this many terms in an\n",
        "            approximation to it.)\n",
        "        factr : float\n",
        "            The iteration stops when\n",
        "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
        "            where ``eps`` is the machine precision, which is automatically\n",
        "            generated by the code. Typical values for `factr` are: 1e12 for\n",
        "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
        "            high accuracy.\n",
        "        pgtol : float\n",
        "            The iteration will stop when\n",
        "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
        "            where ``pg_i`` is the i-th component of the projected gradient.\n",
        "        epsilon : float\n",
        "            Step size used when `approx_grad` is True, for numerically\n",
        "            calculating the gradient\n",
        "        iprint : int\n",
        "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
        "            ``iprint == 0`` means write messages to stdout; ``iprint > 1`` in\n",
        "            addition means write logging information to a file named\n",
        "            ``iterate.dat`` in the current working directory.\n",
        "        disp : int, optional\n",
        "            If zero, then no output.  If a positive number, then this over-rides\n",
        "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluations.\n",
        "        maxiter : int\n",
        "            Maximum number of iterations.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : array_like\n",
        "            Estimated position of the minimum.\n",
        "        f : float\n",
        "            Value of `func` at the minimum.\n",
        "        d : dict\n",
        "            Information dictionary.\n",
        "        \n",
        "            * d['warnflag'] is\n",
        "        \n",
        "              - 0 if converged,\n",
        "              - 1 if too many function evaluations or too many iterations,\n",
        "              - 2 if stopped for another reason, given in d['task']\n",
        "        \n",
        "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
        "            * d['funcalls'] is the number of function calls made.\n",
        "            * d['nit'] is the number of iterations.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'L-BFGS-B' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        License of L-BFGS-B (FORTRAN code):\n",
        "        \n",
        "        The version included here (in fortran code) is 3.0\n",
        "        (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd,\n",
        "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
        "        condition for use:\n",
        "        \n",
        "        This software is freely available, but we expect that all publications\n",
        "        describing work using this software, or all commercial products using it,\n",
        "        quote at least one of the references given below. This software is released\n",
        "        under the BSD License.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
        "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
        "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
        "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
        "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
        "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
        "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
        "          ACM Transactions on Mathematical Software, 38, 1.\n",
        "    \n",
        "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "        Unconstrained minimization of a function using the Newton-CG method.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable ``f(x, *args)``\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        fprime : callable ``f'(x, *args)``\n",
        "            Gradient of f.\n",
        "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
        "            Function which computes the Hessian of f times an\n",
        "            arbitrary vector, p.\n",
        "        fhess : callable ``fhess(x, *args)``, optional\n",
        "            Function to compute the Hessian matrix of f.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
        "            (the same set of extra arguments is supplied to all of\n",
        "            these functions).\n",
        "        epsilon : float or ndarray, optional\n",
        "            If fhess is approximated, use this value for the step size.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function which is called after\n",
        "            each iteration.  Called as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        avextol : float, optional\n",
        "            Convergence is assumed when the average relative error in\n",
        "            the minimizer falls below this amount.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        full_output : bool, optional\n",
        "            If True, return the optional outputs.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence message.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of results at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
        "        fopt : float\n",
        "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
        "        fcalls : int\n",
        "            Number of function calls made.\n",
        "        gcalls : int\n",
        "            Number of gradient calls made.\n",
        "        hcalls : int\n",
        "            Number of hessian calls made.\n",
        "        warnflag : int\n",
        "            Warnings generated by the algorithm.\n",
        "            1 : Maximum number of iterations exceeded.\n",
        "        allvecs : list\n",
        "            The result at each iteration, if retall is True (see below).\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'Newton-CG' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
        "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
        "        nor `fhess_p` is provided, then the hessian product will be\n",
        "        approximated using finite differences on `fprime`. `fhess_p`\n",
        "        must compute the hessian times an arbitrary vector. If it is not\n",
        "        given, finite-differences on `fprime` are used to compute\n",
        "        it.\n",
        "        \n",
        "        Newton-CG methods are also called truncated Newton methods. This\n",
        "        function differs from scipy.optimize.fmin_tnc because\n",
        "        \n",
        "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
        "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
        "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
        "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
        "            or box constrained minimization. (Box constraints give\n",
        "            lower and upper bounds for each variable seperately.)\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
        "    \n",
        "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
        "        Minimize a function using modified Powell's method. This method\n",
        "        only uses function values, not derivatives.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to func.\n",
        "        callback : callable, optional\n",
        "            An optional user-supplied function, called after each\n",
        "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        direc : ndarray, optional\n",
        "            Initial direction set.\n",
        "        xtol : float, optional\n",
        "            Line-search error tolerance.\n",
        "        ftol : float, optional\n",
        "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to perform.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations to make.\n",
        "        full_output : bool, optional\n",
        "            If True, fopt, xi, direc, iter, funcalls, and\n",
        "            warnflag are returned.\n",
        "        disp : bool, optional\n",
        "            If True, print convergence messages.\n",
        "        retall : bool, optional\n",
        "            If True, return a list of the solution at each iteration.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameter which minimizes `func`.\n",
        "        fopt : number\n",
        "            Value of function at minimum: ``fopt = func(xopt)``.\n",
        "        direc : ndarray\n",
        "            Current direction set.\n",
        "        iter : int\n",
        "            Number of iterations.\n",
        "        funcalls : int\n",
        "            Number of function calls made.\n",
        "        warnflag : int\n",
        "            Integer warning flag:\n",
        "                1 : Maximum number of function evaluations.\n",
        "                2 : Maximum number of iterations.\n",
        "        allvecs : list\n",
        "            List of solutions at each iteration.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to unconstrained minimization algorithms for\n",
        "            multivariate functions. See the 'Powell' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses a modification of Powell's method to find the minimum of\n",
        "        a function of N variables. Powell's method is a conjugate\n",
        "        direction method.\n",
        "        \n",
        "        The algorithm has two loops. The outer loop\n",
        "        merely iterates over the inner loop. The inner loop minimizes\n",
        "        over each current direction in the direction set. At the end\n",
        "        of the inner loop, if certain conditions are met, the direction\n",
        "        that gave the largest decrease is dropped and replaced with\n",
        "        the difference between the current estiamted x and the estimated\n",
        "        x from the beginning of the inner-loop.\n",
        "        \n",
        "        The technical conditions for replacing the direction of greatest\n",
        "        increase amount to checking that\n",
        "        \n",
        "        1. No further gain can be made along the direction of greatest increase\n",
        "           from that iteration.\n",
        "        2. The direction of greatest increase accounted for a large sufficient\n",
        "           fraction of the decrease in the function value from that iteration of\n",
        "           the inner loop.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
        "        function of several variables without calculating derivatives,\n",
        "        Computer Journal, 7 (2):155-162.\n",
        "        \n",
        "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
        "        Numerical Recipes (any edition), Cambridge University Press\n",
        "    \n",
        "    fmin_slsqp(func, x0, eqcons=[], f_eqcons=None, ieqcons=[], f_ieqcons=None, bounds=[], fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08)\n",
        "        Minimize a function using Sequential Least SQuares Programming\n",
        "        \n",
        "        Python interface function for the SLSQP Optimization subroutine\n",
        "        originally implemented by Dieter Kraft.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function.\n",
        "        x0 : 1-D ndarray of float\n",
        "            Initial guess for the independent variable(s).\n",
        "        eqcons : list\n",
        "            A list of functions of length n such that\n",
        "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_eqcons : callable f(x,*args)\n",
        "            Returns a 1-D array in which each element must equal 0.0 in a\n",
        "            successfully optimized problem.  If f_eqcons is specified,\n",
        "            eqcons is ignored.\n",
        "        ieqcons : list\n",
        "            A list of functions of length n such that\n",
        "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
        "            problem.\n",
        "        f_ieqcons : callable f(x,*args)\n",
        "            Returns a 1-D ndarray in which each element must be greater or\n",
        "            equal to 0.0 in a successfully optimized problem.  If\n",
        "            f_ieqcons is specified, ieqcons is ignored.\n",
        "        bounds : list\n",
        "            A list of tuples specifying the lower and upper bound\n",
        "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
        "        fprime : callable `f(x,*args)`\n",
        "            A function that evaluates the partial derivatives of func.\n",
        "        fprime_eqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of equality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
        "        fprime_ieqcons : callable `f(x,*args)`\n",
        "            A function of the form `f(x, *args)` that returns the m by n\n",
        "            array of inequality constraint normals.  If not provided,\n",
        "            the normals will be approximated. The array returned by\n",
        "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
        "        args : sequence\n",
        "            Additional arguments passed to func and fprime.\n",
        "        iter : int\n",
        "            The maximum number of iterations.\n",
        "        acc : float\n",
        "            Requested accuracy.\n",
        "        iprint : int\n",
        "            The verbosity of fmin_slsqp :\n",
        "        \n",
        "            * iprint <= 0 : Silent operation\n",
        "            * iprint == 1 : Print summary upon completion (default)\n",
        "            * iprint >= 2 : Print status of each iterate and summary\n",
        "        disp : int\n",
        "            Over-rides the iprint interface (preferred).\n",
        "        full_output : bool\n",
        "            If False, return only the minimizer of func (default).\n",
        "            Otherwise, output final objective function and summary\n",
        "            information.\n",
        "        epsilon : float\n",
        "            The step size for finite-difference derivative estimates.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray of float\n",
        "            The final minimizer of func.\n",
        "        fx : ndarray of float, if full_output is true\n",
        "            The final value of the objective function.\n",
        "        its : int, if full_output is true\n",
        "            The number of iterations.\n",
        "        imode : int, if full_output is true\n",
        "            The exit mode from the optimizer (see below).\n",
        "        smode : string, if full_output is true\n",
        "            Message describing the exit mode from the optimizer.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'SLSQP' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Exit modes are defined as follows ::\n",
        "        \n",
        "            -1 : Gradient evaluation required (g & a)\n",
        "             0 : Optimization terminated successfully.\n",
        "             1 : Function evaluation required (f & c)\n",
        "             2 : More equality constraints than independent variables\n",
        "             3 : More than 3*n iterations in LSQ subproblem\n",
        "             4 : Inequality constraints incompatible\n",
        "             5 : Singular matrix E in LSQ subproblem\n",
        "             6 : Singular matrix C in LSQ subproblem\n",
        "             7 : Rank-deficient equality constraint subproblem HFTI\n",
        "             8 : Positive directional derivative for linesearch\n",
        "             9 : Iteration limit exceeded\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
        "    \n",
        "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
        "        Minimize a function with variables subject to bounds, using\n",
        "        gradient information in a truncated Newton algorithm. This\n",
        "        method wraps a C implementation of the algorithm.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``func(x, *args)``\n",
        "            Function to minimize.  Must do one of:\n",
        "        \n",
        "            1. Return f and g, where f is the value of the function and g its\n",
        "               gradient (a list of floats).\n",
        "        \n",
        "            2. Return the function value but supply gradient function\n",
        "               seperately as `fprime`.\n",
        "        \n",
        "            3. Return the function value and set ``approx_grad=True``.\n",
        "        \n",
        "            If the function returns None, the minimization\n",
        "            is aborted.\n",
        "        x0 : array_like\n",
        "            Initial estimate of minimum.\n",
        "        fprime : callable ``fprime(x, *args)``\n",
        "            Gradient of `func`. If None, then either `func` must return the\n",
        "            function value and the gradient (``f,g = func(x, *args)``)\n",
        "            or `approx_grad` must be True.\n",
        "        args : tuple\n",
        "            Arguments to pass to function.\n",
        "        approx_grad : bool\n",
        "            If true, approximate the gradient numerically.\n",
        "        bounds : list\n",
        "            (min, max) pairs for each element in x0, defining the\n",
        "            bounds on that parameter. Use None or +/-inf for one of\n",
        "            min or max when there is no bound in that direction.\n",
        "        epsilon : float\n",
        "            Used if approx_grad is True. The stepsize in a finite\n",
        "            difference approximation for fprime.\n",
        "        scale : array_like\n",
        "            Scaling factors to apply to each variable.  If None, the\n",
        "            factors are up-low for interval bounded variables and\n",
        "            1+|x| for the others.  Defaults to None.\n",
        "        offset : array_like\n",
        "            Value to substract from each variable.  If None, the\n",
        "            offsets are (up+low)/2 for interval bounded variables\n",
        "            and x for the others.\n",
        "        messages :\n",
        "            Bit mask used to select messages display during\n",
        "            minimization values defined in the MSGS dict.  Defaults to\n",
        "            MGS_ALL.\n",
        "        disp : int\n",
        "            Integer interface to messages.  0 = no message, 5 = all messages\n",
        "        maxCGit : int\n",
        "            Maximum number of hessian*vector evaluations per main\n",
        "            iteration.  If maxCGit == 0, the direction chosen is\n",
        "            -gradient if maxCGit < 0, maxCGit is set to\n",
        "            max(1,min(50,n/2)).  Defaults to -1.\n",
        "        maxfun : int\n",
        "            Maximum number of function evaluation.  if None, maxfun is\n",
        "            set to max(100, 10*len(x0)).  Defaults to None.\n",
        "        eta : float\n",
        "            Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "            Defaults to -1.\n",
        "        stepmx : float\n",
        "            Maximum step for the line search.  May be increased during\n",
        "            call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "        accuracy : float\n",
        "            Relative precision for finite difference calculations.  If\n",
        "            <= machine_precision, set to sqrt(machine_precision).\n",
        "            Defaults to 0.\n",
        "        fmin : float\n",
        "            Minimum function value estimate.  Defaults to 0.\n",
        "        ftol : float\n",
        "            Precision goal for the value of f in the stoping criterion.\n",
        "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "        xtol : float\n",
        "            Precision goal for the value of x in the stopping\n",
        "            criterion (after applying x scaling factors).  If xtol <\n",
        "            0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "            -1.\n",
        "        pgtol : float\n",
        "            Precision goal for the value of the projected gradient in\n",
        "            the stopping criterion (after applying x scaling factors).\n",
        "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
        "            Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "        rescale : float\n",
        "            Scaling factor (in log10) used to trigger f value\n",
        "            rescaling.  If 0, rescale at each iteration.  If a large\n",
        "            value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as callback(xk), where xk is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution.\n",
        "        nfeval : int\n",
        "            The number of function evaluations.\n",
        "        rc : int\n",
        "            Return code as defined in the RCSTRINGS dict.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for multivariate\n",
        "            functions. See the 'TNC' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The underlying algorithm is truncated Newton, also called\n",
        "        Newton Conjugate-Gradient. This method differs from\n",
        "        scipy.optimize.fmin_ncg in that\n",
        "        \n",
        "        1. It wraps a C implementation of the algorithm\n",
        "        2. It allows each variable to be given an upper and lower bound.\n",
        "        \n",
        "        The algorithm incoporates the bound constraints by determining\n",
        "        the descent direction as in an unconstrained truncated Newton,\n",
        "        but never taking a step-size large enough to leave the space\n",
        "        of feasible x's. The algorithm keeps track of a set of\n",
        "        currently active constraints, and ignores them when computing\n",
        "        the minimum allowable step size. (The x's associated with the\n",
        "        active constraint are kept fixed.) If the maximum allowable\n",
        "        step size is zero then a new constraint is added. At the end\n",
        "        of each iteration one of the constraints may be deemed no\n",
        "        longer active and removed. A constraint is considered\n",
        "        no longer active is if it is currently active\n",
        "        but the gradient for that variable points inward from the\n",
        "        constraint. The specific constraint removed is the one\n",
        "        associated with the variable of largest index whose\n",
        "        constraint is no longer active.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
        "        \n",
        "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
        "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
        "    \n",
        "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
        "        Bounded minimization for scalar functions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable f(x,*args)\n",
        "            Objective function to be minimized (must accept and return scalars).\n",
        "        x1, x2 : float or array scalar\n",
        "            The optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to function.\n",
        "        xtol : float, optional\n",
        "            The convergence tolerance.\n",
        "        maxfun : int, optional\n",
        "            Maximum number of function evaluations allowed.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        disp : int, optional\n",
        "            If non-zero, print messages.\n",
        "                0 : no message printing.\n",
        "                1 : non-convergence notification messages only.\n",
        "                2 : print a message on convergence too.\n",
        "                3 : print iteration results.\n",
        "        \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        xopt : ndarray\n",
        "            Parameters (over given interval) which minimize the\n",
        "            objective function.\n",
        "        fval : number\n",
        "            The function value at the minimum point.\n",
        "        ierr : int\n",
        "            An error flag (0 if converged, 1 if maximum number of\n",
        "            function calls reached).\n",
        "        numfunc : int\n",
        "          The number of function calls made.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Bounded' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Finds a local minimizer of the scalar function `func` in the\n",
        "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
        "        for auto-bracketing).\n",
        "    \n",
        "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
        "        Find the roots of a function.\n",
        "        \n",
        "        Return the roots of the (non-linear) equations defined by\n",
        "        ``func(x) = 0`` given a starting estimate.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable ``f(x, *args)``\n",
        "            A function that takes at least one (possibly vector) argument.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the roots of ``func(x) = 0``.\n",
        "        args : tuple, optional\n",
        "            Any extra arguments to `func`.\n",
        "        fprime : callable(x), optional\n",
        "            A function to compute the Jacobian of `func` with derivatives\n",
        "            across the rows. By default, the Jacobian will be estimated.\n",
        "        full_output : bool, optional\n",
        "            If True, return optional outputs.\n",
        "        col_deriv : bool, optional\n",
        "            Specify whether the Jacobian function computes derivatives down\n",
        "            the columns (faster, because there is no transpose operation).\n",
        "        xtol : float\n",
        "            The calculation will terminate if the relative error between two\n",
        "            consecutive iterates is at most `xtol`.\n",
        "        maxfev : int, optional\n",
        "            The maximum number of calls to the function. If zero, then\n",
        "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "            in `x0`.\n",
        "        band : tuple, optional\n",
        "            If set to a two-sequence containing the number of sub- and\n",
        "            super-diagonals within the band of the Jacobi matrix, the\n",
        "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "        epsfcn : float, optional\n",
        "            A suitable step length for the forward-difference\n",
        "            approximation of the Jacobian (for ``fprime=None``). If\n",
        "            `epsfcn` is less than the machine precision, it is assumed\n",
        "            that the relative errors in the functions are of the order of\n",
        "            the machine precision.\n",
        "        factor : float, optional\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``).  Should be in the interval\n",
        "            ``(0.1, 100)``.\n",
        "        diag : sequence, optional\n",
        "            N positive entries that serve as a scale factors for the\n",
        "            variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for\n",
        "            an unsuccessful call).\n",
        "        infodict : dict\n",
        "            A dictionary of optional outputs with the keys:\n",
        "        \n",
        "            ``nfev``\n",
        "                number of function calls\n",
        "            ``njev``\n",
        "                number of Jacobian calls\n",
        "            ``fvec``\n",
        "                function evaluated at the output\n",
        "            ``fjac``\n",
        "                the orthogonal matrix, q, produced by the QR\n",
        "                factorization of the final approximate Jacobian\n",
        "                matrix, stored column wise\n",
        "            ``r``\n",
        "                upper triangular matrix produced by QR factorization\n",
        "                of the same matrix\n",
        "            ``qtf``\n",
        "                the vector ``(transpose(q) * fvec)``\n",
        "        \n",
        "        ier : int\n",
        "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
        "            to `mesg` for more information.\n",
        "        mesg : str\n",
        "            If no solution is found, `mesg` details the cause of failure.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        root : Interface to root finding algorithms for multivariate\n",
        "        functions. See the 'hybr' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
        "    \n",
        "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0)\n",
        "        Return the minimum of a function of one variable.\n",
        "        \n",
        "        Given a function of one variable and a possible bracketing interval,\n",
        "        return the minimum of the function isolated to a fractional precision of\n",
        "        tol.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable func(x,*args)\n",
        "            Objective function to minimize.\n",
        "        args : tuple\n",
        "            Additional arguments (if present), passed to func.\n",
        "        brack : tuple\n",
        "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
        "            func(a),func(c).  If bracket consists of two numbers (a,\n",
        "            c), then they are assumed to be a starting interval for a\n",
        "            downhill bracket search (see `bracket`); it doesn't always\n",
        "            mean that obtained solution will satisfy a<=x<=c.\n",
        "        tol : float\n",
        "            x tolerance stop criterion\n",
        "        full_output : bool\n",
        "            If True, return optional outputs.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions. See the 'Golden' `method` in particular.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses analog of bisection method to decrease the bracketed\n",
        "        interval.\n",
        "    \n",
        "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
        "        Minimize the sum of squares of a set of equations.\n",
        "        \n",
        "        ::\n",
        "        \n",
        "            x = arg min(sum(func(y)**2,axis=0))\n",
        "                     y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : callable\n",
        "            should take at least one (possibly length N vector) argument and\n",
        "            returns M floating point numbers.\n",
        "        x0 : ndarray\n",
        "            The starting estimate for the minimization.\n",
        "        args : tuple\n",
        "            Any extra arguments to func are placed in this tuple.\n",
        "        Dfun : callable\n",
        "            A function or method to compute the Jacobian of func with derivatives\n",
        "            across the rows. If this is None, the Jacobian will be estimated.\n",
        "        full_output : bool\n",
        "            non-zero to return all optional outputs.\n",
        "        col_deriv : bool\n",
        "            non-zero to specify that the Jacobian function computes derivatives\n",
        "            down the columns (faster, because there is no transpose operation).\n",
        "        ftol : float\n",
        "            Relative error desired in the sum of squares.\n",
        "        xtol : float\n",
        "            Relative error desired in the approximate solution.\n",
        "        gtol : float\n",
        "            Orthogonality desired between the function vector and the columns of\n",
        "            the Jacobian.\n",
        "        maxfev : int\n",
        "            The maximum number of calls to the function. If zero, then 100*(N+1) is\n",
        "            the maximum where N is the number of elements in x0.\n",
        "        epsfcn : float\n",
        "            A suitable step length for the forward-difference approximation of the\n",
        "            Jacobian (for Dfun=None). If epsfcn is less than the machine precision,\n",
        "            it is assumed that the relative errors in the functions are of the\n",
        "            order of the machine precision.\n",
        "        factor : float\n",
        "            A parameter determining the initial step bound\n",
        "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "        diag : sequence\n",
        "            N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            The solution (or the result of the last iteration for an unsuccessful\n",
        "            call).\n",
        "        cov_x : ndarray\n",
        "            Uses the fjac and ipvt optional outputs to construct an\n",
        "            estimate of the jacobian around the solution. None if a\n",
        "            singular matrix encountered (indicates very flat curvature in\n",
        "            some direction).  This matrix must be multiplied by the\n",
        "            residual variance to get the covariance of the\n",
        "            parameter estimates -- see curve_fit.\n",
        "        infodict : dict\n",
        "            a dictionary of optional outputs with the key s:\n",
        "        \n",
        "            ``nfev``\n",
        "                The number of function calls\n",
        "            ``fvec``\n",
        "                The function evaluated at the output\n",
        "            ``fjac``\n",
        "                A permutation of the R matrix of a QR\n",
        "                factorization of the final approximate\n",
        "                Jacobian matrix, stored column wise.\n",
        "                Together with ipvt, the covariance of the\n",
        "                estimate can be approximated.\n",
        "            ``ipvt``\n",
        "                An integer array of length N which defines\n",
        "                a permutation matrix, p, such that\n",
        "                fjac*p = q*r, where r is upper triangular\n",
        "                with diagonal elements of nonincreasing\n",
        "                magnitude. Column j of p is column ipvt(j)\n",
        "                of the identity matrix.\n",
        "            ``qtf``\n",
        "                The vector (transpose(q) * fvec).\n",
        "        \n",
        "        mesg : str\n",
        "            A string message giving information about the cause of failure.\n",
        "        ier : int\n",
        "            An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\n",
        "            found.  Otherwise, the solution was not found. In either case, the\n",
        "            optional output variable 'mesg' gives more information.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
        "        \n",
        "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
        "        objective function.\n",
        "        This approximation assumes that the objective function is based on the\n",
        "        difference between some observed target data (ydata) and a (non-linear)\n",
        "        function of the parameters `f(xdata, params)` ::\n",
        "        \n",
        "               func(params) = ydata - f(xdata, params)\n",
        "        \n",
        "        so that the objective function is ::\n",
        "        \n",
        "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
        "             params\n",
        "    \n",
        "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)\n",
        "        Find alpha that satisfies strong Wolfe conditions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : callable f(x,*args)\n",
        "            Objective function.\n",
        "        myfprime : callable f'(x,*args)\n",
        "            Objective function gradient.\n",
        "        xk : ndarray\n",
        "            Starting point.\n",
        "        pk : ndarray\n",
        "            Search direction.\n",
        "        gfk : ndarray, optional\n",
        "            Gradient value for x=xk (xk being the current parameter\n",
        "            estimate). Will be recomputed if omitted.\n",
        "        old_fval : float, optional\n",
        "            Function value for x=xk. Will be recomputed if omitted.\n",
        "        old_old_fval : float, optional\n",
        "            Function value for the point preceding x=xk\n",
        "        args : tuple, optional\n",
        "            Additional arguments passed to objective function.\n",
        "        c1 : float, optional\n",
        "            Parameter for Armijo condition rule.\n",
        "        c2 : float, optional\n",
        "            Parameter for curvature condition rule.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        alpha0 : float\n",
        "            Alpha for which ``x_new = x0 + alpha * pk``.\n",
        "        fc : int\n",
        "            Number of function evaluations made.\n",
        "        gc : int\n",
        "            Number of gradient evaluations made.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses the line search algorithm to enforce strong Wolfe\n",
        "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
        "        1999, pg. 59-60.\n",
        "        \n",
        "        For the zoom phase it uses an algorithm by [...].\n",
        "    \n",
        "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using a scalar Jacobian approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "           This algorithm may be useful for specific problems, but whether\n",
        "           it will work may depend strongly on the problem.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        alpha : float, optional\n",
        "            The Jacobian approximation is (-1/alpha).\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "    \n",
        "    minimize(fun, x0, args=(), method='BFGS', jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
        "        Minimization of scalar function of one or more variables.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its\n",
        "            derivatives (Jacobian, Hessian).\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Nelder-Mead'\n",
        "                - 'Powell'\n",
        "                - 'CG'\n",
        "                - 'BFGS'\n",
        "                - 'Newton-CG'\n",
        "                - 'Anneal'\n",
        "                - 'L-BFGS-B'\n",
        "                - 'TNC'\n",
        "                - 'COBYLA'\n",
        "                - 'SLSQP'\n",
        "                - 'dogleg'\n",
        "                - 'trust-ncg'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            Jacobian of objective function. Only for CG, BFGS, Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of the\n",
        "            objective. In this case, it must accept the same arguments as `fun`.\n",
        "        hess, hessp : callable, optional\n",
        "            Hessian of objective function or Hessian of objective function\n",
        "            times an arbitrary vector p.  Only for Newton-CG,\n",
        "            dogleg, trust-ncg.\n",
        "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
        "            provided, then `hessp` will be ignored.  If neither `hess` nor\n",
        "            `hessp` is provided, then the hessian product will be approximated\n",
        "            using finite differences on `jac`. `hessp` must compute the Hessian\n",
        "            times an arbitrary vector.\n",
        "        bounds : sequence, optional\n",
        "            Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
        "            ``(min, max)`` pairs for each element in ``x``, defining\n",
        "            the bounds on that parameter. Use None for one of ``min`` or\n",
        "            ``max`` when there is no bound in that direction.\n",
        "        constraints : dict or sequence of dict, optional\n",
        "            Constraints definition (only for COBYLA and SLSQP).\n",
        "            Each constraint is defined in a dictionary with fields:\n",
        "                type : str\n",
        "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
        "                fun : callable\n",
        "                    The function defining the constraint.\n",
        "                jac : callable, optional\n",
        "                    The Jacobian of `fun` (only for SLSQP).\n",
        "                args : sequence, optional\n",
        "                    Extra arguments to be passed to the function and Jacobian.\n",
        "            Equality constraint means that the constraint function result is to\n",
        "            be zero whereas inequality means that it is to be non-negative.\n",
        "            Note that COBYLA only supports inequality constraints.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. All methods accept the following\n",
        "            generic options:\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "            For method-specific options, see `show_options('minimize', method)`.\n",
        "        callback : callable, optional\n",
        "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
        "            current parameter vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize_scalar: Interface to minimization algorithms for scalar\n",
        "            univariate functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *BFGS*.\n",
        "        \n",
        "        **Unconstrained minimization**\n",
        "        \n",
        "        Method *Nelder-Mead* uses the Simplex algorithm [1]_, [2]_. This\n",
        "        algorithm has been successful in many applications but other algorithms\n",
        "        using the first and/or second derivatives information might be preferred\n",
        "        for their better performances and robustness in general.\n",
        "        \n",
        "        Method *Powell* is a modification of Powell's method [3]_, [4]_ which\n",
        "        is a conjugate direction method. It performs sequential one-dimensional\n",
        "        minimizations along each vector of the directions set (`direc` field in\n",
        "        `options` and `info`), which is updated at each iteration of the main\n",
        "        minimization loop. The function need not be differentiable, and no\n",
        "        derivatives are taken.\n",
        "        \n",
        "        Method *CG* uses a nonlinear conjugate gradient algorithm by Polak and\n",
        "        Ribiere, a variant of the Fletcher-Reeves method described in [5]_ pp.\n",
        "        120-122. Only the first derivatives are used.\n",
        "        \n",
        "        Method *BFGS* uses the quasi-Newton method of Broyden, Fletcher,\n",
        "        Goldfarb, and Shanno (BFGS) [5]_ pp. 136. It uses the first derivatives\n",
        "        only. BFGS has proven good performance even for non-smooth\n",
        "        optimizations. This method also returns an approximation of the Hessian\n",
        "        inverse, stored as `hess_inv` in the Result object.\n",
        "        \n",
        "        Method *Newton-CG* uses a Newton-CG algorithm [5]_ pp. 168 (also known\n",
        "        as the truncated Newton method). It uses a CG method to the compute the\n",
        "        search direction. See also *TNC* method for a box-constrained\n",
        "        minimization with a similar algorithm.\n",
        "        \n",
        "        Method *Anneal* uses simulated annealing, which is a probabilistic\n",
        "        metaheuristic algorithm for global optimization. It uses no derivative\n",
        "        information from the function being optimized.\n",
        "        \n",
        "        Method *dogleg* uses the dog-leg trust-region algorithm [5]_\n",
        "        for unconstrained minimization. This algorithm requires the gradient\n",
        "        and Hessian; furthermore the Hessian is required to be positive definite.\n",
        "        \n",
        "        Method *trust-ncg* uses the Newton conjugate gradient trust-region\n",
        "        algorithm [5]_ for unconstrained minimization. This algorithm requires\n",
        "        the gradient and either the Hessian or a function that computes the\n",
        "        product of the Hessian with a given vector.\n",
        "        \n",
        "        **Constrained minimization**\n",
        "        \n",
        "        Method *L-BFGS-B* uses the L-BFGS-B algorithm [6]_, [7]_ for bound\n",
        "        constrained minimization.\n",
        "        \n",
        "        Method *TNC* uses a truncated Newton algorithm [5]_, [8]_ to minimize a\n",
        "        function with variables subject to bounds. This algorithm uses\n",
        "        gradient information; it is also called Newton Conjugate-Gradient. It\n",
        "        differs from the *Newton-CG* method described above as it wraps a C\n",
        "        implementation and allows each variable to be given upper and lower\n",
        "        bounds.\n",
        "        \n",
        "        Method *COBYLA* uses the Constrained Optimization BY Linear\n",
        "        Approximation (COBYLA) method [9]_, [10]_, [11]_. The algorithm is\n",
        "        based on linear approximations to the objective function and each\n",
        "        constraint. The method wraps a FORTRAN implementation of the algorithm.\n",
        "        \n",
        "        Method *SLSQP* uses Sequential Least SQuares Programming to minimize a\n",
        "        function of several variables with any combination of bounds, equality\n",
        "        and inequality constraints. The method wraps the SLSQP Optimization\n",
        "        subroutine originally implemented by Dieter Kraft [12]_.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
        "            Minimization. The Computer Journal 7: 308-13.\n",
        "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
        "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
        "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
        "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
        "            191-208.\n",
        "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
        "           a function of several variables without calculating derivatives. The\n",
        "           Computer Journal 7: 155-162.\n",
        "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
        "           Numerical Recipes (any edition), Cambridge University Press.\n",
        "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
        "           Springer New York.\n",
        "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
        "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
        "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
        "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
        "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
        "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
        "           550-560.\n",
        "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
        "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
        "        .. [9] Powell, M J D. A direct search optimization method that models\n",
        "           the objective and constraint functions by linear interpolation.\n",
        "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
        "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
        "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
        "           calculations. 1998. Acta Numerica 7: 287-336.\n",
        "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
        "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
        "           2007/NA03\n",
        "        .. [12] Kraft, D. A software package for sequential quadratic\n",
        "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
        "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
        "        function (and its respective derivatives) is implemented in `rosen`\n",
        "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
        "        \n",
        "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
        "        \n",
        "        A simple application of the *Nelder-Mead* method is:\n",
        "        \n",
        "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
        "        >>> res = minimize(rosen, x0, method='Nelder-Mead')\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        \n",
        "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
        "        options:\n",
        "        \n",
        "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
        "        ...                options={'gtol': 1e-6, 'disp': True})\n",
        "        Optimization terminated successfully.\n",
        "                 Current function value: 0.000000\n",
        "                 Iterations: 52\n",
        "                 Function evaluations: 64\n",
        "                 Gradient evaluations: 64\n",
        "        >>> res.x\n",
        "        [ 1.  1.  1.  1.  1.]\n",
        "        >>> print res.message\n",
        "        Optimization terminated successfully.\n",
        "        >>> res.hess\n",
        "        [[ 0.00749589  0.01255155  0.02396251  0.04750988  0.09495377]\n",
        "         [ 0.01255155  0.02510441  0.04794055  0.09502834  0.18996269]\n",
        "         [ 0.02396251  0.04794055  0.09631614  0.19092151  0.38165151]\n",
        "         [ 0.04750988  0.09502834  0.19092151  0.38341252  0.7664427 ]\n",
        "         [ 0.09495377  0.18996269  0.38165151  0.7664427   1.53713523]]\n",
        "        \n",
        "        \n",
        "        Next, consider a minimization problem with several constraints (namely\n",
        "        Example 16.4 from [5]_). The objective function is:\n",
        "        \n",
        "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
        "        \n",
        "        There are three constraints defined as:\n",
        "        \n",
        "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
        "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
        "        \n",
        "        And variables must be positive, hence the following bounds:\n",
        "        \n",
        "        >>> bnds = ((0, None), (0, None))\n",
        "        \n",
        "        The optimization problem is solved using the SLSQP method as:\n",
        "        \n",
        "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
        "        ...                constraints=cons)\n",
        "        \n",
        "        It should converge to the theoretical solution (1.4 ,1.7).\n",
        "    \n",
        "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
        "        Minimization of scalar function of one variable.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            Objective function.\n",
        "            Scalar function, must return a scalar.\n",
        "        bracket : sequence, optional\n",
        "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
        "            interval and can either have three items `(a, b, c)` so that `a < b\n",
        "            < c` and `fun(b) < fun(a), fun(c)` or two items `a` and `c` which\n",
        "            are assumed to be a starting interval for a downhill bracket search\n",
        "            (see `bracket`); it doesn't always mean that the obtained solution\n",
        "            will satisfy `a <= x <= c`.\n",
        "        bounds : sequence, optional\n",
        "            For method 'bounded', `bounds` is mandatory and must have two items\n",
        "            corresponding to the optimization bounds.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'Brent'\n",
        "                - 'Bounded'\n",
        "                - 'Golden'\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options.\n",
        "                xtol : float\n",
        "                    Relative error in solution `xopt` acceptable for\n",
        "                    convergence.\n",
        "                maxiter : int\n",
        "                    Maximum number of iterations to perform.\n",
        "                disp : bool\n",
        "                    Set to True to print convergence messages.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res : Result\n",
        "            The optimization result represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the optimizer exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        See also\n",
        "        --------\n",
        "        minimize: Interface to minimization algorithms for scalar multivariate\n",
        "            functions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *Brent*.\n",
        "        \n",
        "        Method *Brent* uses Brent's algorithm to find a local minimum.\n",
        "        The algorithm uses inverse parabolic interpolation when possible to\n",
        "        speed up convergence of the golden section method.\n",
        "        \n",
        "        Method *Golden* uses the golden section search technique. It uses\n",
        "        analog of the bisection method to decrease the bracketed interval. It\n",
        "        is usually preferable to use the *Brent* method.\n",
        "        \n",
        "        Method *Bounded* can perform bounded minimization. It uses the Brent\n",
        "        method to find a local minimum in the interval x1 < xopt < x2.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        Consider the problem of minimizing the following function.\n",
        "        \n",
        "        >>> def f(x):\n",
        "        ...     return (x - 2) * x * (x + 2)**2\n",
        "        \n",
        "        Using the *Brent* method, we find the local minimum as:\n",
        "        \n",
        "        >>> from scipy.optimize import minimize_scalar\n",
        "        >>> res = minimize_scalar(f)\n",
        "        >>> res.x\n",
        "        1.28077640403\n",
        "        \n",
        "        Using the *Bounded* method, we find a local minimum with specified\n",
        "        bounds as:\n",
        "        \n",
        "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
        "        >>> res.x\n",
        "        -2.0000002026\n",
        "    \n",
        "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)\n",
        "        Find a zero using the Newton-Raphson or secant method.\n",
        "        \n",
        "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
        "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
        "        is provided, otherwise the secant method is used.  If the second order\n",
        "        derivate `fprime2` of `func` is provided, parabolic Halley's method\n",
        "        is used.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        func : function\n",
        "            The function whose zero is wanted. It must be a function of a\n",
        "            single variable of the form f(x,a,b,c...), where a,b,c... are extra\n",
        "            arguments that can be passed in the `args` parameter.\n",
        "        x0 : float\n",
        "            An initial estimate of the zero that should be somewhere near the\n",
        "            actual zero.\n",
        "        fprime : function, optional\n",
        "            The derivative of the function when available and convenient. If it\n",
        "            is None (default), then the secant method is used.\n",
        "        args : tuple, optional\n",
        "            Extra arguments to be used in the function call.\n",
        "        tol : float, optional\n",
        "            The allowable error of the zero value.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations.\n",
        "        fprime2 : function, optional\n",
        "            The second order derivative of the function when available and\n",
        "            convenient. If it is None (default), then the normal Newton-Raphson\n",
        "            or the secant method is used. If it is given, parabolic Halley's\n",
        "            method is used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        zero : float\n",
        "            Estimated location where function is zero.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, ridder, bisect\n",
        "        fsolve : find zeroes in n dimensions.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The convergence rate of the Newton-Raphson method is quadratic,\n",
        "        the Halley method is cubic, and the secant method is\n",
        "        sub-quadratic.  This means that if the function is well behaved\n",
        "        the actual error in the estimated zero is approximately the square\n",
        "        (cube for Halley) of the requested tolerance up to roundoff\n",
        "        error. However, the stopping criterion used here is the step size\n",
        "        and there is no guarantee that a zero has been found. Consequently\n",
        "        the result should be verified. Safer algorithms are brentq,\n",
        "        brenth, ridder, and bisect, but they all require that the root\n",
        "        first be bracketed in an interval where the function changes\n",
        "        sign. The brentq algorithm is recommended for general use in one\n",
        "        dimensional problems when such an interval has been found.\n",
        "    \n",
        "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
        "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
        "        \n",
        "        This method is suitable for solving large-scale problems.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        F : function(x) -> f\n",
        "            Function whose root to find; should take and return an array-like\n",
        "            object.\n",
        "        x0 : array_like\n",
        "            Initial guess for the solution\n",
        "        rdiff : float, optional\n",
        "            Relative step size to use in numerical differentiation.\n",
        "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
        "            Krylov method to use to approximate the Jacobian.\n",
        "            Can be a string, or a function implementing the same interface as\n",
        "            the iterative solvers in `scipy.sparse.linalg`.\n",
        "        \n",
        "            The default is `scipy.sparse.linalg.lgmres`.\n",
        "        inner_M : LinearOperator or InverseJacobian\n",
        "            Preconditioner for the inner Krylov iteration.\n",
        "            Note that you can use also inverse Jacobians as (adaptive)\n",
        "            preconditioners. For example,\n",
        "        \n",
        "            >>> jac = BroydenFirst()\n",
        "            >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "            If the preconditioner has a method named 'update', it will be called\n",
        "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
        "            the current point, and ``f`` the current function value.\n",
        "        inner_tol, inner_maxiter, ...\n",
        "            Parameters to pass on to the \\\"inner\\\" Krylov solver.\n",
        "            See `scipy.sparse.linalg.gmres` for details.\n",
        "        outer_k : int, optional\n",
        "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
        "            See `scipy.sparse.linalg.lgmres` for details.\n",
        "        iter : int, optional\n",
        "            Number of iterations to make. If omitted (default), make as many\n",
        "            as required to meet tolerances.\n",
        "        verbose : bool, optional\n",
        "            Print status to stdout on every iteration.\n",
        "        maxiter : int, optional\n",
        "            Maximum number of iterations to make. If more are needed to\n",
        "            meet convergence, `NoConvergence` is raised.\n",
        "        f_tol : float, optional\n",
        "            Absolute tolerance (in max-norm) for the residual.\n",
        "            If omitted, default is 6e-6.\n",
        "        f_rtol : float, optional\n",
        "            Relative tolerance for the residual. If omitted, not used.\n",
        "        x_tol : float, optional\n",
        "            Absolute minimum step size, as determined from the Jacobian\n",
        "            approximation. If the step size is smaller than this, optimization\n",
        "            is terminated as successful. If omitted, not used.\n",
        "        x_rtol : float, optional\n",
        "            Relative minimum step size. If omitted, not used.\n",
        "        tol_norm : function(vector) -> scalar, optional\n",
        "            Norm to use in convergence check. Default is the maximum norm.\n",
        "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "            Which type of a line search to use to determine the step size in the\n",
        "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : ndarray\n",
        "            An array (of similar array type as `x0`) containing the final solution.\n",
        "        \n",
        "        Raises\n",
        "        ------\n",
        "        NoConvergence\n",
        "            When a solution was not found.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        scipy.sparse.linalg.gmres\n",
        "        scipy.sparse.linalg.lgmres\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This function implements a Newton-Krylov solver. The basic idea is\n",
        "        to compute the inverse of the Jacobian with an iterative Krylov\n",
        "        method. These methods require only evaluating the Jacobian-vector\n",
        "        products, which are conveniently approximated by numerical\n",
        "        differentiation:\n",
        "        \n",
        "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
        "        \n",
        "        Due to the use of iterative matrix inverses, these methods can\n",
        "        deal with large nonlinear problems.\n",
        "        \n",
        "        Scipy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
        "        solvers to choose from. The default here is `lgmres`, which is a\n",
        "        variant of restarted GMRES iteration that reuses some of the\n",
        "        information obtained in the previous Newton steps to invert\n",
        "        Jacobians in subsequent steps.\n",
        "        \n",
        "        For a review on Newton-Krylov methods, see for example [KK]_,\n",
        "        and for the LGMRES sparse inverse method, see [BJM]_.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [KK] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2003).\n",
        "        .. [BJM] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
        "                 SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
        "    \n",
        "    nnls(A, b)\n",
        "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
        "        for a FORTAN non-negative least squares solver.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : ndarray\n",
        "            Matrix ``A`` as shown above.\n",
        "        b : ndarray\n",
        "            Right-hand side vector.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x : ndarray\n",
        "            Solution vector.\n",
        "        rnorm : float\n",
        "            The residual, ``|| Ax-b ||_2``.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        The FORTRAN code was published in the book below. The algorithm\n",
        "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
        "        conditions for the non-negative least squares problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
        "    \n",
        "    ridder(f, a, b, args=(), xtol=1e-12, rtol=4.4408920985006262e-16, maxiter=100, full_output=False, disp=True)\n",
        "        Find a root of a function in an interval.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        f : function\n",
        "            Python function returning a number.  f must be continuous, and f(a) and\n",
        "            f(b) must have opposite signs.\n",
        "        a : number\n",
        "            One end of the bracketing interval [a,b].\n",
        "        b : number\n",
        "            The other end of the bracketing interval [a,b].\n",
        "        xtol : number, optional\n",
        "            The routine converges when a root is known to lie within xtol of the\n",
        "            value return. Should be >= 0.  The routine modifies this to take into\n",
        "            account the relative precision of doubles.\n",
        "        rtol : number, optional\n",
        "            The routine converges when a root is known to lie within `rtol` times\n",
        "            the value returned of the value returned. Should be >= 0. Defaults to\n",
        "            ``np.finfo(float).eps * 2``.\n",
        "        maxiter : number, optional\n",
        "            if convergence is not achieved in maxiter iterations, and error is\n",
        "            raised.  Must be >= 0.\n",
        "        args : tuple, optional\n",
        "            containing extra arguments for the function `f`.\n",
        "            `f` is called by ``apply(f, (x)+args)``.\n",
        "        full_output : bool, optional\n",
        "            If `full_output` is False, the root is returned.  If `full_output` is\n",
        "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
        "            a RootResults object.\n",
        "        disp : bool, optional\n",
        "            If True, raise RuntimeError if the algorithm didn't converge.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x0 : float\n",
        "            Zero of `f` between `a` and `b`.\n",
        "        r : RootResults (present if ``full_output = True``)\n",
        "            Object containing information about the convergence.\n",
        "            In particular, ``r.converged`` is True if the routine converged.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        brentq, brenth, bisect, newton : one-dimensional root-finding\n",
        "        fixed_point : scalar fixed-point finder\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
        "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
        "        generally as fast as the Brent rountines. [Ridders1979]_ provides the\n",
        "        classic description and source of the algorithm. A description can also be\n",
        "        found in any recent edition of Numerical Recipes.\n",
        "        \n",
        "        The routine used here diverges slightly from standard presentations in\n",
        "        order to be a bit more careful of tolerance.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [Ridders1979]\n",
        "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
        "           Single Root of a Real Continuous Function.\"\n",
        "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
        "    \n",
        "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
        "        Find a root of a vector function.\n",
        "        \n",
        "        .. versionadded:: 0.11.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        fun : callable\n",
        "            A vector function to find a root of.\n",
        "        x0 : ndarray\n",
        "            Initial guess.\n",
        "        args : tuple, optional\n",
        "            Extra arguments passed to the objective function and its Jacobian.\n",
        "        method : str, optional\n",
        "            Type of solver.  Should be one of\n",
        "        \n",
        "                - 'hybr'\n",
        "                - 'lm'\n",
        "                - 'broyden1'\n",
        "                - 'broyden2'\n",
        "                - 'anderson'\n",
        "                - 'linearmixing'\n",
        "                - 'diagbroyden'\n",
        "                - 'excitingmixing'\n",
        "                - 'krylov'\n",
        "        \n",
        "        jac : bool or callable, optional\n",
        "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
        "            value of Jacobian along with the objective function. If False, the\n",
        "            Jacobian will be estimated numerically.\n",
        "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
        "            this case, it must accept the same arguments as `fun`.\n",
        "        tol : float, optional\n",
        "            Tolerance for termination. For detailed control, use solver-specific\n",
        "            options.\n",
        "        callback : function, optional\n",
        "            Optional callback function. It is called on every iteration as\n",
        "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
        "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
        "        options : dict, optional\n",
        "            A dictionary of solver options. E.g. `xtol` or `maxiter`, see\n",
        "            ``show_options('root', method)`` for details.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sol : Result\n",
        "            The solution represented as a ``Result`` object.\n",
        "            Important attributes are: ``x`` the solution array, ``success`` a\n",
        "            Boolean flag indicating if the algorithm exited successfully and\n",
        "            ``message`` which describes the cause of the termination. See\n",
        "            `Result` for a description of other attributes.\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        This section describes the available solvers that can be selected by the\n",
        "        'method' parameter. The default method is *hybr*.\n",
        "        \n",
        "        Method *hybr* uses a modification of the Powell hybrid method as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Method *lm* solves the system of nonlinear equations in a least squares\n",
        "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
        "        implemented in MINPACK [1]_.\n",
        "        \n",
        "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
        "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
        "        with backtracking or full line searches [2]_. Each method corresponds\n",
        "        to a particular Jacobian approximations. See `nonlin` for details.\n",
        "        \n",
        "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
        "          known as Broyden's good method.\n",
        "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
        "          is known as Broyden's bad method.\n",
        "        - Method *anderson* uses (extended) Anderson mixing.\n",
        "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
        "          is suitable for large-scale problem.\n",
        "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
        "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
        "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
        "          approximation.\n",
        "        \n",
        "        .. warning::\n",
        "        \n",
        "            The algorithms implemented for methods *diagbroyden*,\n",
        "            *linearmixing* and *excitingmixing* may be useful for specific\n",
        "            problems, but whether they will work may depend strongly on the\n",
        "            problem.\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
        "           1980. User Guide for MINPACK-1.\n",
        "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
        "            Equations. Society for Industrial and Applied Mathematics.\n",
        "            <http://www.siam.org/books/kelley/>\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        The following functions define a system of nonlinear equations and its\n",
        "        jacobian.\n",
        "        \n",
        "        >>> def fun(x):\n",
        "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
        "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
        "        \n",
        "        >>> def jac(x):\n",
        "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
        "        ...                       -1.5 * (x[0] - x[1])**2],\n",
        "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
        "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
        "        \n",
        "        A solution can be obtained as follows.\n",
        "        \n",
        "        >>> from scipy import optimize\n",
        "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
        "        >>> sol.x\n",
        "        array([ 0.8411639,  0.1588361])\n",
        "    \n",
        "    rosen(x)\n",
        "        The Rosenbrock function.\n",
        "        \n",
        "        The function computed is::\n",
        "        \n",
        "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Rosenbrock function is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        f : float\n",
        "            The value of the Rosenbrock function.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen_der, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_der(x)\n",
        "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the derivative is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_der : (N,) ndarray\n",
        "            The gradient of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_hess, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess(x)\n",
        "        The Hessian matrix of the Rosenbrock function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess_prod\n",
        "    \n",
        "    rosen_hess_prod(x, p)\n",
        "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array_like\n",
        "            1-D array of points at which the Hessian matrix is to be computed.\n",
        "        p : array_like\n",
        "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        rosen_hess_prod : ndarray\n",
        "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
        "            by the vector `p`.\n",
        "        \n",
        "        See Also\n",
        "        --------\n",
        "        rosen, rosen_der, rosen_hess\n",
        "    \n",
        "    show_options(solver, method=None)\n",
        "        Show documentation for additional options of optimization solvers.\n",
        "        \n",
        "        These are method-specific options that can be supplied through the\n",
        "        ``options`` dict.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        solver : str\n",
        "            Type of optimization solver. One of {`minimize`, `root`}.\n",
        "        method : str, optional\n",
        "            If not given, shows all methods of the specified solver. Otherwise,\n",
        "            show only the options for the specified method. Valid values\n",
        "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
        "            'minimize').\n",
        "        \n",
        "        Notes\n",
        "        -----\n",
        "        \n",
        "        ** minimize options\n",
        "        \n",
        "        * BFGS options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Nelder-Mead options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "        \n",
        "        * Newton-CG options:\n",
        "            xtol : float\n",
        "                Average relative error in solution `xopt` acceptable for\n",
        "                convergence.\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * CG options:\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "            norm : float\n",
        "                Order of norm (Inf is max, -Inf is min).\n",
        "            eps : float or ndarray\n",
        "                If `jac` is approximated, use this value for the step size.\n",
        "        \n",
        "        * Powell options:\n",
        "            xtol : float\n",
        "                Relative error in solution `xopt` acceptable for convergence.\n",
        "            ftol : float\n",
        "                Relative error in ``fun(xopt)`` acceptable for convergence.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            direc : ndarray\n",
        "                Initial set of direction vectors for the Powell method.\n",
        "        \n",
        "        * Anneal options:\n",
        "            ftol : float\n",
        "                Relative error in ``fun(x)`` acceptable for convergence.\n",
        "            schedule : str\n",
        "                Annealing schedule to use. One of: 'fast', 'cauchy' or\n",
        "                'boltzmann'.\n",
        "            T0 : float\n",
        "                Initial Temperature (estimated as 1.2 times the largest\n",
        "                cost-function deviation over random points in the range).\n",
        "            Tf : float\n",
        "                Final goal temperature.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations to make.\n",
        "            maxaccept : int\n",
        "                Maximum changes to accept.\n",
        "            boltzmann : float\n",
        "                Boltzmann constant in acceptance test (increase for less\n",
        "                stringent test at each temperature).\n",
        "            learn_rate : float\n",
        "                Scale constant for adjusting guesses.\n",
        "            quench, m, n : float\n",
        "                Parameters to alter fast_sa schedule.\n",
        "            lower, upper : float or ndarray\n",
        "                Lower and upper bounds on `x`.\n",
        "            dwell : int\n",
        "                The number of times to search the space at each temperature.\n",
        "        \n",
        "        * L-BFGS-B options:\n",
        "            ftol : float\n",
        "                The iteration stops when ``(f^k -\n",
        "                f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
        "            gtol : float\n",
        "                The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
        "                <= gtol`` where ``pg_i`` is the i-th component of the\n",
        "                projected gradient.\n",
        "            maxcor : int\n",
        "                The maximum number of variable metric corrections used to\n",
        "                define the limited memory matrix. (The limited memory BFGS\n",
        "                method does not store the full hessian but uses this many terms\n",
        "                in an approximation to it.)\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * TNC options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stoping criterion.\n",
        "                If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
        "            xtol : float\n",
        "                Precision goal for the value of x in the stopping\n",
        "                criterion (after applying x scaling factors).  If xtol <\n",
        "                0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
        "                -1.\n",
        "            gtol : float\n",
        "                Precision goal for the value of the projected gradient in\n",
        "                the stopping criterion (after applying x scaling factors).\n",
        "                If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
        "                Setting it to 0.0 is not recommended.  Defaults to -1.\n",
        "            scale : list of floats\n",
        "                Scaling factors to apply to each variable.  If None, the\n",
        "                factors are up-low for interval bounded variables and\n",
        "                1+|x] fo the others.  Defaults to None\n",
        "            offset : float\n",
        "                Value to substract from each variable.  If None, the\n",
        "                offsets are (up+low)/2 for interval bounded variables\n",
        "                and x for the others.\n",
        "            maxCGit : int\n",
        "                Maximum number of hessian*vector evaluations per main\n",
        "                iteration.  If maxCGit == 0, the direction chosen is\n",
        "                -gradient if maxCGit < 0, maxCGit is set to\n",
        "                max(1,min(50,n/2)).  Defaults to -1.\n",
        "            maxiter : int\n",
        "                Maximum number of function evaluation.  if None, `maxiter` is\n",
        "                set to max(100, 10*len(x0)).  Defaults to None.\n",
        "            eta : float\n",
        "                Severity of the line search. if < 0 or > 1, set to 0.25.\n",
        "                Defaults to -1.\n",
        "            stepmx : float\n",
        "                Maximum step for the line search.  May be increased during\n",
        "                call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
        "            accuracy : float\n",
        "                Relative precision for finite difference calculations.  If\n",
        "                <= machine_precision, set to sqrt(machine_precision).\n",
        "                Defaults to 0.\n",
        "            minfev : float\n",
        "                Minimum function value estimate.  Defaults to 0.\n",
        "            rescale : float\n",
        "                Scaling factor (in log10) used to trigger f value\n",
        "                rescaling.  If 0, rescale at each iteration.  If a large\n",
        "                value, never rescale.  If < 0, rescale is set to 1.3.\n",
        "        \n",
        "        * COBYLA options:\n",
        "            tol : float\n",
        "                Final accuracy in the optimization (not precisely guaranteed).\n",
        "                This is a lower bound on the size of the trust region.\n",
        "            rhobeg : float\n",
        "                Reasonable initial changes to the variables.\n",
        "            maxfev : int\n",
        "                Maximum number of function evaluations.\n",
        "        \n",
        "        * SLSQP options:\n",
        "            ftol : float\n",
        "                Precision goal for the value of f in the stopping criterion.\n",
        "            eps : float\n",
        "                Step size used for numerical approximation of the jacobian.\n",
        "            maxiter : int\n",
        "                Maximum number of iterations.\n",
        "        \n",
        "        * dogleg options:\n",
        "            initial_trust_radius : float\n",
        "                Initial trust-region radius.\n",
        "            max_trust_radius : float\n",
        "                Maximum value of the trust-region radius. No steps that are longer\n",
        "                than this value will be proposed.\n",
        "            eta : float\n",
        "                Trust region related acceptance stringency for proposed steps.\n",
        "            gtol : float\n",
        "                Gradient norm must be less than `gtol` before successful\n",
        "                termination.\n",
        "        \n",
        "        * trust-ncg options:\n",
        "            see dogleg options.\n",
        "        \n",
        "        ** root options\n",
        "        \n",
        "        * hybrd options:\n",
        "            col_deriv : bool\n",
        "                Specify whether the Jacobian function computes derivatives down\n",
        "                the columns (faster, because there is no transpose operation).\n",
        "            xtol : float\n",
        "                The calculation will terminate if the relative error between\n",
        "                two consecutive iterates is at most `xtol`.\n",
        "            maxfev : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                ``100*(N+1)`` is the maximum where N is the number of elements\n",
        "                in `x0`.\n",
        "            band : sequence\n",
        "                If set to a two-sequence containing the number of sub- and\n",
        "                super-diagonals within the band of the Jacobi matrix, the\n",
        "                Jacobi matrix is considered banded (only for ``fprime=None``).\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation\n",
        "                of the Jacobian (for ``fprime=None``). If `epsfcn` is less than\n",
        "                the machine precision, it is assumed that the relative errors\n",
        "                in the functions are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound (``factor * ||\n",
        "                diag * x||``).  Should be in the interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the\n",
        "                variables.\n",
        "        \n",
        "        * LM options:\n",
        "            col_deriv : bool\n",
        "                non-zero to specify that the Jacobian function computes derivatives\n",
        "                down the columns (faster, because there is no transpose operation).\n",
        "            ftol : float\n",
        "                Relative error desired in the sum of squares.\n",
        "            xtol : float\n",
        "                Relative error desired in the approximate solution.\n",
        "            gtol : float\n",
        "                Orthogonality desired between the function vector and the columns\n",
        "                of the Jacobian.\n",
        "            maxiter : int\n",
        "                The maximum number of calls to the function. If zero, then\n",
        "                100*(N+1) is the maximum where N is the number of elements in x0.\n",
        "            epsfcn : float\n",
        "                A suitable step length for the forward-difference approximation of\n",
        "                the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
        "                precision, it is assumed that the relative errors in the functions\n",
        "                are of the order of the machine precision.\n",
        "            factor : float\n",
        "                A parameter determining the initial step bound\n",
        "                (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
        "            diag : sequence\n",
        "                N positive entries that serve as a scale factors for the variables.\n",
        "        \n",
        "        * Broyden1 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Broyden2 options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    reduction_method : str or tuple, optional\n",
        "                        Method used in ensuring that the rank of the Broyden\n",
        "                        matrix stays low. Can either be a string giving the\n",
        "                        name of the method, or a tuple of the form ``(method,\n",
        "                        param1, param2, ...)`` that gives the name of the\n",
        "                        method and values for additional parameters.\n",
        "        \n",
        "                        Methods available:\n",
        "                            - ``restart``: drop all matrix columns. Has no\n",
        "                                extra parameters.\n",
        "                            - ``simple``: drop oldest matrix column. Has no\n",
        "                                extra parameters.\n",
        "                            - ``svd``: keep only the most significant SVD\n",
        "                                components.\n",
        "                              Extra parameters:\n",
        "                                  - ``to_retain`: number of SVD components to\n",
        "                                      retain when rank reduction is done.\n",
        "                                      Default is ``max_rank - 2``.\n",
        "                    max_rank : int, optional\n",
        "                        Maximum rank for the Broyden matrix.\n",
        "                        Default is infinity (ie., no rank reduction).\n",
        "        \n",
        "        * Anderson options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial guess for the Jacobian is (-1/alpha).\n",
        "                    M : float, optional\n",
        "                        Number of previous vectors to retain. Defaults to 5.\n",
        "                    w0 : float, optional\n",
        "                        Regularization parameter for numerical stability.\n",
        "                        Compared to unity, good values of the order of 0.01.\n",
        "        \n",
        "        * LinearMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * DiagBroyden options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        initial guess for the jacobian is (-1/alpha).\n",
        "        \n",
        "        * ExcitingMixing options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    alpha : float, optional\n",
        "                        Initial Jacobian approximation is (-1/alpha).\n",
        "                    alphamax : float, optional\n",
        "                        The entries of the diagonal Jacobian are kept in the range\n",
        "                        ``[alpha, alphamax]``.\n",
        "        \n",
        "        * Krylov options:\n",
        "            nit : int, optional\n",
        "                Number of iterations to make. If omitted (default), make as many\n",
        "                as required to meet tolerances.\n",
        "            disp : bool, optional\n",
        "                Print status to stdout on every iteration.\n",
        "            maxiter : int, optional\n",
        "                Maximum number of iterations to make. If more are needed to\n",
        "                meet convergence, `NoConvergence` is raised.\n",
        "            ftol : float, optional\n",
        "                Relative tolerance for the residual. If omitted, not used.\n",
        "            fatol : float, optional\n",
        "                Absolute tolerance (in max-norm) for the residual.\n",
        "                If omitted, default is 6e-6.\n",
        "            xtol : float, optional\n",
        "                Relative minimum step size. If omitted, not used.\n",
        "            xatol : float, optional\n",
        "                Absolute minimum step size, as determined from the Jacobian\n",
        "                approximation. If the step size is smaller than this, optimization\n",
        "                is terminated as successful. If omitted, not used.\n",
        "            tol_norm : function(vector) -> scalar, optional\n",
        "                Norm to use in convergence check. Default is the maximum norm.\n",
        "            line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
        "                Which type of a line search to use to determine the step size in\n",
        "                the direction given by the Jacobian approximation. Defaults to\n",
        "                'armijo'.\n",
        "            jac_options : dict, optional\n",
        "                Options for the respective Jacobian approximation.\n",
        "                    rdiff : float, optional\n",
        "                        Relative step size to use in numerical differentiation.\n",
        "                    method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or\n",
        "                        function\n",
        "                        Krylov method to use to approximate the Jacobian.\n",
        "                        Can be a string, or a function implementing the same\n",
        "                        interface as the iterative solvers in\n",
        "                        `scipy.sparse.linalg`.\n",
        "        \n",
        "                        The default is `scipy.sparse.linalg.lgmres`.\n",
        "                    inner_M : LinearOperator or InverseJacobian\n",
        "                        Preconditioner for the inner Krylov iteration.\n",
        "                        Note that you can use also inverse Jacobians as (adaptive)\n",
        "                        preconditioners. For example,\n",
        "        \n",
        "                        >>> jac = BroydenFirst()\n",
        "                        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
        "        \n",
        "                        If the preconditioner has a method named 'update', it will\n",
        "                        be called as ``update(x, f)`` after each nonlinear step,\n",
        "                        with ``x`` giving the current point, and ``f`` the current\n",
        "                        function value.\n",
        "                    inner_tol, inner_maxiter, ...\n",
        "                        Parameters to pass on to the \"inner\" Krylov solver.\n",
        "                        See `scipy.sparse.linalg.gmres` for details.\n",
        "                    outer_k : int, optional\n",
        "                        Size of the subspace kept across LGMRES nonlinear\n",
        "                        iterations.\n",
        "        \n",
        "                        See `scipy.sparse.linalg.lgmres` for details.\n",
        "\n",
        "DATA\n",
        "    __all__ = ['OptimizeWarning', 'Result', 'absolute_import', 'anderson',...\n",
        "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
        "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
        "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(optimize.fmin)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function fmin in module scipy.optimize.optimize:\n",
        "\n",
        "fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None)\n",
        "    Minimize a function using the downhill simplex algorithm.\n",
        "    \n",
        "    This algorithm only uses function values, not derivatives or second\n",
        "    derivatives.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    func : callable func(x,*args)\n",
        "        The objective function to be minimized.\n",
        "    x0 : ndarray\n",
        "        Initial guess.\n",
        "    args : tuple, optional\n",
        "        Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
        "    callback : callable, optional\n",
        "        Called after each iteration, as callback(xk), where xk is the\n",
        "        current parameter vector.\n",
        "    xtol : float, optional\n",
        "        Relative error in xopt acceptable for convergence.\n",
        "    ftol : number, optional\n",
        "        Relative error in func(xopt) acceptable for convergence.\n",
        "    maxiter : int, optional\n",
        "        Maximum number of iterations to perform.\n",
        "    maxfun : number, optional\n",
        "        Maximum number of function evaluations to make.\n",
        "    full_output : bool, optional\n",
        "        Set to True if fopt and warnflag outputs are desired.\n",
        "    disp : bool, optional\n",
        "        Set to True to print convergence messages.\n",
        "    retall : bool, optional\n",
        "        Set to True to return list of solutions at each iteration.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    xopt : ndarray\n",
        "        Parameter that minimizes function.\n",
        "    fopt : float\n",
        "        Value of function at minimum: ``fopt = func(xopt)``.\n",
        "    iter : int\n",
        "        Number of iterations performed.\n",
        "    funcalls : int\n",
        "        Number of function calls made.\n",
        "    warnflag : int\n",
        "        1 : Maximum number of function evaluations made.\n",
        "        2 : Maximum number of iterations reached.\n",
        "    allvecs : list\n",
        "        Solution at each iteration.\n",
        "    \n",
        "    See also\n",
        "    --------\n",
        "    minimize: Interface to minimization algorithms for multivariate\n",
        "        functions. See the 'Nelder-Mead' `method` in particular.\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
        "    one or more variables.\n",
        "    \n",
        "    This algorithm has a long history of successful use in applications.\n",
        "    But it will usually be slower than an algorithm that uses first or\n",
        "    second derivative information. In practice it can have poor\n",
        "    performance in high-dimensional problems and is not robust to\n",
        "    minimizing complicated functions. Additionally, there currently is no\n",
        "    complete theory describing when the algorithm will successfully\n",
        "    converge to the minimum, or how fast it will if it does.\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
        "           minimization\", The Computer Journal, 7, pp. 308-313\n",
        "    \n",
        "    .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
        "           Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
        "           1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
        "           Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
        "           Harlow, UK, pp. 191-208.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f_test(x):\n",
      "    return x[0]**2+x[1]**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(f_test,np.array([1,2]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 43\n",
        "         Function evaluations: 82\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "array([ -3.62769110e-05,  -3.03662006e-05])"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(f_test,np.array([1,2]), xtol = 1.E-10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 85\n",
        "         Function evaluations: 164\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "array([  3.31355231e-11,  -1.82983207e-11])"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use vectorized arrays!, more efficient\n",
      "def f_test(x):\n",
      "    #return x[0]**2+x[1]**2\n",
      "    return np.dot(x,x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(f_test,np.array([1,2]), xtol = 1.E-10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 85\n",
        "         Function evaluations: 164\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "array([  3.31355231e-11,  -1.82983207e-11])"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (y_rand - A * sin(b*x))**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_rand_cut = y_rand[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_cut= x[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def objective_func(A):\n",
      "    return sum((y_rand_cut - 1 * sin(0.1*x_cut))**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(objective_func,np.array([1,0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.049409\n",
        "         Iterations: 10\n",
        "         Function evaluations: 39\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "array([ 1.,  0.])"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def objective_func(A):\n",
      "    return sum((y_rand - A[0] * sin(A[1]*x))**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_fit = optimize.fmin(objective_func,np.array([0.9,0.9]), xtol = 1.E-10, ftol = 1.E-10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 9.720672\n",
        "         Iterations: 80\n",
        "         Function evaluations: 161\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A_fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "array([ 1.00420077,  1.00025566])"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plb.plot(x,A_fit[0]*sin(A_fit[1])*x, 'r')\n",
      "plb.plot(x,y_rand,'k')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "[<matplotlib.lines.Line2D at 0x10b42bdd0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlYVGX/BvB7VNzDXdzTEBXQFHfRFBdILczUN3PPJa2s\nn5pp9vaW5luKW4lpZZjmrpW5ZEqJAqmElrjgBiqgiIKigiA7fH9/nJcjEyDLzHCG4f5c17mc/Xzn\nOHNz5jnPeR6diAiIiKhUK6d1AUREZDiGORGRBWCYExFZAIY5EZEFYJgTEVkAhjkRkQUoVJhPmjQJ\nNjY2aNeunXrb/fv34erqilatWsHNzQ1xcXEmK5KIiJ6sUGE+ceJEeHt7693m4eEBV1dXhIaGon//\n/vDw8DBJgUREVDBdYU8aioiIgLu7O4KDgwEAbdq0gb+/P2xsbBAdHQ0XFxdcvnzZpMUSEVHeit1m\nHhMTAxsbGwCAjY0NYmJijFYUEREVjVEOgOp0Ouh0OmO8FBERFUOF4j4xu3mlQYMGuH37NurXr5/n\n41q2bIlr164Vu0AiorLI1tYWV69eLfTji71nPmTIEGzcuBEAsHHjRgwdOjTPx127dg0iwkUE8+fP\n17wGc1m4LbgtuC2evBR1J7hQYT5q1Cg4OzsjJCQETZs2xYYNGzBv3jwcOnQIrVq1wpEjRzBv3rwi\nrZiIiIynUM0s27dvz/N2Hx8foxZDRETFwzNAS5CLi4vWJZgNbovHuC0e47YovkL3My/2CnQ6mHgV\nREQWp6jZyT1zIiILwDAnIrIADHMiIgvAMCcisgAMcyIiC8AwJyKyAAxzIiILwDAnIrIADHMiIgvA\nMCcisgAMcyIiC8AwJyKyAAxzIiILwDAnIrIADHMiIgvAMCcisgAMcyIiC8AwJyKyAAxzIiILwDAn\nIrIADHMiIgvAMCciMjcZGUV+SgUTlEFERMVx8SKwdClw40aRn8o9cyIirZ04Abz8MtC3L9CyJbBr\nV5FfgnvmRERaEAF+/x3w8ADCw4H33gO2bgWqVi3WyzHMiYhKUmYm8NNPSoinpwPz5gEjRwJWVga9\nrMHNLIsXL4ajoyPatWuH0aNHIzU11dCXJCKyPCkpwLffAq1bA56ewMKFwLlzwNixBgc5YGCYR0RE\nwMvLC0FBQQgODkZmZiZ27NhhcFFERBbj4UPloOYzzwB79wIbNgDHjwPu7kA54x22NKiZxdraGlZW\nVkhKSkL58uWRlJSExo0bG6s2IqLSKyYGWLUKWLsWeP554OBBoH17k63OoD8LtWvXxuzZs9GsWTM0\natQINWvWxIABA4xVGxFR6RMeDkyfDrRpAzx4AJw8qRzYNGGQAwaG+bVr17By5UpERETg1q1bSExM\nxNatW41VGxFR6REcDIwZA3TuDNSoAVy+DHz1ldK8UgIMamb5+++/4ezsjDp16gAAhg0bhoCAAIwZ\nM0bvcQsWLFAvu7i4wMXFxZDVEhGZj2PHlJ4pp04BM2cqAV6jRpFfxs/PD35+fsUuQyciUtwnnz17\nFmPGjMFff/2FypUr47XXXkPXrl0xffr0xyvQ6WDAKoiIzI8I8OuvSojfvg3MnQtMmABUrmy0VRQ1\nOw3aM2/fvj3Gjx+Pzp07o1y5cujYsSOmTp1qyEsSEZmvjAxg504lxMuXV/qIjxgBVND+lB2D9swL\ntQLumRNRaZecDKxfDyxfDjz9tBLizz8P6HQmW2WJ7pkTEVm0uDilDXzVKqBbN2DbNqBHD62ryhMH\n2iIi+qdbt5R2cFtbICQEOHxYOeHHTIMcYJgTET125QowdSrQti2QmgoEBQEbNwKOjlpXViCGORFR\nUJAy2JWzM9CggbI37umptI+XEgxzIiqbRABfX+VA5pAhSpt4WJgyAFa9elpXV2Q8AEpEZUtWFrBv\nn9K98MEDpW187FigUiWtKzMIw5yIyoa0NKU3ypIlQLVqwAcfAEOHKv3FLQDDnIgs26NHwLp1wIoV\nQKtWwOrVQL9+Ju0jrgWGORFZpnv3lOBeswbo3VuZV7NLF62rMhkeACUiy3LzJvDuu4CdnTLL/dGj\nyjRtFhzkAMOciCzF5cvApEnAs88qTSjnzgHffadM01YGsJmFiEq3kyeVninHjgHvvANcvQrUrq11\nVSWOYU5EpY8I4OOjhPiVK8B77wGbNyu9VMoohjkRlR6ZmcDPPyshnpICvP8+MGqUUWa3L+0Y5kRk\n/lJTlT3vpUuVJpSPPzb67PalHcOciMxXQoIyu/0XXygHNr28lG6GFtZH3BgY5kRkfu7eVQa6+uYb\nwNVVmaKtQwetqzJr/I1CROYjIkLpkdK6NRAbCwQGAtu3M8gLgWFORNo7fx4YNw7o1EnpkXLhgrJX\n3rKl1pWVGgxzItJOQIAy/KyrqzIBxLVrSk+Vhg21rqzUYZs5EZUsEeDgQSW0b94E5sxRZryvUkXr\nyko1hjkRlYyMDODHH5UQB5QZ7v/1L6ACY8gYuBWJyLSSk4HvvweWLQOaNAEWLwYGDWL3QiNjmBOR\nacTFAV9/DaxapYxYuHkz0LOn1lVZLIY5ERlXdDSwcqVygs/gwcChQ8ps92RS7M1CRMZx7RrwxhuA\ng4Myu8+pU8reOIO8RDDMicgwZ84og1116wbUrauMK/7ll0Dz5lpXVqYwzImo6EQAf3/lQOYLLygn\n+4SFAZ9+CtSvr3V1ZRLbzImo8LKygF9+UboX3rsHzJ0L7NkDVKqkdWVlnsF75nFxcRgxYgTs7e3h\n4OCAwMBAY9RFROYkPR3YtAlo1w5YuFCZY/PSJWDKFAa5mTB4z3zGjBkYPHgwfvrpJ2RkZODRo0fG\nqIuIzEFSErBuHbBihTJOysqVwIAB7CNuhnQiIsV9cnx8PJycnBAWFpb/CnQ6GLAKItLC/fvAmjXA\n6tVAr17KjD5du2pdVZlS1Ow0qJklPDwc9erVw8SJE9GxY0e8/vrrSEpKMuQliUhLN28Cs2cre+Hh\n4YCfH7BrF4O8FDComSUjIwNBQUFYvXo1unTpgpkzZ8LDwwMLFy7Ue9yCBQvUyy4uLnBxcTFktURk\nbCEhypRsu3cDr70GnD0LNG2qdVVlip+fH/z8/Ir9fIOaWaKjo9GjRw+Eh4cDAI4dOwYPDw/s37//\n8QrYzEJkvv7+W+mZ8scfwPTpwNtvA3XqaF0VoYSbWRo0aICmTZsiNDQUAODj4wNHR0dDXpKITE0E\nOHxYOZA5bBjw3HNKk8r8+QzyUsygPXMAOHv2LKZMmYK0tDTY2tpiw4YNqFGjxuMVcM+cyDxkZip9\nwj08gMRE5aDm6NFAxYpaV0Z5KGp2GhzmBa6AYU6krdRUYMsWpU28Zk3ggw+U2X3K8QRwc1bU7OQZ\noESWKiFBGbnw88+VKdnWrgX69GEfcQvFMCeyNHfvKgNdff010K8fsG8f0LGj1lWRifF3FpGluH4d\nmDEDaN0aiIlRJkveuZNBXkYwzIlKuwsXgAkTlNCuVEm5vnYtYGendWVUghjmRKXVn38CL70E9O+v\n7I1fvaoc5GzYUOvKSANsMycqTUSA335TuhdGRABz5gA7dgBVqmhdGWmMYU5UGmRkAD/9pIR4ZiYw\nbx7wyiuAlZXWlZGZYJgTmbOUFGDjxsfNJ599pkySzO6F9A8McyJzFB8PfPONMn54p05KoPfqpXVV\nZMYY5kTmJCYG8PQEvv0WGDhQaR9/9lmtq6JSgL1ZiMxBWBjw1luAvb2yV37ypHIKPoOcColhTqSl\ns2eVwa66dgVq1VLm1VyzBnjmGa0ro1KGYU5U0kSAo0eBF14ABg0COnRQ9sw/+wywsdG6Oiql2GZO\nVFKysoBff1W6F8bEAHPnKlOyVa6sdWVkARjmRKaWnq6MkbJkCVChgtJHfMQIoHx5rSsjC8IwJzKV\npCRg/Xpg+XKgRQvlXzc39hEnk2CYExnbgwfKQcwvvwScnZXT7bt317oqsnA8AEpkLLduKWOl2Noq\ng175+iqz3TPIqQQwzIkMdeUK8PrrQNu2Svv4mTPA998DDg5aV0ZlCJtZiIrr1CnloKavLzB9OhAa\nCtStq3VVVEYxzImKQkQJbw8P4OJFYPZs5SBn9epaV0ZlHMOcqDCysoC9e5UQj48H3n8fGDMGqFhR\n68qIADDMiZ4sLQ3YulVpTnnqKeCDD5TZfdhHnMwMw5woL4mJgJcX8PnnyuBXX30F9O3LPuJkthjm\nRDnFxgKrVyv9xF1cgD17lPHEicwcuyYSAcCNG8DMmUCrVkBUFHD8OPDjjwxyKjUY5lS2XboETJyo\njFxYoQIQHKw0r7RqpXVlREXCZhYqm06cUHqmBAQA77yjnLFZu7bWVREVm1H2zDMzM+Hk5AR3d3dj\nvByRaYgAv/8O9OsHjBwJ9O8PhIcD//kPg5xKPaPsmXt6esLBwQEJCQnGeDki48rMVMYN9/BQuhq+\n/z7w6quAlZXWlREZjcF75jdv3sSBAwcwZcoUiIgxaiIyjtRUZWLkNm2UWe4/+QQ4dw4YN45BThbH\n4D3zWbNmYdmyZXj48KEx6iEy3MOHwNq1wBdfKAc2v/sOeO459hEni2ZQmO/fvx/169eHk5MT/Pz8\n8n3cggUL1MsuLi5wcXExZLVEebtzB/D0VILczQ04eBBo317rqogKxc/P74k5WhCdGNA28u9//xub\nN29GhQoVkJKSgocPH2L48OHYtGnT4xXodGx+IdMKD1dm8dm2DRg1CnjvPc5uT6VeUbPToDDPyd/f\nH8uXL8cvv/xiUEFEhRYcrIyZcvAgMHUqMGMG0KCB1lURGUVRs9OoJw3p2CZJJeH4ceDFF5WmlHbt\ngLAwYPFiBjmVaUbbM893BdwzJ2MQAQ4cULoXZk/PNmECUKWK1pURmURRs5NngJJ5y8gAdu5UmlPK\nlQPmzQNGjFBOvSciFb8RZJ6Sk4ENG4Bly4BmzYClS4Hnn2f3QqJ8MMzJvMTFKWOHr1oFdOumTAzh\n7Kx1VURmj6Mmknm4fVs5zd7WFggJAQ4fVqZpY5ATFQrDnLR19SowbRrg6Kg0rQQFARs3KteJqNAY\n5qSN06eVkQt79ABsbJS98VWrgKef1royolKJYU4lRwTw8wMGDlT6iXftqvQRX7gQqFdP6+qISjUe\nACXTy8oC9u1T+og/eADMnau0h1eqpHVlRBaDYU6mk5YGbN+u9BGvWhX44ANg6FCgfHmtKyOyOAxz\nMr5Hj4B164AVK5S5NFetUmb1YR9xIpNhmJPx3LsHrF4NrFkD9O6tzO7TpYvWVRGVCTwASoa7eRN4\n913Azg64cQP44w/gp58Y5EQliGFOxXf5MjBpEvDss0oTyrlzyqw+bdpoXRlRmcNmFiq6v/5SeqYc\nPQq8/bZy4g9ntyfSFMOcCkcE8PFRQvzKFWU2n02bgGrVtK6MiMAwp4JkZgK7dyshnpSkjJ8yejRn\ntycyMwxzyltqKrB5szL0bO3awEcfAe7uypjiRGR2GOakLyFBmd3+iy+UA5teXko3Q/YRJzJrDHNS\n3L2rnNzz9dfAgAHA/v2Ak5PWVRFRIfE3c1l3/TrwzjtA69ZKoAcGAjt2MMiJShmGeVl1/jwwfjzQ\nsaPSI+XCBeCbb4CWLbWujIiKgWFe1gQEAEOGKE0p9vbAtWtKT5WGDbWujIgMwDbzskAE8PYGFi9W\nTr2fM0eZ8b5KFa0rIyIjYZhbsowM4McflT1vEWDePOCVV4AK/G8nsjT8VluilBTg+++BZcuAxo2V\nPfJBg9i9kMiCMcwtSXy80rXQ0xPo3Fk53b5nT62rIqISwDC3BNHRwMqVygk+gwcDv/8OtGundVVE\nVILYm6U0u3YNePNNwMEBSEwETp1STsFnkBOVOQaFeWRkJPr27QtHR0e0bdsWq1atMlZd9CRnzgCj\nRgHdugF16ijjiq9eDTRvrnVlRKQRnYhIcZ8cHR2N6OhodOjQAYmJiejUqRP27NkDe3v7xyvQ6WDA\nKiibiDJ+uIcHcPYsMGsWMHUqYG2tdWVEZAJFzU6D2swbNGiABg0aAACqV68Oe3t73Lp1Sy/MyUBZ\nWco4KR4eyun2c+cqQ9JWqqR1ZURkRox2ADQiIgKnT59Gt27djPWSZVt6OrB9O7BkiRLcH3wADBsG\nlC+vdWVEZIaMEuaJiYkYMWIEPD09Ub169Vz3L1iwQL3s4uICFxcXY6zWMiUlKfNoLl8O2NoqQ9G6\nurKPOJGF8/Pzg5+fX7Gfb1CbOQCkp6fjxRdfxKBBgzBz5szcK2CbeeE8eKAcxFy9Wukb/v77ygFO\nIiqTipqdBvVmERFMnjwZDg4OeQY5FUJUlDKfpq0tEB4O+PkBP//MICeiIjEozI8fP44tW7bA19cX\nTk5OcHJygre3t7Fqs2whIcCUKUqf8MxMpYfK+vXKSIZEREVkcDNLgStgM4u+v/9Weqb88QcwfTrw\n9ttKX3EiohxKtGsiFZIIcOSIEuKXLwOzZysDYeVxsJiIqDgY5qaUmQns2aOEeGKiclBz9GigYkWt\nKyMiC8MwN4W0NGDLFmDpUqBGDeDDD5XZfcpxKBwiMg2GuTElJgLffgt8/jng6KgMR+viwj7iRGRy\nDHNjiI0FvvwS+OoroF8/YN8+ZaJkIqISwt/9hrhxA5gxA2jVCrh9W5kseedOBjkRlTiGeXFcuABM\nmAA4OSnjppw/rzSv2NlpXRkRlVFsZimKwEClZ8qffwL/93/A1atArVpaV0VExD3zAokA3t7KgcxX\nX1UGvQoPV3qoMMiJiuz06dM8kdAEGOZ5uH//PpCR8bj9e84c4PXX8fPSpXg4bhxQtWqBr/Hnn39i\ny5YtJVAtkflJSEjAL7/8kud9HTt2xLFjx0q4IsvHMP8H/0OHUKdOHaBNG2DVKsjChYg/ehQYMwbD\nR47E1q1bC/U6hw8fxp49e0xcLZF58vLywpAhQ3Ldnr1HnpmZmeu++/fvq5d1Oh3++uuvJ67j0qVL\nyMjIMLBSy1EmwzwkJARV/7l3/fAhsGQJzg0frlz//nvg+HF43b6NmrVqISYmBgD0xmu/fv26ejk2\nNha6HP3Jb968qT6HqKzJ+V1IS0vD0aNHER4ejsuXLwMAUlJS1PszMjIQEBCAOnXqwMvLC9HR0QCU\n72lqaip2796d5zocHBywdu1aE76L0sViwzwtLQ2BgYF53hcWFobk5GTExsYCMTG4OHUqyteogZTT\npxE9ciQAwON/PwNPnToFAOr0eGlpaQCAq1evonnz5hARlCtXDuvXrwcA3Lt3D9HR0WqYJycnQ6fT\n4caNG0hISEBkZKRJ3zeRsel0OsTHx+vdtmnTJnz77be4cOECAODOnTuoXLkyli9frve4jIwM+Pj4\noHfv3njmmWfg4OAAAPD390enTp1w9OhR+Pr6omfPngCAGTNmoGHDhgCAcePGoVq1ahg2bFi+tYWH\nhxvtfZZ6YmIlsIo8rV27Nt91L1q0SADIt336CAD5vl8/ASAApFatWurlqKgoGTBggLz22mvqbdmL\nh4eHAJDt27fr3d6jRw/1srW1tYSFheV6bk6ZmZkya9asktgkRIWSmJgo58+fFxGRtLQ0ASCRkZEi\nIvLDDz/k+jxnZWXJ3Llz1eu9e/eWjz76SP0Off3117mek3MZOnToE+/PfszYsWPl0aNH8scff6h1\nDR8+XGbOnKnl5jKZomanxYb58uXL9dadlJQkNjY2eh8Qdzs7ASDdunV74gfJ19dXAEj58uXV23r2\n7FngBzC/JaeYmBgBIElJSSW9iYjytHPnTundu7eIiNy7d08AyC+//CI7duyQ999/P9fnuUWLFvkG\ndMOGDeWtt94q9HfD0dFRAMjmzZvl/v37+T5u9+7dAkDq1asnAGTevHmSmpoqUVFRkpWVles9zZ8/\nX0JCQkp6UxqEYf4/n332md66u9rbF/oDtWrVKr3rqampkpaWJi1bttS7vW/fvgJAFi5cmOfrNG7c\nOM/b33jjDUlKSpKsrCw5e/asABBXV1dNthNRtl27dklWVpZ4enpKs2bNRETk77//1vvsDhkyRO+6\nk5NTnp/xcuXKqcGc39KhQwf18sqVK8XHx0dElMz44Ycf1Mv/+c9/cj23YsWK+b6ul5dXrveW/Tql\nSVGzs1S1mYeGhj7xoGJKSgrS09MBPG7bfrBjB6537gyEhamP6969OyZPngwAaNq0KU6fPq33Oi+8\n8ILe9YoVK8LKygpbt27FlClT1NvfeOMN7NixA9OnT8eVK1fw1VdfAQDWrVuHM2fOIDk5WX3sRx99\npF7+5ptvMGrUKJQrV05t8zt06BC++eYbHDp0SO95RCUhJCQEw4cPR3x8vHrMZ+jQoejcubP6mJYt\nW2Lfvn3qdS8vL/z6669YunRprtd78cUXce7cOVy9ehWA0lU3Zw+WJk2a4OOPP1avN2jQAP3791ev\nZ2VlAQB2796NefPmYdu2bRg4cCAAoF27dur3Oy/nzp1TL9+5c0c97lWpUqXCbYzSykR/VFTGWkVk\nZKQAkO7du+f7mOyfXPktHh4eej+1Zs2apf61DgoKko0bNwqgtAEGBQXJiRMncq3jzJkzAkAuXryY\n676HDx8KANmyZYuIiN6efPZ9HTp0kP/+979PrHP48OGSkpJi6CYjKtCtW7ckMTFRpk2bJgDkypUr\nMnHixDw/l97e3url2bNnq6+RlZWl3r5ixQqJjo6Whw8fqvd7eXmpn+dZs2bJokWLREQ5XhQcHCwA\nJDg4WH28jY2NXLp0KVet2b8KmjdvLgAkLi5Opk2bJiNHjtSr08XFRQDIjh079G5ftmyZpKeni6en\np6k2p1EVNTtLTZjn/E/JzMxUb//000/Fzc1NZs+eXWDzyfHjxwtcT3JycoGPuXfv3hPr/P3330VE\naQ/v1KmT6HQ6ERG1LS82NlZ8fX3l6tWrAkAGDx6cb819+vRRXzswMFBu3bqlXr9x40aBtRI9ia2t\nrTRs2FA9BtSvXz9p1apVnp/Fixcv5voMZrt7965069ZN4uLiTFbrmDFjpG7durJ+/Xr59NNP9e4D\nIPPnz5e9e/fm+11aunSpXLx4UQBIaGio+Pr6PvG7rDWLDPN//uXNXs6fP5/n7Q3r1pXExMRct4eF\nhRnhHRVNSkrKEz/gMTExkp6e/sQ/Qo8ePRIRUffaRR7vDd2/f19ERPz9/fM88EMkovxqvHr1qogo\nOwEHDx5UDyICkGrVqul95qpXr56rXTq7R4tWUlJS8u0o8Pvvv0tCQoKIPN7xGzt2rF79zZs3l/nz\n5+vd9uGHH5bkWygSiwvzGzduFLjH/c+lQYMGIqLsAaempkpwcLDZB11GRoYkJibKyy+/LADk008/\nzfO9jR49WkQeN+lERkaqwa71l43M15QpUwRQek1Nnz5dAOVAfM7P1vLly9UADw0NVb97Tk5O8sMP\nP+j9IjZnGzdulPnz54uIyHfffffErFi2bJnec//66y9Zs2aNBlXnZlFhfuHChQKD+8iwYfmGeWmU\n3R3r2rVr+b7njIwMWbdundqkExISIgDybGeksisrK0u8vb0lMzNTXnjhhXx3Dho3bixjxowREZEl\nS5YIAElLSxMR0duBKK1ynvuR13Ljxg05evSo7Ny5U73NHJT6MI+Pj5eAgIBC7YG/NGiQiIh6ok/2\nAUwbGxtTvJUSkZmZKd27d5fMzEz59NNP5dtvvxUA8vrrr6vv+8svv8xzexw7dkzr8smMZPcR37dv\nX77nRfz888+SnJys/nLN3knIFhsbW+rPgejUqZMAkMqVK+e5DV588cVct+XlwYMH6h+5klDqw3zS\npEm5NuxbL70kFXU60QGyb/Jk2bdzp6xbt07S09NFRNnIiYmJ6vpKc5jnBVD6yM6ZM+eJf9y8vLxk\n+/btWpdLGjh9+rT6HciWfbAvr+XSpUt5dgjIysoq0cAqCf7+/mrf8y+//FI+/vjjAncU8+rJBqBE\nz9YuNWGe3b4bEREhjx49ktjYWDl27FieG1ZatZI7K1dKTCF6bwQGBup1c7IEUVFRkpSUJG+++eYT\nP4Dt27dXfzbGx8eLiNI90pQ9DMg8AJAqVapI165dRUQkODhYDh48mO9nhfSHJnB2dtbbPvXr15fe\nvXtL8+bNReRxhwM3N7cSq69UhHlGRoYAkNjYWL0NaGdnJ6+MGCEA5JO6dWXAU0/J30uXimRkmLrM\nUsHf31/dVp06dZI+ffrI5MmTZf78+dKgQQN5+umn1fs//PBDeeeddwR43Id31apVkpqaqvG7IGP7\n5Zdf9L5H586dUy87OTnJiP99p7KX7IAiZZgPADJ+/Ph8//C5u7url7OHOSgJpSLM79y5IwD0Djhk\nL/GNGsnd3r1FDh8WMfMeKFqYNm2aDBo0KNcewrZt2/S2Y84DXtOmTVNPzjh79qxGlZOxrV27VsaO\nHat3Wvw/l9dee02ysrIkMzNTAIitra3WZZsdQBnb5aeffiqw+aVt27bq8QV3d3fZuHGjSesq0uNN\nVMfjFeRRUHYvlUaNGultKPunnhL56y9Tl1TqpaSk5OozHx8fLwBkzJgxeY5SV7duXQGUAYoKc2IU\nmbfo6Gj1/zbn96hx48bSpk0b9frEiRPV58TGxlpce7gxnD9/XhITEyUhIUHdbjkvA0oPOQCi0+nU\n7xkAGTBggMnqKmqYGzw2i7e3N9q0aQM7OzssWbLkiY9NS0uDiMDR0REAcOvWLb37q7dpA+QYC4Ly\nVqlSJbRo0ULvNmtrawBAXFxcrvsAZfIMAHj55ZdRpUoV0xdJJuXp6alezvk9+uijj1CvXr08n1On\nTh1YWVmZvLbSxtHREdWqVUP16tVx4sQJZGVloXr16hg7dqz6mC+++AJff/01WrZsCQDqjGM+Pj5I\nTk5GRESEFqXrM+QvR0ZGhtja2kp4eLikpaVJ+/btc41ZAiijDmafgNDRwUEASLk8fsKsW7fOkHLK\nPEAZejTn9Tp16qh77TkXEZGTJ0/KggULtCqXCgAoQ89my8zMlN9++00OHTokXbp00fv/7NixowCQ\n9PR06fO/cfoBmM0JMKVR9jkfL730knpbvxxzH2Qv2W3qxlbU16xgyB+CkydPomXLlmjevDkA4NVX\nX8XevXtCAqDvAAAQnUlEQVRhb2+v97ico5UFXbwIALh88iT2+Pmhffv2SEpKgouLC2rWrGlIOWVe\nWFhYrm1Yo0YNWFtbIyQkBK1btwagjAIZHx+PDz/8EIcOHULnzp3h5ubGvTYzkpqaCgD4/fff0bNn\nT9SqVQuXLl3C888/n+uxgwcPhqenJ8LCwlChQgX07t0boaGhuX75UtHUqlULANTvDQA89dRTuR6X\n38TVJc6Qvxw//vijTJkyRb2+efNmefvtt3P9dclryR5HgUzH29tbTp8+rV739/eXEydO5NnWmj04\nGJmH7GEdspe33npL9uzZo/crNvuyOQ8WVdrdu3dP7zjD0aNH9WZV6tWrl3r5ypUrIiKSmpoqGUbo\ngVfUeDaozTznpK2FNW/ePGzcuFFvYmQyjeeffx4dOnRQr/fu3Rtdu3ZVrw8aNAhffvkldDodTp48\nqfdcnU6HM2fOlFitZV1GRobeOPZBQUF693/11VeYPn26ej17PH7g8fESMr7atWvr/WLt1asXlixZ\nAmtrazg7O8Pd3V29b8KECViwYAEqVaqEOXPmICMjA7dv3wYAtG/fHl9//bVJazWomaVx48Z6ExRH\nRkaiSZMmeT528uTJaNKkCVxcXODi4mLIaskIGjVqhHXr1gEAqlatCn9/fwDArl271A/o2bNn9f4Y\nkOnY2dnB0dER+/fvR1RUFK5fv57rMVFRUahXr57eJA8AUKGCQV9jKoYHDx5Ap9Nh06ZNAIAuXbog\nICAAAQEBAJQDpmvXrkVSUhJEBOfOncvz/zQnPz8/+Pn5Fb8oQ34GpKenyzPPPCPh4eGSmpqa7wHQ\nHTt2GLIaMrLg4GCJiIhQrwcGBgoAiYiI0Ptp/9FHH2lYZdkCKPNfPnr0KFeT5IwZM9TLKSkp6sQP\nHTp0kD179mhcedmWPa3erl27Cuyj/u9//7tIr13UeDboT3qFChWwevVqPP/888jMzMTkyZNzHfwE\ngJEjRxqyGjKytm3b6l3v2rUrBg8ejNdee03v9tOnTyMqKgrTpk3DypUrcfz4cVhZWWH06NElWG3Z\nceHCBVSrVg01a9bEvn37ULFiRQwZMgTDhw9H5cqVUalSJXUBkGu6Qyp5nTp1wp49e9CzZ0/1trZt\n2+L8+fO5HvukKS+NokjRXwwlsAoygoLGfc5erK2ttS61VMrKypJr166JiNKl18/PT3bt2iUJCQny\nySef6G3jYcOGaVwtFUf2kAA3btxQhxL+5/Lcc8/lmhIyeziGfypqdjLMSUREbWI5cuSIPPvss2Jv\nb5/nh7FVq1Zal1oqZc9PK6JMgJC9PT/44AP18rhx4wSATJ48WeNqqbi+//57dRKPrVu36v1fZy9t\n27aV9PR09XEeHh4MczKup556Sm7evCmPHj1SByAClCnFli9fLoAyJvSDBw+kZs2acvnyZa1LNmu9\nevWSixcvys8//6xuy4iIiFztqw4ODtK6dWsRUb4vK1as0LhyMqZGjRrlOZb6e++9J/v371ev/3M2\ntKJmp+5/TzIZnU4HE6+CTCS762n2/9+RI0fQv39/9f7PP/8c1apVw7/+9S9kZmaiTp06xequaolS\nU1NRuXJlNGvWDDdu3Mj3cbNnz8Z///tfdYiFCxcuoHXr1uyhYkESEhJQqVIlVKlSBYMGDcKvv/6K\nhg0bqt0Wc4qPj4ePjw+GDRtW5OxkmFO+fHx84Orqqv7/iQjKlVNOTejevTsePHiAkJAQzJkzB8uW\nLcPmzZv1xrMoqzIzMwsVxgcPHsTAgQNLoCIyB/fu3UPlypURGBiIM2fO4L333sv3sSJS5Ow0eKAt\nslwDBgzQ+zDpdDp8/vnn6n0hISGoVKkSDhw4AAA4cOAARo8ejSNHjqBv376oVq0afH19c73uSy+9\nhFOnTpXMmzCymzdv4u7du7luX7RoEeLj43Hnzh29E0my5TwNvF+/fti7dy+DvIypU6cOqlWrhv79\n+xcY0sXq+WJYa1DBSmAVVMK6d+8uoaGhUq1aNZk6daoAyiw3OYdezV7ef/99iY2N1Tu9GYDMmTNH\nvX7q1CnZvXu3Fm+lyMqVKycApGrVqnq3438Hh//5/rOX7IObACQgIECj6slcLFq0SP085DV4l5WV\nVcmezk9l059//gk7Ozt88sknmDt3LgCgfPnyuHLlSq7HhoaGom7dunj33Xf1bo+Li1MvT5o0CS+/\n/DLWr1+fq6+7ucnKygIAJCUlAQBWrFiBb775BoDyXnNq3bo1+vXrB0Dpyx8bG4usrCz06NGjBCsm\nc/Tmm29iz549EBEcPnwYq1ev1ru/cuXKRX5NtpmTwXQ6HXQ6HaytrREfH5/nY8aOHYvvvvsOvr6+\navNCWFgYWrRogQ4dOuDs2bNwdnZGQECAWX1e9u7dCxsbG3Tv3h2A/nhECQkJeY6iBwD79u3DwIED\nYWVlBX9/f/To0QMVK1YskZqpdNLpdLhw4QIuXboEa2truLm5sc2cSpabmxu6deuW5wfP2toaVatW\nxZYtW1CpUiW9duLdu3cDeNxbpqBJM+rVq4f169cbpeaEhIRcg4vlZejQoRg8eDB0Oh2+//57vfty\nnuXXqFEjAMr4GnPnzkWvXr3UAZr69OnDIKcCJSUlwcHBAcOHD4erq2vRX8DITUG5lMAqSGPp6emS\nnp4uBw8e1JvXNds/JxzOueQcd+Sfz1u/fr0MHDhQ7t+/r85hOW7cOKPUvGLFijw/m5cvX5a3335b\nkpKSJDk5+Ylnw+ZsB79+/bpcv349V19houIqanYyzMno6tSpo/f/7ufnp4Ze9kzxBw8ezDck79+/\nLyKiN2P6gAEDBICMHDlSRJSx9LMvP0l+Bxs3bdokAOTRo0fqbeHh4XozsedcXnnllTxvd3d3l6lT\npxqyuYjyVNTsZDMLGV3Dhg31xoDOblfetm2beiB04MCB2LBhg97z3NzcAChjSK9Zs0avacLHxweA\nMo8sACxevBg7d+5U71+5cqXe+OsLFizAb7/9BmdnZzx8+BBZWVnw9fWFiOCVV17B+PHjAQDh4eEA\ngOnTp6NFixa5Zo3JnvOxbt262LZtGw4fPqyuDwBsbGywdu3aom8kImMzzd+Ux0pgFWRm7t69K1FR\nUer1y5cvCwCJjo6W+Ph4dUbz7KF327Ztq45T8s/ujTVq1NC7bmdnJz/++GOuJhkAUq5cOXn48KHa\nJJNXV8mcs8RkL+np6dKyZcs897yzx05544031HWFhYWJiDLbfVxcXAltVSpripqdDHMyufj4eOna\ntWuu2+/fvy/16tUTEeVzsnDhQhERuXjxohqmzs7OT2y3fvTokQQEBOjd1rRpUwGUER6f9NzspXHj\nxurlJk2ayOeffy4AxM3NTSIjI2X8+PGyadOmEt1mREXNTnZNJLPg6+sLJycn1KxZUx3X5KmnnsKu\nXbvU5pe8jBs3Dps3by7UOubNmwcPDw/1uqurK6KionDxf5OMA0BwcDDatm0LnU6HO3fuoF69esV/\nU0QG4NgsZBGSk5NRuXJltV/3rl27MGLECPX+d999F7/++itCQkLQpk0bAMDly5fzfb3u3bvjyJEj\nqFq1Kl588UXs378fvXv3hru7OyIiItCrVy+MGjUKERERePrpp0375ogKgWFOFunOnTuwsbHBiRMn\nUKVKFTz99NOIj49Hs2bNEBsbi+PHj2PDhg348MMP4eXlhSFDhuC5557DCy+8gD59+uDjjz9GxYoV\nMXXqVKxZswa1atXChAkTsGbNGnUdYWFheOaZZzR8l0SPMczJIiUnJ6Nq1aq5Pkvnzp3Ds88+W+TX\nS01NRYUKFVC+fHljlUhkVAxzIiILwCFwiYjKIIY5EZEFYJgTEVkAhjkRkQVgmBMRWQCGORGRBWCY\nExFZAIY5EZEFYJgTEVmAYof5nDlzYG9vj/bt22PYsGH5TuRLRESmV+wwd3Nzw4ULF3D27Fm0atUK\nixcvNmZdFsnPz0/rEswGt8Vj3BaPcVsUX7HD3NXVFeXKKU/v1q0bbt68abSiLBU/qI9xWzzGbfEY\nt0XxGaXNfP369Rg8eLAxXoqIiIqhwpPudHV1RXR0dK7bFy1aBHd3dwDAZ599hooVK2L06NGmqZCI\niApmyBx1GzZsEGdnZ0lOTs73Mba2toWah5ELFy5cuDxebG1ti5THxR7P3NvbG7Nnz4a/vz/q1q1b\nnJcgIiIjKXaY29nZIS0tDbVr1wYA9OjRA1999ZVRiyMiosIx+UxDRERkeiY7A9Tb2xtt2rSBnZ0d\nlixZYqrVmL3IyEj07dsXjo6OaNu2LVatWqV1SZrLzMyEk5OTehC9rIqLi8OIESNgb28PBwcHBAYG\nal2SZhYvXgxHR0e0a9cOo0ePRmpqqtYllZhJkybBxsYG7dq1U2+7f/8+XF1d0apVK7i5uSEuLq7A\n1zFJmGdmZuLtt9+Gt7c3Ll68iO3bt+PSpUumWJXZs7KywhdffIELFy4gMDAQa9asKbPbIpunpycc\nHByg0+m0LkVTM2bMwODBg3Hp0iWcO3cO9vb2WpekiYiICHh5eSEoKAjBwcHIzMzEjh07tC6rxEyc\nOBHe3t56t3l4eMDV1RWhoaHo378/PDw8Cnwdk4T5yZMn0bJlSzRv3hxWVlZ49dVXsXfvXlOsyuw1\naNAAHTp0AABUr14d9vb2uHXrlsZVaefmzZs4cOAApkyZUqYn+o6Pj8fRo0cxadIkAECFChVQo0YN\njavShrW1NaysrJCUlISMjAwkJSWhcePGWpdVYp577jnUqlVL77Z9+/ZhwoQJAIAJEyZgz549Bb6O\nScI8KioKTZs2Va83adIEUVFRplhVqRIREYHTp0+jW7duWpeimVmzZmHZsmXq2cNlVXh4OOrVq4eJ\nEyeiY8eOeP3115GUlKR1WZqoXbs2Zs+ejWbNmqFRo0aoWbMmBgwYoHVZmoqJiYGNjQ0AwMbGBjEx\nMQU+xyTfqLL+8zkviYmJGDFiBDw9PVG9enWty9HE/v37Ub9+fTg5OZXpvXIAyMjIQFBQEN566y0E\nBQWhWrVqhfopbYmuXbuGlStXIiIiArdu3UJiYiK2bt2qdVlmQ6fTFSpTTRLmjRs3RmRkpHo9MjIS\nTZo0McWqSoX09HQMHz4cY8eOxdChQ7UuRzMBAQHYt28fWrRogVGjRuHIkSMYP3681mVpokmTJmjS\npAm6dOkCABgxYgSCgoI0rkobf//9N5ydnVGnTh1UqFABw4YNQ0BAgNZlacrGxkY9+/727duoX79+\ngc8xSZh37twZV65cQUREBNLS0rBz504MGTLEFKsyeyKCyZMnw8HBATNnztS6HE0tWrQIkZGRCA8P\nx44dO9CvXz9s2rRJ67I00aBBAzRt2hShoaEAAB8fHzg6OmpclTbatGmDwMBAJCcnQ0Tg4+MDBwcH\nrcvS1JAhQ7Bx40YAwMaNGwu3E1i8E/kLduDAAWnVqpXY2trKokWLTLUas3f06FHR6XTSvn176dCh\ng3To0EEOHjyodVma8/PzE3d3d63L0NSZM2ekc+fO8uyzz8rLL78scXFxWpekmSVLloiDg4O0bdtW\nxo8fL2lpaVqXVGJeffVVadiwoVhZWUmTJk1k/fr1cu/ePenfv7/Y2dmJq6urPHjwoMDX4UlDREQW\noGx3KSAishAMcyIiC8AwJyKyAAxzIiILwDAnIrIADHMiIgvAMCcisgAMcyIiC/D/URRFTYXGWlwA\nAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b42b8d0>"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exercise:\n",
      "# Generalize the code in fitting_sine_wave.py\n",
      "# to generate sample data with a phase shift\n",
      "\n",
      "# y = amp + sin(freq * (x+delta))\n",
      "\n",
      "# and to fit to a curve of this form. In other \n",
      "# words, you will have three parameters to the \n",
      "# new fit: amp, freq, delta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}